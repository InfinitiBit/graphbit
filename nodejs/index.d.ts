/* tslint:disable */
/* eslint-disable */

/* auto-generated by NAPI-RS */

export interface DocumentContent {
  /** The extracted text content */
  content: string
  /** Metadata about the document */
  metadata: Record<string, string>
  /** Number of pages (for multi-page documents) */
  pageCount?: number
  /** Document type detected */
  documentType: string
  /** File size in bytes */
  fileSize?: number
}
export interface EmbeddingResponse {
  /** Generated embeddings */
  embeddings: Array<Array<number>>
  /** Response metadata */
  metadata: Record<string, string>
  /** Whether the response was successful */
  success: boolean
  /** Error message if unsuccessful */
  error?: string
  /** Token usage information */
  tokenUsage?: number
}
export interface LlmResponse {
  /** Generated text content */
  content: string
  /** Token usage information */
  tokenUsage?: TokenUsage
  /** Response metadata */
  metadata: Record<string, string>
  /** Whether the response was successful */
  success: boolean
  /** Error message if unsuccessful */
  error?: string
}
export interface TokenUsage {
  /** Number of prompt tokens */
  promptTokens: number
  /** Number of completion tokens */
  completionTokens: number
  /** Total number of tokens */
  totalTokens: number
}
export interface GenerationOptions {
  /** Temperature for generation (0.0 to 2.0) */
  temperature?: number
  /** Maximum number of tokens to generate */
  maxTokens?: number
  /** Top-p for nucleus sampling */
  topP?: number
  /** Stop sequences */
  stop?: Array<string>
  /** Whether to stream the response */
  stream?: boolean
}
export interface TextSplitterConfig {
  /** Chunk size in characters or tokens */
  chunkSize: number
  /** Overlap between chunks */
  chunkOverlap: number
}
export interface TextChunk {
  /** The text content of the chunk */
  content: string
  /** Metadata for the chunk */
  metadata: Record<string, string>
  /** Start position in the original text */
  startPos: number
  /** End position in the original text */
  endPos: number
  /** Chunk index */
  index: number
}
/**
 * Initialize the GraphBit library with production-grade configuration
 *
 * This function should be called once before using any other GraphBit functionality.
 * It sets up:
 * - Logging and tracing infrastructure
 * - Runtime configuration
 * - Core library initialization
 * - Resource management
 */
export declare function init(logLevel?: string | undefined | null, enableTracing?: boolean | undefined | null, debug?: boolean | undefined | null): Promise<void>
/**
 * Get the current version of GraphBit
 *
 * Returns the version string of the GraphBit core library.
 */
export declare function version(): string
/** System information structure */
export interface SystemInfo {
  /** Version information */
  version: string
  /** Node.js binding version */
  nodejsBindingVersion: string
  /** Runtime uptime in seconds */
  runtimeUptimeSeconds?: number
  /** Number of worker threads */
  runtimeWorkerThreads?: number
  /** Maximum blocking threads */
  runtimeMaxBlockingThreads?: number
  /** CPU count */
  cpuCount: number
  /** Runtime initialization status */
  runtimeInitialized: boolean
  /** Memory allocator */
  memoryAllocator: string
  /** Build target */
  buildTarget: string
  /** Build profile */
  buildProfile: string
}
/**
 * Get comprehensive system information and health status
 *
 * Returns an object containing:
 * - Version information
 * - Runtime statistics
 * - System capabilities
 * - Health status
 */
export declare function getSystemInfo(): Promise<SystemInfo>
/** Health check result structure */
export interface HealthCheck {
  /** Overall health status */
  overallHealthy: boolean
  /** Runtime health status */
  runtimeHealthy: boolean
  /** Runtime uptime status */
  runtimeUptimeOk?: boolean
  /** Worker threads status */
  workerThreadsOk?: boolean
  /** Runtime stats availability */
  runtimeStatsAvailable: boolean
  /** Total memory in MB */
  totalMemoryMb?: number
  /** Available memory in MB */
  availableMemoryMb?: number
  /** Memory health status */
  memoryHealthy: boolean
  /** Memory info availability */
  memoryInfoAvailable: boolean
  /** Timestamp of the health check */
  timestamp: number
}
/**
 * Validate the current environment and configuration
 *
 * Performs comprehensive health checks including:
 * - Runtime status
 * - Memory availability
 * - Thread pool status
 * - Core library health
 */
export declare function healthCheck(): Promise<HealthCheck>
/** Runtime configuration structure */
export interface RuntimeConfig {
  /** Number of worker threads */
  workerThreads?: number
  /** Maximum blocking threads */
  maxBlockingThreads?: number
  /** Thread stack size in MB */
  threadStackSizeMb?: number
}
/**
 * Configure the global runtime with custom settings
 *
 * This is an advanced function that allows customization of the async runtime.
 * It should be called before `init()` if custom configuration is needed.
 */
export declare function configureRuntime(config: RuntimeConfig): Promise<void>
/**
 * Gracefully shutdown the library (for testing and cleanup)
 *
 * This function cleans up resources and shuts down background threads.
 * It's primarily intended for testing and should not be called in normal usage.
 */
export declare function shutdown(): Promise<void>
export declare class DocumentLoaderConfig {
  /** Type of document loader (e.g., "pdf", "docx", "txt", "url") */
  loaderType: string
  /** Additional configuration parameters */
  config: Record<string, string>
  /** Chunk size for processing large documents */
  chunkSize?: number
  /** Chunk overlap for text splitting */
  chunkOverlap?: number
  /** Create a new document loader configuration */
  constructor(loaderType: string, chunkSize?: number | undefined | null, chunkOverlap?: number | undefined | null)
  /** Create a configuration for PDF documents */
  static pdf(chunkSize?: number | undefined | null, chunkOverlap?: number | undefined | null): DocumentLoaderConfig
  /** Create a configuration for DOCX documents */
  static docx(chunkSize?: number | undefined | null, chunkOverlap?: number | undefined | null): DocumentLoaderConfig
  /** Create a configuration for text files */
  static text(chunkSize?: number | undefined | null, chunkOverlap?: number | undefined | null): DocumentLoaderConfig
  /** Create a configuration for URLs */
  static url(chunkSize?: number | undefined | null, chunkOverlap?: number | undefined | null): DocumentLoaderConfig
  /** Set a configuration parameter */
  setConfig(key: string, value: string): void
  /** Get a configuration parameter */
  getConfig(key: string): string | null
}
export declare class DocumentLoader {
  /** Create a new document loader */
  constructor(config: DocumentLoaderConfig)
  /** Load a document from a file path */
  loadFromPath(filePath: string): Promise<DocumentContent>
  /** Load a document from text content */
  loadFromText(content: string, metadata?: Record<string, string> | undefined | null): Promise<DocumentContent>
  /** Load a document from a URL */
  loadFromUrl(url: string): Promise<DocumentContent>
  /** Get the current configuration */
  get config(): DocumentLoaderConfig
  /** Update the configuration */
  updateConfig(config: DocumentLoaderConfig): void
}
export declare class EmbeddingClient {
  /** Create a new embedding client */
  constructor(config: EmbeddingConfig)
  /** Generate embeddings for text */
  embed(text: string): Promise<EmbeddingResponse>
  /** Generate embeddings for multiple texts */
  embedBatch(texts: Array<string>): Promise<EmbeddingResponse>
  /** Get embedding dimensions for the current model */
  getDimensions(): Promise<number>
  /** Get the current configuration */
  get config(): EmbeddingConfig
  /** Update the configuration */
  updateConfig(config: EmbeddingConfig): void
}
export declare class EmbeddingConfig {
  /** Provider name (e.g., "openai", "huggingface", "local") */
  provider: string
  /** API key for the provider */
  apiKey?: string
  /** Base URL for the API */
  baseUrl?: string
  /** Model name/ID to use */
  model?: string
  /** Additional configuration parameters */
  config: Record<string, string>
  /** Request timeout in milliseconds */
  timeoutMs?: number
  /** Maximum retry attempts */
  maxRetries?: number
  /** Create a new embedding configuration */
  constructor(provider: string, apiKey?: string | undefined | null, baseUrl?: string | undefined | null, model?: string | undefined | null, timeoutMs?: number | undefined | null, maxRetries?: number | undefined | null)
  /** Create a configuration for OpenAI embeddings */
  static openai(apiKey: string, model?: string | undefined | null): EmbeddingConfig
  /** Create a configuration for Hugging Face embeddings */
  static huggingface(apiKey: string, model?: string | undefined | null): EmbeddingConfig
  /** Create a configuration for local embeddings */
  static local(baseUrl: string, model?: string | undefined | null): EmbeddingConfig
  /** Set a configuration parameter */
  setConfig(key: string, value: string): void
  /** Get a configuration parameter */
  getConfig(key: string): string | null
  /** Validate the configuration */
  validate(): void
}
export declare class LlmClient {
  /** Create a new LLM client */
  constructor(config: LlmConfig)
  /** Generate text from a prompt */
  generate(prompt: string, options?: GenerationOptions | undefined | null): Promise<LlmResponse>
  /** Generate text with streaming (future enhancement) */
  generateStream(prompt: string, options?: GenerationOptions | undefined | null): Promise<LlmResponse>
  /** Get the current configuration */
  get config(): LlmConfig
  /** Update the configuration */
  updateConfig(config: LlmConfig): void
}
export declare class LlmConfig {
  /** Provider name (e.g., "openai", "anthropic", "local") */
  provider: string
  /** API key for the provider */
  apiKey?: string
  /** Base URL for the API */
  baseUrl?: string
  /** Model name/ID to use */
  model?: string
  /** Additional configuration parameters */
  config: Record<string, string>
  /** Request timeout in milliseconds */
  timeoutMs?: number
  /** Maximum retry attempts */
  maxRetries?: number
  /** Create a new LLM configuration */
  constructor(provider: string, apiKey?: string | undefined | null, baseUrl?: string | undefined | null, model?: string | undefined | null, timeoutMs?: number | undefined | null, maxRetries?: number | undefined | null)
  /** Create a configuration for OpenAI */
  static openai(apiKey: string, model?: string | undefined | null): LlmConfig
  /** Create a configuration for Anthropic */
  static anthropic(apiKey: string, model?: string | undefined | null): LlmConfig
  /** Create a configuration for DeepSeek */
  static deepseek(apiKey: string, model?: string | undefined | null): LlmConfig
  /** Create a configuration for HuggingFace */
  static huggingface(apiKey: string, model?: string | undefined | null, baseUrl?: string | undefined | null): LlmConfig
  /** Create a configuration for Ollama */
  static ollama(model?: string | undefined | null, baseUrl?: string | undefined | null): LlmConfig
  /** Create a configuration for Perplexity */
  static perplexity(apiKey: string, model?: string | undefined | null): LlmConfig
  /** Create a configuration for local LLM */
  static local(baseUrl: string, model?: string | undefined | null): LlmConfig
  /** Set a configuration parameter */
  setConfig(key: string, value: string): void
  /** Get a configuration parameter */
  getConfig(key: string): string | null
  /** Set temperature for generation */
  setTemperature(temperature: number): void
  /** Set max tokens for generation */
  setMaxTokens(maxTokens: number): void
  /** Set top_p for generation */
  setTopP(topP: number): void
  /** Validate the configuration */
  validate(): void
}
export declare class CharacterSplitter {
  /** Create a new character-based text splitter */
  constructor(chunkSize: number, chunkOverlap: number)
  /** Split text into chunks */
  split(text: string): Array<TextChunk>
}
export declare class TokenSplitter {
  /** Create a new token-based text splitter */
  constructor(chunkSize: number, chunkOverlap: number)
  /** Split text into chunks based on tokens */
  split(text: string): Array<TextChunk>
}
export declare class SentenceSplitter {
  /** Create a new sentence-based text splitter */
  constructor(chunkSize: number, chunkOverlap: number)
  /** Split text into chunks based on sentences */
  split(text: string): Array<TextChunk>
}
export declare class RecursiveSplitter {
  /** Create a new recursive text splitter */
  constructor(chunkSize: number, chunkOverlap: number)
  /** Split text into chunks using recursive approach */
  split(text: string): Array<TextChunk>
}
export declare class WorkflowContext {
  /** Context data as key-value pairs */
  data: Record<string, string>
  /** Metadata for the context */
  metadata: Record<string, string>
  /** Create a new workflow context */
  constructor()
  /** Set a value in the context */
  set(key: string, value: string): void
  /** Get a value from the context */
  get(key: string): string | null
  /** Set metadata */
  setMetadata(key: string, value: string): void
  /** Get metadata */
  getMetadata(key: string): string | null
  /** Check if context contains a key */
  has(key: string): boolean
  /** Remove a key from context */
  remove(key: string): string | null
  /** Clear all context data */
  clear(): void
  /** Get all keys in the context */
  keys(): Array<string>
}
export declare class Executor {
  /** Create a new workflow executor */
  constructor(timeoutMs?: number | undefined | null, maxRetries?: number | undefined | null)
  /** Execute a workflow with the given context */
  execute(workflow: Workflow, context: WorkflowContext): Promise<WorkflowResult>
  /** Execute a workflow with input data directly */
  executeWithInput(workflow: Workflow, inputData: Record<string, string>): Promise<WorkflowResult>
  /** Set execution timeout */
  setTimeout(timeoutMs: number): void
  /** Set maximum retry attempts */
  setMaxRetries(maxRetries: number): void
  /** Get current timeout setting */
  get timeout(): number | null
  /** Get current max retries setting */
  get maxRetries(): number | null
}
export declare class Node {
  /** Create an agent node */
  static agent(name: string, prompt: string, agentId?: string | undefined | null, outputName?: string | undefined | null): Node
  /** Create a transform node */
  static transform(name: string, transformation: string): Node
  /** Create an input node */
  static input(name: string, inputSchema?: string | undefined | null): Node
  /** Create an output node */
  static output(name: string, outputSchema?: string | undefined | null): Node
  /** Get the node name */
  get name(): string
  /** Get the node description */
  get description(): string
  /** Get the node ID as a string */
  get id(): string
}
export declare class WorkflowResult {
  /** Whether the workflow execution was successful */
  success: boolean
  /** Result data from the workflow */
  data: Record<string, string>
  /** Error message if execution failed */
  error?: string
  /** Execution metadata */
  metadata: Record<string, string>
  /** Execution duration in milliseconds */
  durationMs?: number
  /** Create a successful workflow result */
  static success(data: Record<string, string>, durationMs?: number | undefined | null): WorkflowResult
  /** Create a failed workflow result */
  static failure(error: string, durationMs?: number | undefined | null): WorkflowResult
  /** Get a specific result value */
  get(key: string): string | null
  /** Check if result has a specific key */
  has(key: string): boolean
  /** Get all result keys */
  keys(): Array<string>
  /** Set metadata */
  setMetadata(key: string, value: string): void
  /** Get metadata */
  getMetadata(key: string): string | null
}
export declare class Workflow {
  /** Create a new workflow */
  constructor(name: string)
  /** Add a node to the workflow */
  addNode(node: Node): string
  /** Connect two nodes in the workflow */
  connect(fromId: string, toId: string): void
  /** Validate the workflow */
  validate(): void
  /** Get the workflow name */
  get name(): string
  /** Get the number of nodes in the workflow */
  get nodeCount(): number
  /** Get the number of edges in the workflow */
  get edgeCount(): number
}
