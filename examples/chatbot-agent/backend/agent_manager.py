"""
Chatbot Manager module for GraphBit-based conversational AI.

This module provides a comprehensive chatbot implementation using GraphBit's
workflow system, with vector database integration for context retrieval and
memory storage capabilities.
"""

import asyncio
import logging
import os
from typing import Optional

from dotenv import load_dotenv

from graphbit import Executor, LlmClient, LlmConfig, Node, Workflow

from .const import ConfigConstants
from .tool_manager import ToolManager

load_dotenv()

os.makedirs("logs", exist_ok=True)
logging.basicConfig(filename="logs/chatbot.log", filemode="a", format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)


class AgentManager:
    """
    AgentManager handles the configuration and interaction with the language model client.

    This class manages the OpenAI language model and embedding clients, providing methods
    for generating responses, embeddings, and streaming chat completions.
    """

    def __init__(self, api_key: str, model: str, tool_manager: ToolManager):
        """
        Initialize the AgentManager with the OpenAI API key.

        Args:
            api_key (str): OpenAI API key for accessing the language model.
        """
        # Configure LLM
        self.llm_config = LlmConfig.openai(model=model, api_key=api_key)
        self.llm_client = LlmClient(self.llm_config)

        # Assign tool manager
        self.tool_manager = tool_manager

        self.workflow = None

    def build_workflow(self, query: str):
        workflow = Workflow("Chatbot Workflow")

        prompt = f"""
You are a tool-using assistant.

TOOLS:
- save_personal_info(doc_content:str, metadata:dict) — call when the user reveals NEW personal facts
  (name, email, phone, role, employer, preferences, city/country). Include which fields you detected in metadata.fields.
- save_chat_history(doc_content:str, metadata:dict) — call once per turn with the full turn text
  ("user: {"user_query"}\\nassistant: {"your_answer"}") BEFORE finalizing your answer.
- get_personal_info(similarity_search_query:str) — call when you need profile facts already saved.
- get_chat_history(similarity_search_query:str) — call when you need conversation memory.

POLICY:
1) If THIS message includes new personal facts → call save_personal_info first.
2) If you generate a response → call save_chat_history for this turn.
3) Only then produce the final answer to the user.
4) If no saving is needed, skip the save tools.

EXAMPLE (personal info present):
User: "Hi, I'm Tanima. My email is tanimahossain01@gmail.com."
Assistant calls: save_personal_info({{
  "doc_content": "Hi, I'm Tanima. My email is tanimahossain01@gmail.com.",
  "metadata": {{"fields": ["name","email"], "source":"chat"}}
}})
Assistant (final): "Nice to meet you, Tanima! I've noted that."

Now handle the current user query:
{query}
"""

        save_personal_info = self.tool_manager.save_personal_info
        save_chat_history = self.tool_manager.save_chat_history
        get_personal_info = self.tool_manager.get_personal_info
        get_chat_history = self.tool_manager.get_chat_history

        node = Node.agent(name="chatbot", prompt=prompt, agent_id="chatbot", tools=[save_personal_info, save_chat_history, get_personal_info, get_chat_history])
        output_node = Node.agent(
            name="output", prompt="Respond to user query based on the output of previous node as if you are having a conversation no need to mention your internal technical steps", agent_id="output"
        )
        node1 = workflow.add_node(node)
        # node2=workflow.add_node(output_node)
        # workflow.connect(node1, node2)
        self.workflow = workflow

        return workflow

    def get_executor(self):
        return Executor(self.llm_config)

    def execute_workflow(self, query: str):
        workflow = self.build_workflow(query=query)
        executor = self.get_executor()
        result = executor.execute(workflow)
        print("result: ", result.is_success())

        if result.is_success():
            output = result.get_node_output("chatbot")
            print("output: ", output)
            return output
        else:
            return "Error: " + result.get_error()

    async def chat_stream(self, prompt: str):
        """
        Stream chat response tokens from the LLM client.

        This method provides a streaming interface for chat completions, yielding
        response tokens as they are generated by the language model.

        Args:
            prompt (str): The input prompt to send to the language model.

        Yields:
            str: Individual response tokens from the streaming completion.
        """
        print("prompt: ", prompt)
        response = await asyncio.to_thread(self.execute_workflow, prompt)
        for char in response:
            yield char
