import asyncio
import os

import graphbit

# Check API key first
api_key = os.getenv("HUGGINGFACE_API_KEY")
if not api_key:
    print("‚ùå HUGGINGFACE_API_KEY not set")
    print("üí° To fix this:")
    print("1. Get your API key from: https://huggingface.co/settings/tokens")
    print("2. Set it in your environment:")
    print("   export HUGGINGFACE_API_KEY='your-api-key-here'")
    print("3. Or run this script with the API key:")
    print("   HUGGINGFACE_API_KEY='your-key' poetry run python huggingface_integration.py")
    exit(1)

print("‚úÖ API key found")

# Initialize Graphbit
graphbit.init()

# Try different models that don't require 'sentences' parameter
models_to_try = [
    "sentence-transformers/paraphrase-MiniLM-L3-v2",
    "sentence-transformers/all-mpnet-base-v2", 
    "sentence-transformers/all-MiniLM-L12-v2",
    "sentence-transformers/msmarco-MiniLM-L-6-v3",
    # Try some non-sentence-transformers models
    "intfloat/multilingual-e5-large",
    "BAAI/bge-large-en-v1.5",
    "BAAI/bge-base-en-v1.5"
]

working_embedding_client = None

for model in models_to_try:
    print(f"\nüîç Trying model: {model}")
    try:
        embedding_client = graphbit.EmbeddingClient(
            graphbit.EmbeddingConfig.huggingface(
                model=model,
                api_key=api_key,
            )
        )
        
        # Test with single embedding
        test_embedding = embedding_client.embed("test")
        print(f"‚úÖ Model {model} works! Generated {len(test_embedding)}-dimensional embedding")
        working_embedding_client = embedding_client
        break
        
    except Exception as e:
        print(f"‚ùå Model {model} failed: {e}")

if not working_embedding_client:
    print("\n‚ùå No embedding models worked")
    print("üí° This might be due to:")
    print("1. Invalid API key")
    print("2. Network connectivity issues")
    print("3. HuggingFace API rate limiting")
    print("4. The GraphBit code needs to be updated to handle sentence similarity models")
    exit(1)

# Configure Hugging Face LLM
try:
    llm_config = graphbit.LlmConfig.huggingface(
        model="microsoft/DialoGPT-medium",
        api_key=api_key,
    )
    llm_client = graphbit.LlmClient(llm_config)
    print("‚úÖ LLM client created")
except Exception as e:
    print(f"‚ùå LLM client failed: {e}")
    print("üí° Will continue with embeddings only")

# Generate embeddings with error handling
try:
    texts = [
        "GraphBit is a framework for LLM workflows and agent orchestration.", 
        "Hugging Face provides transformers and models for NLP tasks.", 
        "OpenAI offers tools for LLMs and embeddings."
    ]
    
    print("üìä Generating embeddings...")
    embeddings = working_embedding_client.embed_many(texts)
    print(f"‚úÖ Generated {len(embeddings)} embeddings")
    
    query = "GraphBit"
    print(f"üîç Generating query embedding for: '{query}'")
    query_embedding = working_embedding_client.embed(query)
    print(f"‚úÖ Query embedding generated with {len(query_embedding)} dimensions")

    # Calculate similarities with error handling
    context = []
    threshold = 0.3  # Lower threshold for better results
    print("üéØ Calculating similarities...")
    
    for i, (text, embedding) in enumerate(zip(texts, embeddings)):
        try:
            similarity = working_embedding_client.similarity(query_embedding, embedding)
            print(f"  Similarity {i+1}: {similarity:.3f}")
            if similarity > threshold:
                context.append((text, similarity))
        except Exception as e:
            print(f"  ‚ùå Error calculating similarity for text {i+1}: {e}")
    
    print(f"‚úÖ Found {len(context)} texts above threshold")

    # Try LLM if available
    if 'llm_client' in locals():
        # Simple prompt for text generation
        if context:
            context_text = "\n".join([f"- {text} (similarity: {sim:.3f})" for text, sim in context])
            prompt = f"""Based on the following context, explain what GraphBit is in one sentence:

Context:
{context_text}

Answer:"""
        else:
            prompt = "Explain what GraphBit is in one sentence."
        
        print("ü§ñ Generating LLM response...")
        try:
            # Use asyncio.run() which creates a new event loop
            result = asyncio.run(llm_client.complete_async(prompt, max_tokens=100))
            print(f"‚úÖ LLM Response: {result}")
        except Exception as e:
            print(f"‚ùå LLM failed: {e}")
            print("üìä Showing embedding results instead:")
            for i, (text, sim) in enumerate(context):
                print(f"  {i+1}. Similarity {sim:.3f}: {text[:50]}...")
    else:
        print("üìä Embedding-only mode - showing similarity results:")
        for i, (text, sim) in enumerate(context):
            print(f"  {i+1}. Similarity {sim:.3f}: {text[:50]}...")

except Exception as e:
    print(f"‚ùå Error during execution: {e}")
    print("üí° Please check your API key and internet connection")
