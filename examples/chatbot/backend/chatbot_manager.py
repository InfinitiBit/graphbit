"""
Chatbot Manager module for GraphBit-based conversational AI.

This module provides a comprehensive chatbot implementation using GraphBit's
workflow system, with vector database integration for context retrieval and
memory storage capabilities.
"""

import json
import logging
import os
from typing import Any, Dict, List, Optional

from chromadb import Client
from chromadb.config import Settings
from dotenv import load_dotenv
from fastapi import WebSocket

import graphbit

load_dotenv()

os.makedirs("logs", exist_ok=True)
logging.basicConfig(filename="logs/chatbot.log", filemode="a", format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)

VECTOR_DB_TEXT_FILE = "backend/data/vectordb.txt"
VECTOR_DB_INDEX_NAME = "vector_index_chatbot"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_LLM_MODEL = "gpt-3.5-turbo"
OPENAI_EMBEDDING_MODEL = "text-embedding-3-small"
CHUNK_SIZE = 1000
OVERLAP_SIZE = 100
MAX_TOKENS = 200
RETRIEVE_CONTEXT_N_RESULTS = 5
COLLECTION_NAME = "chatbot_memory"


class LLMManager:
    """
    LLMManager handles the configuration and interaction with the language model client.

    This class manages the OpenAI language model and embedding clients, providing methods
    for generating responses, embeddings, and streaming chat completions.
    """

    def __init__(self, api_key: str):
        """
        Initialize the LLMManager with the OpenAI API key.

        Args:
            api_key (str): OpenAI API key for accessing the language model.
        """
        graphbit.init()

        # Ensure OpenAI API key is present
        if not api_key:
            api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set. Please set it in your environment.")

        # Configure LLM
        self.llm_config = graphbit.LlmConfig.openai(model=OPENAI_LLM_MODEL, api_key=api_key)
        self.llm_client = graphbit.LlmClient(self.llm_config)

        # Configure embeddings
        self.embedding_config = graphbit.EmbeddingConfig.openai(model=OPENAI_EMBEDDING_MODEL, api_key=api_key)
        self.embedding_client = graphbit.EmbeddingClient(self.embedding_config)

    def embed(self, text: str) -> List[float]:
        """Generate embeddings for the given text using the configured embedding model."""
        return self.embedding_client.embed(text)

    def embed_many(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts using the configured embedding model."""
        return self.embedding_client.embed_many(texts)

    async def chat_stream(self, prompt: str):
        """
        Stream chat response tokens from the LLM client.

        This method provides a streaming interface for chat completions, yielding
        response tokens as they are generated by the language model.

        Args:
            prompt (str): The input prompt to send to the language model.

        Yields:
            str: Individual response tokens from the streaming completion.
        """
        response = await self.llm_client.complete_stream(prompt, max_tokens=MAX_TOKENS)
        for chunk in response:
            yield chunk


class VectorDBManager:
    """
    VectorDBManager handles the initialization and management of the vector database.

    This class manages ChromaDB operations including collection creation, document
    indexing, similarity search, and conversation history storage for the chatbot.
    """

    def __init__(self, index_name: str = VECTOR_DB_INDEX_NAME, llm_manager: Optional[LLMManager] = None):
        """
        Initialize the VectorDBManager with the specified index name and LLM manager.

        Args:
            index_name (str, optional): Name of the vector database index to use.
            llm_manager (Optional[LLMManager], optional): LLM manager instance for
                                                        generating embeddings.
        """
        if llm_manager is None:
            llm_manager = LLMManager(os.getenv("OPENAI_API_KEY"))
        self.llm_manager = llm_manager

        # Initialize ChromaDB
        self.index_name: str = index_name
        self.chroma_client: Optional[Client] = None
        self.collection = None

        self._init_vectorstore()

    def _init_vectorstore(self) -> None:
        """
        Initialize ChromaDB client and create or load the chatbot memory collection.

        This method sets up the persistent ChromaDB client and either loads an existing
        collection or creates a new one named 'chatbot_memory'.
        """
        try:
            self.chroma_client = Client(Settings(persist_directory=self.index_name, is_persistent=True))
            if self.chroma_client is not None:
                if COLLECTION_NAME in [c.name for c in self.chroma_client.list_collections()]:
                    self.collection = self.chroma_client.get_collection(name=COLLECTION_NAME)
                    logging.info("Loaded existing ChromaDB collection")
                else:
                    self.collection = self.chroma_client.create_collection(name=COLLECTION_NAME)
                    logging.info("Created new ChromaDB collection")

        except Exception as e:
            logging.error(f"Error initializing vector store: {str(e)}")
            self.chroma_client = None
            self.collection = None

    def _create_index(self, file_path: str = VECTOR_DB_TEXT_FILE) -> None:
        """
        Create vector index from a text file by chunking and embedding the content.

        This method reads content from the specified file, splits it into chunks,
        generates embeddings for each chunk, and stores them in the vector database.

        Args:
            file_path (str, optional): Path to the text file to index.
                                     Defaults to VECTOR_DB_TEXT_FILE.
        """
        try:
            content = self.get_or_create_initial_file(file_path)

            chunks = self._split_text(content, chunk_size=CHUNK_SIZE, overlap=OVERLAP_SIZE)

            if self.collection and chunks:
                embeddings = self.llm_manager.embed_many(chunks)

                for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                    doc_id = f"doc_{i}"
                    self.collection.add(documents=[chunk], embeddings=[embedding], ids=[doc_id], metadatas=[{"source": "initial_knowledge", "chunk_id": i}])

                logging.info(f"Vector store created with {len(chunks)} chunks")
            else:
                logging.warning("No content to index or collection not available")

        except Exception as e:
            logging.error(f"Error creating vector index: {str(e)}")
            raise

    def get_or_create_initial_file(self, file_path: str = VECTOR_DB_TEXT_FILE) -> str:
        """Ensure the initial knowledge file exists and return its content."""
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        if not os.path.exists(file_path):
            with open(file_path, "w", encoding="utf-8") as f:
                f.write("Conversation History:\n")
                f.write("This is the initial knowledge base for the chatbot.\n")
                f.write("The chatbot can answer questions and hold conversations.\n")
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
        return content

    def _split_text(self, text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP_SIZE) -> List[str]:
        """
        Split text into overlapping chunks for vector indexing.

        Args:
            text (str): Text to split.
            chunk_size (int): Max chunk size.
            overlap (int): Overlap between chunks.

        Returns:
            List[str]: List of non-empty text chunks.
        """
        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]

            if end < len(text):
                last_space = chunk.rfind(" ")
                if last_space > chunk_size:
                    chunk = chunk[:last_space]
                    end = start + last_space

            chunks.append(chunk.strip())
            start = end - overlap

            if start >= len(text):
                break

        return [chunk for chunk in chunks if chunk.strip()]

    def _save_to_vectordb(self, doc_content: str, metadata: dict) -> None:
        """
        Save document content after embedding to the vector database with metadata.

        Args:
            doc_content (str): The document content to save.
            metadata (dict): Metadata associated with the document, including
                           session_id, type, and source information.
        """
        try:
            if not self.collection:
                logging.warning("Vector store not initialized, skipping save")
                return

            with open(VECTOR_DB_TEXT_FILE, "a", encoding="utf-8") as f:
                f.write(f"\n{doc_content}\n")

            session_id = metadata.get("session_id", "default")
            doc_id = f"session_{session_id}_{hash(doc_content)}"
            doc_embedding = self.llm_manager.embed(doc_content)

            # Add to vector store
            self.collection.add(documents=[doc_content], embeddings=[doc_embedding], ids=[doc_id], metadatas=[metadata])
            logging.info(f"Saved conversation to vector DB for session {session_id}")

        except Exception as e:
            logging.error(f"Error saving to vector DB: {str(e)}")

    def _retrieve_context(self, query: str) -> str:
        """
        Retrieve relevant context from the vector database based on similarity search.

        This method generates embeddings for the query and searches the vector
        database for the most similar documents to provide context for responses.

        Args:
            query (str): The user query to search for relevant context.

        Returns:
            str: Concatenated relevant documents as context, or error message
                 if retrieval fails or no documents are found.
        """
        try:
            if not self.collection:
                return "No vector store available"

            query_embedding = self.llm_manager.embed(query)

            results = self.collection.query(query_embeddings=[query_embedding], n_results=RETRIEVE_CONTEXT_N_RESULTS)

            if "documents" in results and results["documents"]:
                context_docs = [doc for docs in results["documents"] for doc in docs]
                context = "\n\n".join(context_docs)
                logging.info(f"Retrieved {len(context_docs)} documents for context")
                return context
            else:
                logging.info("No documents found in similarity search")
                return "No relevant context found in vector database"

        except Exception as e:
            logging.error(f"Error retrieving context: {str(e)}")
            return f"Error retrieving context: {str(e)}"


class ChatbotManager:
    """
    ChatbotManager orchestrates conversation handling for the chatbot.

    This class coordinates between VectorDBManager and LLMManager to provide
    complete chatbot functionality including context retrieval, response generation,
    and conversation memory storage using GraphBit's workflow system.
    """

    def __init__(self, index_name: str = VECTOR_DB_INDEX_NAME):
        """
        Initialize the ChatbotManager with necessary configurations.

        Args:
            index_name (str, optional): Name of the vector database index to use.
                                      Defaults to VECTOR_DB_INDEX_NAME.
        """
        self.index_name: str = index_name

        # Ensure OpenAI API key is present
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set. Please set it in your environment.")

        self.llm_manager = LLMManager(openai_api_key)

        # Initialize ChromaDB
        self.vector_db_manager = VectorDBManager(index_name=self.index_name, llm_manager=self.llm_manager)

        # Session storage for message history
        self.sessions: Dict[str, List[Any]] = {}

    def _create_index(self, file_path: str = VECTOR_DB_TEXT_FILE) -> None:
        """Create vector index from a text file."""
        self.vector_db_manager._create_index(file_path)

    def _retrieve_context(self, query: str) -> str:
        """Retrieve relevant context from the vector database."""
        return self.vector_db_manager._retrieve_context(query)

    def _save_to_vectordb(self, query: str, session_id: str, response: str) -> None:
        """Save conversation to vector database by delegating to VectorDBManager."""
        try:
            if response:
                doc_content = f"Question: {query}\nAnswer: {response}"
            else:
                doc_content = f"Question: {query}\nAnswer: No processed summary available"
            metadata = {"session_id": session_id, "type": "qa_pair", "source": "chatbot_response"}
            self.vector_db_manager._save_to_vectordb(doc_content, metadata)
        except Exception as e:
            logging.error(f"Error saving to vector DB: {str(e)}")

    def format_prompt_ai_response(self, context: Optional[str] = "", chat_history: Optional[str] = "", query: Optional[str] = "") -> str:
        """
        Build the AI prompt using context, chat history, and the current question.

        Args:
            context (str): Relevant document context.
            chat_history (str): Recent conversation history.
            query (str): User's current question.

        Returns:
            str: Formatted prompt for the AI assistant.
        """
        prompt = f"""You are a helpful and friendly AI assistant. You can answer questions, hold normal conversations, and remember what the user has told you in this session.
You have access to external documents and chat history that you should use to enhance your answer when relevant.
Always try to:
- Understand the intent behind short or vague inputs
- Ask clarifying questions if needed
- Keep the conversation engaging and natural
- Use the chat history for personalization
- Reference the document context when it's clearly relevant

Document Context:
{context}

Recent Chat History:
{chat_history}

Current Question: {query}

Provide a helpful and engaging response:"""
        return prompt

    async def stream_full_chat(self, websocket: WebSocket, session_id: str, prompt: str):
        """Stream chat response tokens to the client via WebSocket."""
        response = ""
        async for token in self.llm_manager.chat_stream(prompt):
            response += token
            await websocket.send_text(json.dumps({"response": token, "session_id": session_id, "type": "chunk"}))
        return response

    async def chat(self, websocket: WebSocket, session_id: str, query: str) -> str:
        """
        Handle a chat message: manage session, retrieve context, generate and stream response, and store conversation.

        Args:
            websocket (WebSocket): WebSocket connection for streaming.
            session_id (str): Unique chat session ID.
            query (str): User's input message.

        Returns:
            str: Generated chatbot response or error message.
        """
        try:
            if session_id not in self.sessions:
                self.sessions[session_id] = []

            user_message = {"role": "user", "content": query}
            self.sessions[session_id].append(user_message)

            # Retrieve Context
            retrieved_docs = self._retrieve_context(query)

            # Get AI response
            prompt = self.format_prompt_ai_response(context=retrieved_docs, query=query)
            stream_response = await self.stream_full_chat(websocket, session_id, prompt)
            await websocket.send_text(json.dumps({"response": "", "session_id": session_id, "type": "end"}))

            # Add AI response to session
            ai_message = {"role": "assistant", "content": stream_response}
            self.sessions[session_id].append(ai_message)

            # Save to vector database
            self._save_to_vectordb(query, session_id, stream_response)

            return stream_response

        except Exception as e:
            logging.error(f"Error in chat: {str(e)}")
            return f"Sorry, I encountered an error: {str(e)}"
