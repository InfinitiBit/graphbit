"""
Chatbot Manager module for GraphBit-based conversational AI.

This module provides a comprehensive chatbot implementation using GraphBit's
workflow system, with vector database integration for context retrieval and
memory storage capabilities.
"""

import logging
import os
from typing import List, Optional

from dotenv import load_dotenv

from graphbit import EmbeddingClient, EmbeddingConfig, LlmClient, LlmConfig

from .const import ConfigConstants

load_dotenv()

os.makedirs("logs", exist_ok=True)
logging.basicConfig(filename="logs/chatbot.log", filemode="a", format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)


class LLMManager:
    """
    LLMManager handles the configuration and interaction with the language model client.

    This class manages the OpenAI language model and embedding clients, providing methods
    for generating responses, embeddings, and streaming chat completions.
    """

    def __init__(self, api_key: Optional[str] = ConfigConstants.OPENAI_API_KEY):
        """
        Initialize the LLMManager with the OpenAI API key.

        Args:
            api_key (str): OpenAI API key for accessing the language model.
        """
        # Ensure OpenAI API key is present
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set. Please set it in your environment.")

        # Configure LLM
        self.llm_config = LlmConfig.openai(model=ConfigConstants.OPENAI_LLM_MODEL, api_key=api_key)
        self.llm_client = LlmClient(self.llm_config)

        # Configure embeddings
        self.embedding_config = EmbeddingConfig.openai(model=ConfigConstants.OPENAI_EMBEDDING_MODEL, api_key=api_key)
        self.embedding_client = EmbeddingClient(self.embedding_config)

    def embed(self, text: str) -> List[float]:
        """Generate embeddings for the given text using the configured embedding model."""
        return self.embedding_client.embed(text)

    def embed_many(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts using the configured embedding model."""
        return self.embedding_client.embed_many(texts)

    async def chat_stream(self, prompt: str):
        """
        Stream chat response tokens from the LLM client.

        This method provides a streaming interface for chat completions, yielding
        response tokens as they are generated by the language model.

        Args:
            prompt (str): The input prompt to send to the language model.

        Yields:
            str: Individual response tokens from the streaming completion.
        """
        response = await self.llm_client.complete_stream(prompt, max_tokens=ConfigConstants.MAX_TOKENS)
        for chunk in response:
            yield chunk
