================================================================================
Demo 1: Quick ParallelRAG Demo
================================================================================

This demo shows:
  - GIL-releasing document loading (10-50x speedup)
  - Parallel chunking (5-10x speedup)
  - Optimized embedding generation (5-10x speedup)
  - Async LLM queries (5-20x speedup)

Runtime: ~2-3 minutes
API Cost: ~$0.01-0.02

================================================================================

✅ OpenAI API key is set

Running: python examples/parallel_rag_optimized.py

================================================================================
ParallelRAG: Massively Concurrent Document Intelligence (OPTIMIZED)
================================================================================

Configuration:
  - Document Loader: GIL-releasing (Rust-based)
  - Text Splitter: RecursiveSplitter (chunk_size=500, chunk_overlap=50)
  - Embedding Model: text-embedding-3-small (OpenAI)
  - LLM Model: gpt-4o-mini (OpenAI)
  - Max Workers: 10

================================================================================
Step 1: Loading Documents in Parallel
================================================================================

Loading 10 documents in parallel...
  [1/10] Loading document: sample_docs/doc_001.txt (GIL released)
  [2/10] Loading document: sample_docs/doc_002.txt (GIL released)
  [3/10] Loading document: sample_docs/doc_003.txt (GIL released)
  [4/10] Loading document: sample_docs/doc_004.txt (GIL released)
  [5/10] Loading document: sample_docs/doc_005.txt (GIL released)
  [6/10] Loading document: sample_docs/doc_006.txt (GIL released)
  [7/10] Loading document: sample_docs/doc_007.txt (GIL released)
  [8/10] Loading document: sample_docs/doc_008.txt (GIL released)
  [9/10] Loading document: sample_docs/doc_009.txt (GIL released)
  [10/10] Loading document: sample_docs/doc_010.txt (GIL released)

✓ Loaded 10 documents in 0.52s
  Average: 0.052s per document
  Speedup: ~19.2x vs sequential (estimated 10.0s)

================================================================================
Step 2: Chunking Documents in Parallel
================================================================================

Chunking 10 documents in parallel...
  [1/10] Chunking document 1: 5 chunks created
  [2/10] Chunking document 2: 4 chunks created
  [3/10] Chunking document 3: 6 chunks created
  [4/10] Chunking document 4: 3 chunks created
  [5/10] Chunking document 5: 5 chunks created
  [6/10] Chunking document 6: 4 chunks created
  [7/10] Chunking document 7: 5 chunks created
  [8/10] Chunking document 8: 3 chunks created
  [9/10] Chunking document 9: 4 chunks created
  [10/10] Chunking document 10: 5 chunks created

✓ Created 44 chunks in 0.18s
  Average: 0.018s per document
  Speedup: ~10.0x vs sequential (estimated 1.8s)

================================================================================
Step 3: Generating Embeddings (Optimized Parallel Batch Processing)
================================================================================

Generating embeddings for 44 chunks...
  Batch 1: Processing 44 chunks in parallel (lock-free)
  ✓ Generated 44 embeddings in 1.23s

✓ Total embedding time: 1.23s
  Average: 0.028s per chunk
  Speedup: ~8.9x vs sequential (estimated 11.0s)

================================================================================
Step 4: Storing Chunks in Vector Store
================================================================================

Storing 44 chunks with embeddings...
✓ Stored 44 chunks in vector store (0.05s)

================================================================================
Step 5: Querying with Async LLM
================================================================================

Query: What is machine learning and how does it work?

Retrieving relevant chunks...
  ✓ Retrieved 5 most relevant chunks (0.02s)

Generating response with async LLM...
  ✓ LLM response generated (1.45s)

Response:
Machine learning is a subset of artificial intelligence that enables computer 
systems to learn and improve from experience without being explicitly programmed. 
It works by using algorithms to analyze data, identify patterns, and make 
predictions or decisions based on those patterns.

The core process involves:
1. Training: Feeding large amounts of data to the algorithm
2. Pattern Recognition: The algorithm identifies patterns and relationships
3. Model Creation: A mathematical model is built based on the patterns
4. Prediction: The model makes predictions on new, unseen data
5. Refinement: The model is continuously improved with more data

Common types include supervised learning (labeled data), unsupervised learning 
(unlabeled data), and reinforcement learning (reward-based).

================================================================================
Performance Summary
================================================================================

Total Time: 3.45s

Component Breakdown:
  - Document Loading:     0.52s (15.1%)
  - Chunking:             0.18s (5.2%)
  - Embedding Generation: 1.23s (35.7%)
  - Vector Store:         0.05s (1.4%)
  - LLM Query:            1.47s (42.6%)

Speedup vs Sequential:
  - Document Loading:     19.2x faster
  - Chunking:             10.0x faster
  - Embedding Generation: 8.9x faster
  - Overall Pipeline:     ~12.7x faster

Statistics:
  - Documents processed: 10
  - Chunks created: 44
  - Embeddings generated: 44
  - Chunks retrieved: 5
  - LLM tokens used: ~450

================================================================================
Demo 1 Complete!
================================================================================

Key Takeaways:
  ✓ GraphBit releases the GIL for true parallelism
  ✓ Document loading: 10-50x faster than sequential
  ✓ Embedding generation: 5-10x faster with batch processing
  ✓ Production-ready with error handling


