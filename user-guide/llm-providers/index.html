
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../embeddings/">
      
      
        <link rel="next" href="../monitoring/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>LLM Providers - GraphBits AI</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#llm-providers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="GraphBits AI" class="md-header__button md-logo" aria-label="GraphBits AI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            GraphBits AI
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LLM Providers
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="GraphBits AI" class="md-nav__button md-logo" aria-label="GraphBits AI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    GraphBits AI
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Getting Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started/quickstart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quickstart
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started/installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started/examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    User Guide
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            User Guide
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../concepts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Concepts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../agents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agents
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../async-vs-sync/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Async vs Sync
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dynamics-graph/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dynamics Graph
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    LLM Providers
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    LLM Providers
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#supported-providers" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Providers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#available-openai-models" class="md-nav__link">
    <span class="md-ellipsis">
      Available OpenAI Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Anthropic Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#available-anthropic-models" class="md-nav__link">
    <span class="md-ellipsis">
      Available Anthropic Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#perplexity-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Perplexity Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Perplexity Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#available-perplexity-models" class="md-nav__link">
    <span class="md-ellipsis">
      Available Perplexity Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#available-deepseek-models" class="md-nav__link">
    <span class="md-ellipsis">
      Available DeepSeek Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-api-key-setup" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSeek API Key Setup
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingface-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      HuggingFace Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HuggingFace Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#popular-huggingface-models" class="md-nav__link">
    <span class="md-ellipsis">
      Popular HuggingFace Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingface-api-key-setup" class="md-nav__link">
    <span class="md-ellipsis">
      HuggingFace API Key Setup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-selection-tips" class="md-nav__link">
    <span class="md-ellipsis">
      Model Selection Tips
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama Configuration
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llm-client-usage" class="md-nav__link">
    <span class="md-ellipsis">
      LLM Client Usage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Client Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#creating-and-using-clients" class="md-nav__link">
    <span class="md-ellipsis">
      Creating and Using Clients
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#asynchronous-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Asynchronous Operations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-processing" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Processing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chat-style-interactions" class="md-nav__link">
    <span class="md-ellipsis">
      Chat-Style Interactions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#streaming-responses" class="md-nav__link">
    <span class="md-ellipsis">
      Streaming Responses
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#client-management-and-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Client Management and Monitoring
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Client Management and Monitoring">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#client-statistics" class="md-nav__link">
    <span class="md-ellipsis">
      Client Statistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#client-warmup" class="md-nav__link">
    <span class="md-ellipsis">
      Client Warmup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset-statistics" class="md-nav__link">
    <span class="md-ellipsis">
      Reset Statistics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#provider-specific-examples" class="md-nav__link">
    <span class="md-ellipsis">
      Provider-Specific Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Provider-Specific Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai-workflow-example" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI Workflow Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic-workflow-example" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic Workflow Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-workflow-example" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSeek Workflow Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-workflow-example" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama Workflow Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Performance Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#timeout-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Timeout Configuration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#executor-types-for-different-providers" class="md-nav__link">
    <span class="md-ellipsis">
      Executor Types for Different Providers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#error-handling" class="md-nav__link">
    <span class="md-ellipsis">
      Error Handling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Error Handling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#provider-specific-error-handling" class="md-nav__link">
    <span class="md-ellipsis">
      Provider-Specific Error Handling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#workflow-error-handling" class="md-nav__link">
    <span class="md-ellipsis">
      Workflow Error Handling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-provider-selection" class="md-nav__link">
    <span class="md-ellipsis">
      1. Provider Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-api-key-management" class="md-nav__link">
    <span class="md-ellipsis">
      2. API Key Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-client-reuse" class="md-nav__link">
    <span class="md-ellipsis">
      3. Client Reuse
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-monitoring-and-logging" class="md-nav__link">
    <span class="md-ellipsis">
      4. Monitoring and Logging
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-next" class="md-nav__link">
    <span class="md-ellipsis">
      What's Next
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../monitoring/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Monitoring
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../performance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Performance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reliability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reliability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../text-splitters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text Splitters
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../validation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Validation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../workflow-builder/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Workflow Builder
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/configuration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/node-types/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Node Types
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/python-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python API
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Connector Integrations
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Connector Integrations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/aws_boto3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AWS Boto3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/chromadb_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ChromaDB Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/faiss_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FAISS Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/google_search_api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Google Search API
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/mariadb_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MariaDB Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/milvus_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Milvus Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/mongodb/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MongoDB
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/pgvector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PGVector
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/pinecone_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pinecone Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/qdrant_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Qdrant Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/weaviate_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Weaviate Integration
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Development
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Development
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/contributing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contributing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/debugging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Debugging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/python-bindings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python Bindings
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/comprehensive-pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Comprehensive Pipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/content-generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Content Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/data-processing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Processing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/llm-integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../examples/semantic-search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Semantic Search
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#supported-providers" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Providers
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#available-openai-models" class="md-nav__link">
    <span class="md-ellipsis">
      Available OpenAI Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Anthropic Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#available-anthropic-models" class="md-nav__link">
    <span class="md-ellipsis">
      Available Anthropic Models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#perplexity-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Perplexity Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Perplexity Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#available-perplexity-models" class="md-nav__link">
    <span class="md-ellipsis">
      Available Perplexity Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#available-deepseek-models" class="md-nav__link">
    <span class="md-ellipsis">
      Available DeepSeek Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-api-key-setup" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSeek API Key Setup
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingface-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      HuggingFace Configuration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HuggingFace Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#popular-huggingface-models" class="md-nav__link">
    <span class="md-ellipsis">
      Popular HuggingFace Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#huggingface-api-key-setup" class="md-nav__link">
    <span class="md-ellipsis">
      HuggingFace API Key Setup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-selection-tips" class="md-nav__link">
    <span class="md-ellipsis">
      Model Selection Tips
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama Configuration
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llm-client-usage" class="md-nav__link">
    <span class="md-ellipsis">
      LLM Client Usage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LLM Client Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#creating-and-using-clients" class="md-nav__link">
    <span class="md-ellipsis">
      Creating and Using Clients
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#asynchronous-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Asynchronous Operations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-processing" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Processing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chat-style-interactions" class="md-nav__link">
    <span class="md-ellipsis">
      Chat-Style Interactions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#streaming-responses" class="md-nav__link">
    <span class="md-ellipsis">
      Streaming Responses
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#client-management-and-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Client Management and Monitoring
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Client Management and Monitoring">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#client-statistics" class="md-nav__link">
    <span class="md-ellipsis">
      Client Statistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#client-warmup" class="md-nav__link">
    <span class="md-ellipsis">
      Client Warmup
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset-statistics" class="md-nav__link">
    <span class="md-ellipsis">
      Reset Statistics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#provider-specific-examples" class="md-nav__link">
    <span class="md-ellipsis">
      Provider-Specific Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Provider-Specific Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai-workflow-example" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI Workflow Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic-workflow-example" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic Workflow Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-workflow-example" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSeek Workflow Example
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ollama-workflow-example" class="md-nav__link">
    <span class="md-ellipsis">
      Ollama Workflow Example
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Performance Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#timeout-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Timeout Configuration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#executor-types-for-different-providers" class="md-nav__link">
    <span class="md-ellipsis">
      Executor Types for Different Providers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#error-handling" class="md-nav__link">
    <span class="md-ellipsis">
      Error Handling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Error Handling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#provider-specific-error-handling" class="md-nav__link">
    <span class="md-ellipsis">
      Provider-Specific Error Handling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#workflow-error-handling" class="md-nav__link">
    <span class="md-ellipsis">
      Workflow Error Handling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-provider-selection" class="md-nav__link">
    <span class="md-ellipsis">
      1. Provider Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-api-key-management" class="md-nav__link">
    <span class="md-ellipsis">
      2. API Key Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-client-reuse" class="md-nav__link">
    <span class="md-ellipsis">
      3. Client Reuse
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-monitoring-and-logging" class="md-nav__link">
    <span class="md-ellipsis">
      4. Monitoring and Logging
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#whats-next" class="md-nav__link">
    <span class="md-ellipsis">
      What's Next
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="llm-providers">LLM Providers</h1>
<p>GraphBit supports multiple Large Language Model providers through a unified client interface. This guide covers configuration, usage, and optimization for each supported provider.</p>
<h2 id="supported-providers">Supported Providers</h2>
<p>GraphBit supports these LLM providers:
- <strong>OpenAI</strong> - GPT models including GPT-4o, GPT-4o-mini
- <strong>Anthropic</strong> - Claude models including Claude-3.5-Sonnet
- <strong>Perplexity</strong> - Real-time search-enabled models including Sonar models
=======
- <strong>Anthropic</strong> - Claude models including Claude-3.5-Sonnet<br />
- <strong>DeepSeek</strong> - High-performance models including DeepSeek-Chat, DeepSeek-Coder, and DeepSeek-Reasoner
- <strong>HuggingFace</strong> - Access to thousands of models via HuggingFace Inference API
- <strong>Ollama</strong> - Local model execution with various open-source models</p>
<h2 id="configuration">Configuration</h2>
<h3 id="openai-configuration">OpenAI Configuration</h3>
<p>Configure OpenAI provider with API key and model selection:</p>
<pre><code class="language-python">import graphbit
import os

# Basic OpenAI configuration
config = graphbit.LlmConfig.openai(
    api_key=os.getenv(&quot;OPENAI_API_KEY&quot;),
    model=&quot;gpt-4o-mini&quot;  # Optional - defaults to gpt-4o-mini
)

# Access configuration details
print(f&quot;Provider: {config.provider()}&quot;)  # &quot;OpenAI&quot;
print(f&quot;Model: {config.model()}&quot;)        # &quot;gpt-4o-mini&quot;
</code></pre>
<h4 id="available-openai-models">Available OpenAI Models</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Best For</th>
<th>Context Length</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>gpt-4o</code></td>
<td>Complex reasoning, latest features</td>
<td>128K</td>
<td>High quality, slower</td>
</tr>
<tr>
<td><code>gpt-4o-mini</code></td>
<td>Balanced performance and cost</td>
<td>128K</td>
<td>Good quality, faster</td>
</tr>
<tr>
<td><code>gpt-4-turbo</code></td>
<td>High-quality outputs</td>
<td>128K</td>
<td>High quality</td>
</tr>
<tr>
<td><code>gpt-3.5-turbo</code></td>
<td>Fast, cost-effective tasks</td>
<td>16K</td>
<td>Fast, economical</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Model selection examples
creative_config = graphbit.LlmConfig.openai(
    api_key=os.getenv(&quot;OPENAI_API_KEY&quot;),
    model=&quot;gpt-4o&quot;  # For creative and complex tasks
)

production_config = graphbit.LlmConfig.openai(
    api_key=os.getenv(&quot;OPENAI_API_KEY&quot;),
    model=&quot;gpt-4o-mini&quot;  # Balanced for production
)

fast_config = graphbit.LlmConfig.openai(
    api_key=os.getenv(&quot;OPENAI_API_KEY&quot;),
    model=&quot;gpt-3.5-turbo&quot;  # For high-volume, simple tasks
)
</code></pre>
<h3 id="anthropic-configuration">Anthropic Configuration</h3>
<p>Configure Anthropic provider for Claude models:</p>
<pre><code class="language-python"># Basic Anthropic configuration
config = graphbit.LlmConfig.anthropic(
    api_key=os.getenv(&quot;ANTHROPIC_API_KEY&quot;),
    model=&quot;claude-3-5-sonnet-20241022&quot;  # Optional - defaults to claude-3-5-sonnet-20241022
)

print(f&quot;Provider: {config.provider()}&quot;)  # &quot;Anthropic&quot;
print(f&quot;Model: {config.model()}&quot;)        # &quot;claude-3-5-sonnet-20241022&quot;
</code></pre>
<h4 id="available-anthropic-models">Available Anthropic Models</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Best For</th>
<th>Context Length</th>
<th>Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>claude-3-opus-20240229</code></td>
<td>Most capable, complex analysis</td>
<td>200K</td>
<td>Slowest, highest quality</td>
</tr>
<tr>
<td><code>claude-3-sonnet-20240229</code></td>
<td>Balanced performance</td>
<td>200K</td>
<td>Medium speed and quality</td>
</tr>
<tr>
<td><code>claude-3-haiku-20240307</code></td>
<td>Fast, cost-effective</td>
<td>200K</td>
<td>Fastest, good quality</td>
</tr>
<tr>
<td><code>claude-3-5-sonnet-20241022</code></td>
<td>Latest, improved reasoning</td>
<td>200K</td>
<td>Good speed, high quality</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Model selection for different use cases
complex_config = graphbit.LlmConfig.anthropic(
    api_key=os.getenv(&quot;ANTHROPIC_API_KEY&quot;),
    model=&quot;claude-3-opus-20240229&quot;  # For complex analysis
)

balanced_config = graphbit.LlmConfig.anthropic(
    api_key=os.getenv(&quot;ANTHROPIC_API_KEY&quot;),
    model=&quot;claude-3-5-sonnet-20241022&quot;  # For balanced workloads
)

fast_config = graphbit.LlmConfig.anthropic(
    api_key=os.getenv(&quot;ANTHROPIC_API_KEY&quot;),
    model=&quot;claude-3-haiku-20240307&quot;  # For speed and efficiency
)
</code></pre>
<h3 id="perplexity-configuration">Perplexity Configuration</h3>
<p>Configure Perplexity provider to access real-time search-enabled models:</p>
<pre><code class="language-python"># Basic Perplexity configuration
config = graphbit.LlmConfig.perplexity(
    api_key=os.getenv(&quot;PERPLEXITY_API_KEY&quot;),
    model=&quot;sonar&quot;  # Optional - defaults to sonar
)

print(f&quot;Provider: {config.provider()}&quot;)  # &quot;perplexity&quot;
print(f&quot;Model: {config.model()}&quot;)        # &quot;sonar&quot;
</code></pre>
<h4 id="available-perplexity-models">Available Perplexity Models</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Best For</th>
<th>Context Length</th>
<th>Special Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>sonar</code></td>
<td>General purpose with search</td>
<td>8K</td>
<td>Real-time web search, citations</td>
</tr>
<tr>
<td><code>sonar-reasoning</code></td>
<td>Complex reasoning with search</td>
<td>8K</td>
<td>Multi-step reasoning, web research</td>
</tr>
<tr>
<td><code>sonar-deep-research</code></td>
<td>Comprehensive research</td>
<td>32K</td>
<td>Exhaustive research, detailed analysis</td>
</tr>
<tr>
<td><code>pplx-7b-online</code></td>
<td>Fast online inference</td>
<td>4K</td>
<td>Quick responses with web data</td>
</tr>
<tr>
<td><code>pplx-70b-online</code></td>
<td>High-quality online inference</td>
<td>4K</td>
<td>Better quality with web data</td>
</tr>
<tr>
<td><code>pplx-7b-chat</code></td>
<td>General chat without search</td>
<td>8K</td>
<td>Standard chat functionality</td>
</tr>
<tr>
<td><code>pplx-70b-chat</code></td>
<td>High-quality chat</td>
<td>8K</td>
<td>Better chat quality</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Model selection for different use cases
research_config = graphbit.LlmConfig.perplexity(
    api_key=os.getenv(&quot;PERPLEXITY_API_KEY&quot;),
    model=&quot;sonar-deep-research&quot;  # For comprehensive research
)

reasoning_config = graphbit.LlmConfig.perplexity(
    api_key=os.getenv(&quot;PERPLEXITY_API_KEY&quot;),
    model=&quot;sonar-reasoning&quot;  # For complex problem solving
)

fast_search_config = graphbit.LlmConfig.perplexity(
    api_key=os.getenv(&quot;PERPLEXITY_API_KEY&quot;),
    model=&quot;pplx-7b-online&quot;  # For fast web-enabled responses
)

chat_config = graphbit.LlmConfig.perplexity(
    api_key=os.getenv(&quot;PERPLEXITY_API_KEY&quot;),
    model=&quot;pplx-70b-chat&quot;  # For high-quality chat without search
)
=======
### DeepSeek Configuration

Configure DeepSeek provider for high-performance, cost-effective AI models:

```python
# Basic DeepSeek configuration
config = graphbit.LlmConfig.deepseek(
    api_key=os.getenv(&quot;DEEPSEEK_API_KEY&quot;),
    model=&quot;deepseek-chat&quot;  # Optional - defaults to deepseek-chat
)

print(f&quot;Provider: {config.provider()}&quot;)  # &quot;deepseek&quot;
print(f&quot;Model: {config.model()}&quot;)        # &quot;deepseek-chat&quot;
</code></pre>
<h4 id="available-deepseek-models">Available DeepSeek Models</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Best For</th>
<th>Context Length</th>
<th>Performance</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>deepseek-chat</code></td>
<td>General conversation, instruction following</td>
<td>128K</td>
<td>High quality, fast</td>
<td>$0.14/$0.28 per 1M tokens</td>
</tr>
<tr>
<td><code>deepseek-coder</code></td>
<td>Code generation, programming tasks</td>
<td>128K</td>
<td>Specialized for code</td>
<td>$0.14/$0.28 per 1M tokens</td>
</tr>
<tr>
<td><code>deepseek-reasoner</code></td>
<td>Complex reasoning, mathematics</td>
<td>128K</td>
<td>Advanced reasoning</td>
<td>$0.55/$2.19 per 1M tokens</td>
</tr>
</tbody>
</table>
<h4 id="key-features">Key Features</h4>
<ul>
<li><strong>Cost-Effective</strong>: Among the most competitive pricing in the market</li>
<li><strong>Function Calling</strong>: Full support for tool/function calling</li>
<li><strong>Large Context</strong>: 128K token context window for all models</li>
<li><strong>OpenAI Compatible</strong>: Uses OpenAI-compatible API format</li>
<li><strong>High Performance</strong>: Optimized for speed and quality</li>
</ul>
<pre><code class="language-python"># Model selection for different use cases
general_config = graphbit.LlmConfig.deepseek(
    api_key=os.getenv(&quot;DEEPSEEK_API_KEY&quot;),
    model=&quot;deepseek-chat&quot;  # For general tasks and conversation
)

coding_config = graphbit.LlmConfig.deepseek(
    api_key=os.getenv(&quot;DEEPSEEK_API_KEY&quot;),
    model=&quot;deepseek-coder&quot;  # For code generation and programming
)

reasoning_config = graphbit.LlmConfig.deepseek(
    api_key=os.getenv(&quot;DEEPSEEK_API_KEY&quot;),
    model=&quot;deepseek-reasoner&quot;  # For complex reasoning tasks
)
</code></pre>
<h4 id="deepseek-api-key-setup">DeepSeek API Key Setup</h4>
<p>To use DeepSeek models, you need an API key:</p>
<ol>
<li>Create an account at <a href="https://platform.deepseek.com/">DeepSeek</a></li>
<li>Generate an API key in your dashboard</li>
<li>Set the environment variable:</li>
</ol>
<pre><code class="language-bash">export DEEPSEEK_API_KEY=&quot;your-api-key-here&quot;
</code></pre>
<h3 id="huggingface-configuration">HuggingFace Configuration</h3>
<p>Configure HuggingFace provider to access thousands of models via the Inference API:</p>
<pre><code class="language-python"># Basic HuggingFace configuration
config = graphbit.LlmConfig.huggingface(
    api_key=os.getenv(&quot;HUGGINGFACE_API_KEY&quot;),
    model=&quot;microsoft/DialoGPT-medium&quot;  # Optional - defaults to microsoft/DialoGPT-medium
)

print(f&quot;Provider: {config.provider()}&quot;)  # &quot;huggingface&quot;
print(f&quot;Model: {config.model()}&quot;)        # &quot;microsoft/DialoGPT-medium&quot;

# Custom endpoint configuration
custom_config = graphbit.LlmConfig.huggingface(
    api_key=os.getenv(&quot;HUGGINGFACE_API_KEY&quot;),
    model=&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;,
    base_url=&quot;https://my-custom-endpoint.huggingface.co&quot;  # Optional custom endpoint
)
</code></pre>
<h4 id="popular-huggingface-models">Popular HuggingFace Models</h4>
<table>
<thead>
<tr>
<th>Model</th>
<th>Best For</th>
<th>Size</th>
<th>Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>microsoft/DialoGPT-medium</code></td>
<td>Conversational AI, chat</td>
<td>345M</td>
<td>Fast, good dialogue</td>
</tr>
<tr>
<td><code>mistralai/Mistral-7B-Instruct-v0.1</code></td>
<td>General instruction following</td>
<td>7B</td>
<td>High quality, versatile</td>
</tr>
<tr>
<td><code>microsoft/CodeBERT-base</code></td>
<td>Code understanding</td>
<td>125M</td>
<td>Specialized for code</td>
</tr>
<tr>
<td><code>facebook/blenderbot-400M-distill</code></td>
<td>Conversational AI</td>
<td>400M</td>
<td>Balanced dialogue</td>
</tr>
<tr>
<td><code>huggingface/CodeBERTa-small-v1</code></td>
<td>Code generation</td>
<td>84M</td>
<td>Fast code tasks</td>
</tr>
<tr>
<td><code>microsoft/DialoGPT-large</code></td>
<td>Advanced dialogue</td>
<td>762M</td>
<td>Higher quality chat</td>
</tr>
</tbody>
</table>
<pre><code class="language-python"># Model selection for different use cases
dialogue_config = graphbit.LlmConfig.huggingface(
    api_key=os.getenv(&quot;HUGGINGFACE_API_KEY&quot;),
    model=&quot;microsoft/DialoGPT-large&quot;  # For high-quality dialogue
)

instruction_config = graphbit.LlmConfig.huggingface(
    api_key=os.getenv(&quot;HUGGINGFACE_API_KEY&quot;),
    model=&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;  # For instruction following
)

code_config = graphbit.LlmConfig.huggingface(
    api_key=os.getenv(&quot;HUGGINGFACE_API_KEY&quot;),
    model=&quot;microsoft/CodeBERT-base&quot;  # For code-related tasks
)

# Fast and lightweight option
lightweight_config = graphbit.LlmConfig.huggingface(
    api_key=os.getenv(&quot;HUGGINGFACE_API_KEY&quot;),
    model=&quot;microsoft/DialoGPT-medium&quot;  # Balanced performance
)
</code></pre>
<h4 id="huggingface-api-key-setup">HuggingFace API Key Setup</h4>
<p>To use HuggingFace models, you need an API key:</p>
<ol>
<li>Create an account at <a href="https://huggingface.co/">HuggingFace</a></li>
<li>Generate an API token in your <a href="https://huggingface.co/settings/tokens">settings</a></li>
<li>Set the environment variable:</li>
</ol>
<pre><code class="language-bash">export HUGGINGFACE_API_KEY=&quot;your-api-key-here&quot;
</code></pre>
<h4 id="model-selection-tips">Model Selection Tips</h4>
<ul>
<li><strong>Free Tier</strong>: Most models work with free HuggingFace accounts</li>
<li><strong>Custom Models</strong>: You can use any public model from the HuggingFace Hub</li>
<li><strong>Private Models</strong>: Use your own fine-tuned models with appropriate permissions</li>
<li><strong>Performance</strong>: Larger models (7B+) provide better quality but slower responses</li>
<li><strong>Cost</strong>: HuggingFace Inference API has competitive pricing for hosted inference</li>
</ul>
<h3 id="ollama-configuration">Ollama Configuration</h3>
<p>Configure Ollama for local model execution:</p>
<pre><code class="language-python"># Basic Ollama configuration (no API key required)
config = graphbit.LlmConfig.ollama(
    model=&quot;llama3.2&quot;  # Optional - defaults to llama3.2
)

print(f&quot;Provider: {config.provider()}&quot;)  # &quot;Ollama&quot;
print(f&quot;Model: {config.model()}&quot;)        # &quot;llama3.2&quot;

# Other popular models
mistral_config = graphbit.LlmConfig.ollama(model=&quot;mistral&quot;)
codellama_config = graphbit.LlmConfig.ollama(model=&quot;codellama&quot;)
phi_config = graphbit.LlmConfig.ollama(model=&quot;phi&quot;)
</code></pre>
<h2 id="llm-client-usage">LLM Client Usage</h2>
<h3 id="creating-and-using-clients">Creating and Using Clients</h3>
<pre><code class="language-python"># Create client with configuration
client = graphbit.LlmClient(config, debug=False)

# Basic text completion
response = client.complete(
    prompt=&quot;Explain the concept of machine learning&quot;,
    max_tokens=500,     # Optional - controls response length
    temperature=0.7     # Optional - controls randomness (0.0-1.0)
)

print(f&quot;Response: {response}&quot;)
</code></pre>
<h3 id="asynchronous-operations">Asynchronous Operations</h3>
<p>GraphBit provides async methods for non-blocking operations:</p>
<pre><code class="language-python">import asyncio

async def async_completion():
    # Async completion
    response = await client.complete_async(
        prompt=&quot;Write a short story about AI&quot;,
        max_tokens=300,
        temperature=0.8
    )
    return response

# Run async operation
response = asyncio.run(async_completion())
</code></pre>
<h3 id="batch-processing">Batch Processing</h3>
<p>Process multiple prompts efficiently:</p>
<pre><code class="language-python">async def batch_processing():
    prompts = [
        &quot;Summarize quantum computing&quot;,
        &quot;Explain blockchain technology&quot;, 
        &quot;Describe neural networks&quot;,
        &quot;What is machine learning?&quot;
    ]

    responses = await client.complete_batch(
        prompts=prompts,
        max_tokens=200,
        temperature=0.5,
        max_concurrency=3  # Process 3 at a time
    )

    for i, response in enumerate(responses):
        print(f&quot;Response {i+1}: {response}&quot;)

asyncio.run(batch_processing())
</code></pre>
<h3 id="chat-style-interactions">Chat-Style Interactions</h3>
<p>Use chat-optimized methods for conversational interactions:</p>
<pre><code class="language-python">async def chat_example():
    # Chat with message history
    response = await client.chat_optimized(
        messages=[
            (&quot;user&quot;, &quot;Hello, how are you?&quot;),
            (&quot;assistant&quot;, &quot;I'm doing well, thank you!&quot;),
            (&quot;user&quot;, &quot;Can you help me with Python programming?&quot;),
            (&quot;user&quot;, &quot;Specifically, how do I handle exceptions?&quot;)
        ],
        max_tokens=400,
        temperature=0.3
    )

    print(f&quot;Chat response: {response}&quot;)

asyncio.run(chat_example())
</code></pre>
<h3 id="streaming-responses">Streaming Responses</h3>
<p>Get real-time streaming responses:</p>
<pre><code class="language-python">async def streaming_example():
    print(&quot;Streaming response:&quot;)

    async for chunk in client.complete_stream(
        prompt=&quot;Tell me a detailed story about space exploration&quot;,
        max_tokens=1000,
        temperature=0.7
    ):
        print(chunk, end=&quot;&quot;, flush=True)

    print(&quot;\n--- Stream complete ---&quot;)

asyncio.run(streaming_example())
</code></pre>
<h2 id="client-management-and-monitoring">Client Management and Monitoring</h2>
<h3 id="client-statistics">Client Statistics</h3>
<p>Monitor client performance and usage:</p>
<pre><code class="language-python"># Get comprehensive statistics
stats = client.get_stats()

print(f&quot;Total requests: {stats['total_requests']}&quot;)
print(f&quot;Successful requests: {stats['successful_requests']}&quot;)
print(f&quot;Failed requests: {stats['failed_requests']}&quot;)
print(f&quot;Average response time: {stats['average_response_time_ms']}ms&quot;)
print(f&quot;Circuit breaker state: {stats['circuit_breaker_state']}&quot;)
print(f&quot;Client uptime: {stats['uptime']}&quot;)

# Calculate success rate
if stats['total_requests'] &gt; 0:
    success_rate = stats['successful_requests'] / stats['total_requests']
    print(f&quot;Success rate: {success_rate:.2%}&quot;)
</code></pre>
<h3 id="client-warmup">Client Warmup</h3>
<p>Pre-initialize connections for better performance:</p>
<pre><code class="language-python">async def warmup_client():
    # Warmup client to reduce cold start latency
    await client.warmup()
    print(&quot;Client warmed up and ready&quot;)

# Warmup before production use
asyncio.run(warmup_client())
</code></pre>
<h3 id="reset-statistics">Reset Statistics</h3>
<p>Reset client statistics for monitoring periods:</p>
<pre><code class="language-python"># Reset statistics
client.reset_stats()
print(&quot;Client statistics reset&quot;)
</code></pre>
<h2 id="provider-specific-examples">Provider-Specific Examples</h2>
<h3 id="openai-workflow-example">OpenAI Workflow Example</h3>
<pre><code class="language-python">def create_openai_workflow():
    &quot;&quot;&quot;Create workflow using OpenAI&quot;&quot;&quot;
    # Initialize GraphBit
    graphbit.init()

    # Configure OpenAI
    config = graphbit.LlmConfig.openai(
        api_key=os.getenv(&quot;OPENAI_API_KEY&quot;),
        model=&quot;gpt-4o-mini&quot;
    )

    # Create workflow
    workflow = graphbit.Workflow(&quot;OpenAI Analysis Pipeline&quot;)

    # Create analyzer node
    analyzer = graphbit.Node.agent(
        name=&quot;GPT Content Analyzer&quot;,
        prompt=&quot;Analyze the following content for sentiment, key themes, and quality:\n\n{input}&quot;,
        agent_id=&quot;gpt_analyzer&quot;
    )

    # Add to workflow
    analyzer_id = workflow.add_node(analyzer)
    workflow.validate()

    # Create executor and run
    executor = graphbit.Executor(config, timeout_seconds=60)
    return workflow, executor

# Usage
workflow, executor = create_openai_workflow()
result = executor.execute(workflow)
</code></pre>
<h3 id="anthropic-workflow-example">Anthropic Workflow Example</h3>
<pre><code class="language-python">def create_anthropic_workflow():
    &quot;&quot;&quot;Create workflow using Anthropic Claude&quot;&quot;&quot;
    graphbit.init()

    # Configure Anthropic
    config = graphbit.LlmConfig.anthropic(
        api_key=os.getenv(&quot;ANTHROPIC_API_KEY&quot;),
        model=&quot;claude-3-5-sonnet-20241022&quot;
    )

    # Create workflow
    workflow = graphbit.Workflow(&quot;Claude Analysis Pipeline&quot;)

    # Create analyzer with detailed prompt
    analyzer = graphbit.Node.agent(
        name=&quot;Claude Content Analyzer&quot;,
        prompt=&quot;&quot;&quot;
        Analyze the following content with attention to:
        - Factual accuracy and logical consistency
        - Potential biases or assumptions
        - Clarity and structure
        - Key insights and recommendations

        Content: {input}

        Provide your analysis in a structured format.
        &quot;&quot;&quot;,
        agent_id=&quot;claude_analyzer&quot;
    )

    workflow.add_node(analyzer)
    workflow.validate()

    # Create executor with longer timeout for Claude
    executor = graphbit.Executor(config, timeout_seconds=120)
    return workflow, executor

# Usage
workflow, executor = create_anthropic_workflow()
</code></pre>
<h3 id="deepseek-workflow-example">DeepSeek Workflow Example</h3>
<pre><code class="language-python">def create_deepseek_workflow():
    &quot;&quot;&quot;Create workflow using DeepSeek models&quot;&quot;&quot;
    graphbit.init()

    # Configure DeepSeek
    config = graphbit.LlmConfig.deepseek(
        api_key=os.getenv(&quot;DEEPSEEK_API_KEY&quot;),
        model=&quot;deepseek-chat&quot;
    )

    # Create workflow
    workflow = graphbit.Workflow(&quot;DeepSeek Analysis Pipeline&quot;)

    # Create analyzer optimized for DeepSeek's capabilities
    analyzer = graphbit.Node.agent(
        name=&quot;DeepSeek Content Analyzer&quot;,
        prompt=&quot;&quot;&quot;
        Analyze the following content efficiently and accurately:
        - Main topics and themes
        - Key insights and takeaways
        - Actionable recommendations
        - Potential concerns or limitations

        Content: {input}

        Provide a clear, structured analysis.
        &quot;&quot;&quot;,
        agent_id=&quot;deepseek_analyzer&quot;
    )

    workflow.add_node(analyzer)
    workflow.validate()

    # Create executor optimized for DeepSeek's fast inference
    executor = graphbit.Executor(config, timeout_seconds=90)
    return workflow, executor

# Usage for different DeepSeek models
def create_deepseek_coding_workflow():
    &quot;&quot;&quot;Create workflow for code analysis using DeepSeek Coder&quot;&quot;&quot;
    graphbit.init()

    config = graphbit.LlmConfig.deepseek(
        api_key=os.getenv(&quot;DEEPSEEK_API_KEY&quot;),
        model=&quot;deepseek-coder&quot;
    )

    workflow = graphbit.Workflow(&quot;DeepSeek Code Analysis&quot;)

    code_analyzer = graphbit.Node.agent(
        name=&quot;DeepSeek Code Reviewer&quot;,
        prompt=&quot;&quot;&quot;
        Review this code for:
        - Code quality and best practices
        - Potential bugs or issues
        - Performance improvements
        - Security considerations

        Code: {input}
        &quot;&quot;&quot;,
        agent_id=&quot;deepseek_code_analyzer&quot;
    )

    workflow.add_node(code_analyzer)
    workflow.validate()

    executor = graphbit.Executor(config, timeout_seconds=90)
    return workflow, executor

# Usage
workflow, executor = create_deepseek_workflow()
</code></pre>
<h3 id="ollama-workflow-example">Ollama Workflow Example</h3>
<pre><code class="language-python">def create_ollama_workflow():
    &quot;&quot;&quot;Create workflow using local Ollama models&quot;&quot;&quot;
    graphbit.init()

    # Configure Ollama (no API key needed)
    config = graphbit.LlmConfig.ollama(model=&quot;llama3.2&quot;)

    # Create workflow
    workflow = graphbit.Workflow(&quot;Local LLM Pipeline&quot;)

    # Create analyzer optimized for local models
    analyzer = graphbit.Node.agent(
        name=&quot;Local Model Analyzer&quot;,
        prompt=&quot;Analyze this text briefly: {input}&quot;,
        agent_id=&quot;local_analyzer&quot;
    )

    workflow.add_node(analyzer)
    workflow.validate()

    # Create executor with longer timeout for local processing
    executor = graphbit.Executor(config, timeout_seconds=180)
    return workflow, executor

# Usage
workflow, executor = create_ollama_workflow()
</code></pre>
<h2 id="performance-optimization">Performance Optimization</h2>
<h3 id="timeout-configuration">Timeout Configuration</h3>
<p>Configure appropriate timeouts for different providers:</p>
<pre><code class="language-python"># OpenAI - typically faster
openai_executor = graphbit.Executor(
    openai_config, 
    timeout_seconds=60
)


anthropic_executor = graphbit.Executor(
    anthropic_config, 
    timeout_seconds=120
)

deepseek_executor = graphbit.Executor(
    deepseek_config, 
    timeout_seconds=90
)


ollama_executor = graphbit.Executor(
    ollama_config, 
    timeout_seconds=180
)
</code></pre>
<h3 id="executor-types-for-different-providers">Executor Types for Different Providers</h3>
<p>Choose appropriate executor types based on provider characteristics:</p>
<pre><code class="language-python"># High-throughput for cloud providers
cloud_executor = graphbit.Executor.new_high_throughput(
    llm_config=openai_config,
    timeout_seconds=60
)

# Low-latency for fast providers
realtime_executor = graphbit.Executor.new_low_latency(
    llm_config=anthropic_config,
    timeout_seconds=30
)

# Memory-optimized for local models
local_executor = graphbit.Executor.new_memory_optimized(
    llm_config=ollama_config,
    timeout_seconds=180
)
</code></pre>
<h2 id="error-handling">Error Handling</h2>
<h3 id="provider-specific-error-handling">Provider-Specific Error Handling</h3>
<pre><code class="language-python">def robust_llm_usage():
    try:
        # Initialize and configure
        graphbit.init()
        config = graphbit.LlmConfig.openai(
            api_key=os.getenv(&quot;OPENAI_API_KEY&quot;)
        )
        client = graphbit.LlmClient(config)

        # Execute with error handling
        response = client.complete(
            prompt=&quot;Test prompt&quot;,
            max_tokens=100
        )

        return response

    except Exception as e:
        print(f&quot;LLM operation failed: {e}&quot;)
        return None
</code></pre>
<h3 id="workflow-error-handling">Workflow Error Handling</h3>
<pre><code class="language-python">def execute_with_error_handling(workflow, executor):
    try:
        result = executor.execute(workflow)

        if result.is_completed():
            return result.output()
        elif result.is_failed():
            error_msg = result.error()
            print(f&quot;Workflow failed: {error_msg}&quot;)
            return None

    except Exception as e:
        print(f&quot;Execution error: {e}&quot;)
        return None
</code></pre>
<h2 id="best-practices">Best Practices</h2>
<h3 id="1-provider-selection">1. Provider Selection</h3>
<p>Choose providers based on your requirements:</p>
<pre><code class="language-python">def get_optimal_config(use_case):
    &quot;&quot;&quot;Select optimal provider for use case&quot;&quot;&quot;
    if use_case == &quot;creative&quot;:
        return graphbit.LlmConfig.openai(
            api_key=os.getenv(&quot;OPENAI_API_KEY&quot;),
            model=&quot;gpt-4o&quot;
        )
    elif use_case == &quot;analytical&quot;:
        return graphbit.LlmConfig.anthropic(
            api_key=os.getenv(&quot;ANTHROPIC_API_KEY&quot;),
            model=&quot;claude-3-5-sonnet-20241022&quot;
        )
    elif use_case == &quot;cost_effective&quot;:
        return graphbit.LlmConfig.deepseek(
            api_key=os.getenv(&quot;DEEPSEEK_API_KEY&quot;),
            model=&quot;deepseek-chat&quot;
        )
    elif use_case == &quot;coding&quot;:
        return graphbit.LlmConfig.deepseek(
            api_key=os.getenv(&quot;DEEPSEEK_API_KEY&quot;),
            model=&quot;deepseek-coder&quot;
        )
    elif use_case == &quot;reasoning&quot;:
        return graphbit.LlmConfig.deepseek(
            api_key=os.getenv(&quot;DEEPSEEK_API_KEY&quot;),
            model=&quot;deepseek-reasoner&quot;
        )
    elif use_case == &quot;local&quot;:
        return graphbit.LlmConfig.ollama(model=&quot;llama3.2&quot;)
    else:
        return graphbit.LlmConfig.openai(
            api_key=os.getenv(&quot;OPENAI_API_KEY&quot;),
            model=&quot;gpt-4o-mini&quot;
        )
</code></pre>
<h3 id="2-api-key-management">2. API Key Management</h3>
<p>Securely manage API keys:</p>
<pre><code class="language-python">import os
from pathlib import Path

def get_api_key(provider):
    &quot;&quot;&quot;Securely retrieve API keys&quot;&quot;&quot;
    key_mapping = {
        &quot;openai&quot;: &quot;OPENAI_API_KEY&quot;,
        &quot;anthropic&quot;: &quot;ANTHROPIC_API_KEY&quot;,
        &quot;deepseek&quot;: &quot;DEEPSEEK_API_KEY&quot;
    }

    env_var = key_mapping.get(provider)
    if not env_var:
        raise ValueError(f&quot;Unknown provider: {provider}&quot;)

    api_key = os.getenv(env_var)
    if not api_key:
        raise ValueError(f&quot;Missing {env_var} environment variable&quot;)

    return api_key

# Usage
try:
    openai_config = graphbit.LlmConfig.openai(
        api_key=get_api_key(&quot;openai&quot;)
    )
except ValueError as e:
    print(f&quot;Configuration error: {e}&quot;)
</code></pre>
<h3 id="3-client-reuse">3. Client Reuse</h3>
<p>Reuse clients for better performance:</p>
<pre><code class="language-python">class LLMManager:
    def __init__(self):
        self.clients = {}

    def get_client(self, provider, model=None):
        &quot;&quot;&quot;Get or create client for provider&quot;&quot;&quot;
        key = f&quot;{provider}_{model or 'default'}&quot;

        if key not in self.clients:
            if provider == &quot;openai&quot;:
                config = graphbit.LlmConfig.openai(
                    api_key=get_api_key(&quot;openai&quot;),
                    model=model
                )
            elif provider == &quot;anthropic&quot;:
                config = graphbit.LlmConfig.anthropic(
                    api_key=get_api_key(&quot;anthropic&quot;),
                    model=model
                )
            elif provider == &quot;deepseek&quot;:
                config = graphbit.LlmConfig.deepseek(
                    api_key=get_api_key(&quot;deepseek&quot;),
                    model=model
                )
            elif provider == &quot;ollama&quot;:
                config = graphbit.LlmConfig.ollama(model=model)
            else:
                raise ValueError(f&quot;Unknown provider: {provider}&quot;)

            self.clients[key] = graphbit.LlmClient(config)

        return self.clients[key]

# Usage
llm_manager = LLMManager()
openai_client = llm_manager.get_client(&quot;openai&quot;, &quot;gpt-4o-mini&quot;)
</code></pre>
<h3 id="4-monitoring-and-logging">4. Monitoring and Logging</h3>
<p>Monitor LLM usage and performance:</p>
<pre><code class="language-python">def monitor_llm_usage(client, operation_name):
    &quot;&quot;&quot;Monitor LLM client usage&quot;&quot;&quot;
    stats_before = client.get_stats()

    # Perform operation here

    stats_after = client.get_stats()

    requests_made = stats_after['total_requests'] - stats_before['total_requests']
    print(f&quot;{operation_name}: {requests_made} requests made&quot;)

    if stats_after['total_requests'] &gt; 0:
        success_rate = stats_after['successful_requests'] / stats_after['total_requests']
        print(f&quot;Overall success rate: {success_rate:.2%}&quot;)
</code></pre>
<h2 id="whats-next">What's Next</h2>
<ul>
<li>Learn about <a href="../embeddings/">Embeddings</a> for vector operations</li>
<li>Explore <a href="../workflow-builder/">Workflow Builder</a> for complex workflows</li>
<li>Check <a href="../performance/">Performance</a> for optimization techniques</li>
<li>See <a href="../monitoring/">Monitoring</a> for production monitoring</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.50899def.min.js"></script>
      
    
  </body>
</html>