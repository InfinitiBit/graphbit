
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../data-processing/">
      
      
        <link rel="next" href="../semantic-search/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>LLM Integration - GraphBits AI</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#llm-integration-and-advanced-usage" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="GraphBits AI" class="md-header__button md-logo" aria-label="GraphBits AI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            GraphBits AI
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LLM Integration
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="GraphBits AI" class="md-nav__button md-logo" aria-label="GraphBits AI" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    GraphBits AI
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Getting Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started/quickstart/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quickstart
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started/installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting-started/examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    User Guide
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            User Guide
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/concepts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Concepts
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/agents/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Agents
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/async-vs-sync/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Async vs Sync
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/dynamics-graph/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dynamics Graph
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/llm-providers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Providers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/monitoring/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Monitoring
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/performance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Performance
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/reliability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reliability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/text-splitters/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Text Splitters
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/validation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Validation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../user-guide/workflow-builder/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Workflow Builder
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/configuration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/node-types/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Node Types
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api-reference/python-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python API
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Connector Integrations
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Connector Integrations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/aws_boto3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AWS Boto3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/chromadb_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ChromaDB Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/faiss_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    FAISS Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/google_search_api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Google Search API
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/mariadb_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MariaDB Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/milvus_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Milvus Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/mongodb/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MongoDB
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/pgvector/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PGVector
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/pinecone_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pinecone Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/qdrant_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Qdrant Integration
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../connector/weaviate_integration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Weaviate Integration
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Development
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Development
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/contributing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contributing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/debugging/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Debugging
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../development/python-bindings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Python Bindings
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" checked>
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Examples
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../comprehensive-pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Comprehensive Pipeline
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../content-generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Content Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data-processing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Processing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    LLM Integration
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    LLM Integration
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#complete-llm-client-example" class="md-nav__link">
    <span class="md-ellipsis">
      Complete LLM Client Example
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simplified-examples" class="md-nav__link">
    <span class="md-ellipsis">
      Simplified Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Simplified Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quick-openai-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Quick OpenAI Integration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-ollama-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Local Ollama Integration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic-claude-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic Claude Integration
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#configuration-management" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration Management
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#provider-flexibility" class="md-nav__link">
    <span class="md-ellipsis">
      Provider Flexibility
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monitoring-and-reliability" class="md-nav__link">
    <span class="md-ellipsis">
      Monitoring and Reliability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../semantic-search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Semantic Search
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#complete-llm-client-example" class="md-nav__link">
    <span class="md-ellipsis">
      Complete LLM Client Example
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#simplified-examples" class="md-nav__link">
    <span class="md-ellipsis">
      Simplified Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Simplified Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quick-openai-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Quick OpenAI Integration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-ollama-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Local Ollama Integration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#anthropic-claude-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Anthropic Claude Integration
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#configuration-management" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration Management
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Features">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#provider-flexibility" class="md-nav__link">
    <span class="md-ellipsis">
      Provider Flexibility
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monitoring-and-reliability" class="md-nav__link">
    <span class="md-ellipsis">
      Monitoring and Reliability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="llm-integration-and-advanced-usage">LLM Integration and Advanced Usage</h1>
<p>This example demonstrates comprehensive LLM integration with GraphBit, showcasing various providers, execution modes, and advanced features.</p>
<h2 id="overview">Overview</h2>
<p>We'll explore:
1. <strong>Multiple LLM Providers</strong>: OpenAI, Anthropic, HuggingFace, Ollama
2. <strong>Execution Modes</strong>: Sync, async, batch, streaming
3. <strong>Performance Optimization</strong>: High-throughput, low-latency, memory-optimized
4. <strong>Error Handling</strong>: Resilience patterns and fallbacks
5. <strong>Monitoring</strong>: Performance metrics and health checks</p>
<h2 id="complete-llm-client-example">Complete LLM Client Example</h2>
<pre><code class="language-python">import graphbit
import os
import asyncio
import time
from typing import List, Dict, Optional

class AdvancedLLMSystem:
    def __init__(self):
        &quot;&quot;&quot;Initialize the advanced LLM system.&quot;&quot;&quot;
        # Initialize GraphBit
        graphbit.init(enable_tracing=True)

        # Store multiple provider clients
        self.clients = {}
        self.initialize_providers()

    def initialize_providers(self):
        &quot;&quot;&quot;Initialize all available LLM providers.&quot;&quot;&quot;
        print(&quot;Initializing LLM providers...&quot;)

        # OpenAI client
        if os.getenv(&quot;OPENAI_API_KEY&quot;):
            openai_config = graphbit.LlmConfig.openai(
                api_key=os.getenv(&quot;OPENAI_API_KEY&quot;),
                model=&quot;gpt-4o-mini&quot;
            )
            self.clients['openai'] = graphbit.LlmClient(openai_config, debug=True)
            print(&quot;OpenAI client initialized&quot;)

        # Anthropic client
        if os.getenv(&quot;ANTHROPIC_API_KEY&quot;):
            anthropic_config = graphbit.LlmConfig.anthropic(
                api_key=os.getenv(&quot;ANTHROPIC_API_KEY&quot;),
                model=&quot;claude-3-5-sonnet-20241022&quot;
            )
            self.clients['anthropic'] = graphbit.LlmClient(anthropic_config, debug=True)
            print(&quot;Anthropic client initialized&quot;)

        # DeepSeek client
        if os.getenv(&quot;DEEPSEEK_API_KEY&quot;):
            deepseek_config = graphbit.LlmConfig.deepseek(
                api_key=os.getenv(&quot;DEEPSEEK_API_KEY&quot;),
                model=&quot;deepseek-chat&quot;
            )
            self.clients['deepseek'] = graphbit.LlmClient(deepseek_config, debug=True)
            print(&quot;DeepSeek client initialized&quot;)

        # HuggingFace client
        if os.getenv(&quot;HUGGINGFACE_API_KEY&quot;):
            huggingface_config = graphbit.LlmConfig.huggingface(
                api_key=os.getenv(&quot;HUGGINGFACE_API_KEY&quot;),
                model=&quot;microsoft/DialoGPT-medium&quot;
            )
            self.clients['huggingface'] = graphbit.LlmClient(huggingface_config, debug=True)
            print(&quot;HuggingFace client initialized&quot;)

        # Ollama client (no API key required)
        try:
            ollama_config = graphbit.LlmConfig.ollama(&quot;llama3.2&quot;)
            self.clients['ollama'] = graphbit.LlmClient(ollama_config, debug=True)
            print(&quot;Ollama client initialized&quot;)
        except Exception as e:
            print(f&quot;Ollama client failed: {e}&quot;)

        if not self.clients:
            raise Exception(&quot;No LLM providers available. Please set API keys or install Ollama.&quot;)

    def test_basic_completion(self, provider: str = 'openai'):
        &quot;&quot;&quot;Test basic text completion.&quot;&quot;&quot;
        if provider not in self.clients:
            print(f&quot;Provider '{provider}' not available&quot;)
            return None

        client = self.clients[provider]
        prompt = &quot;Explain quantum computing in simple terms.&quot;

        print(f&quot;\nTesting basic completion with {provider}...&quot;)
        print(f&quot;Prompt: {prompt}&quot;)

        try:
            start_time = time.time()
            response = client.complete(
                prompt=prompt,
                max_tokens=200,
                temperature=0.7
            )
            duration = (time.time() - start_time) * 1000

            print(f&quot;Completed in {duration:.2f}ms&quot;)
            print(f&quot;Response: {response[:200]}...&quot;)

            return response
        except Exception as e:
            print(f&quot;Completion failed: {e}&quot;)
            return None

    async def test_async_completion(self, provider: str = 'openai'):
        &quot;&quot;&quot;Test asynchronous completion.&quot;&quot;&quot;
        if provider not in self.clients:
            print(f&quot;Provider '{provider}' not available&quot;)
            return None

        client = self.clients[provider]
        prompt = &quot;Write a haiku about artificial intelligence.&quot;

        print(f&quot;\nTesting async completion with {provider}...&quot;)
        print(f&quot;Prompt: {prompt}&quot;)

        try:
            start_time = time.time()
            response = await client.complete_async(
                prompt=prompt,
                max_tokens=100,
                temperature=0.8
            )
            duration = (time.time() - start_time) * 1000

            print(f&quot;Async completed in {duration:.2f}ms&quot;)
            print(f&quot;Response: {response}&quot;)

            return response
        except Exception as e:
            print(f&quot;Async completion failed: {e}&quot;)
            return None

    async def test_batch_completion(self, provider: str = 'openai'):
        &quot;&quot;&quot;Test batch completion for multiple prompts.&quot;&quot;&quot;
        if provider not in self.clients:
            print(f&quot;Provider '{provider}' not available&quot;)
            return None

        client = self.clients[provider]
        prompts = [
            &quot;What is machine learning?&quot;,
            &quot;Explain neural networks briefly.&quot;,
            &quot;What are the benefits of cloud computing?&quot;,
            &quot;How does blockchain work?&quot;,
            &quot;What is the future of AI?&quot;
        ]

        print(f&quot;\nTesting batch completion with {provider}...&quot;)
        print(f&quot;Processing {len(prompts)} prompts...&quot;)

        try:
            start_time = time.time()
            responses = await client.complete_batch(
                prompts=prompts,
                max_tokens=100,
                temperature=0.6,
                max_concurrency=3
            )
            duration = (time.time() - start_time) * 1000

            print(f&quot;Batch completed in {duration:.2f}ms&quot;)
            print(f&quot;Average per prompt: {duration/len(prompts):.2f}ms&quot;)

            for i, (prompt, response) in enumerate(zip(prompts, responses)):
                print(f&quot;\n{i+1}. {prompt}&quot;)
                print(f&quot;   → {response[:100]}...&quot;)

            return responses
        except Exception as e:
            print(f&quot;Batch completion failed: {e}&quot;)
            return None

    async def test_chat_optimized(self, provider: str = 'openai'):
        &quot;&quot;&quot;Test optimized chat completion.&quot;&quot;&quot;
        if provider not in self.clients:
            print(f&quot;Provider '{provider}' not available&quot;)
            return None

        client = self.clients[provider]
        messages = [
            (&quot;system&quot;, &quot;You are a helpful AI assistant specialized in technology.&quot;),
            (&quot;user&quot;, &quot;What's the difference between AI and ML?&quot;),
            (&quot;assistant&quot;, &quot;AI is the broader concept of machines being able to carry out tasks in a smart way, while ML is a specific subset of AI that involves training algorithms on data.&quot;),
            (&quot;user&quot;, &quot;Can you give me a practical example?&quot;)
        ]

        print(f&quot;\nTesting chat-optimized completion with {provider}...&quot;)

        try:
            start_time = time.time()
            response = await client.chat_optimized(
                messages=messages,
                max_tokens=150,
                temperature=0.7
            )
            duration = (time.time() - start_time) * 1000

            print(f&quot;Chat completed in {duration:.2f}ms&quot;)
            print(f&quot;Response: {response}&quot;)

            return response
        except Exception as e:
            print(f&quot;Chat completion failed: {e}&quot;)
            return None

    async def test_streaming_completion(self, provider: str = 'openai'):
        &quot;&quot;&quot;Test streaming completion.&quot;&quot;&quot;
        if provider not in self.clients:
            print(f&quot;Provider '{provider}' not available&quot;)
            return None

        client = self.clients[provider]
        prompt = &quot;Write a detailed explanation of how machine learning works, covering the key concepts step by step.&quot;

        print(f&quot;\nTesting streaming completion with {provider}...&quot;)
        print(f&quot;Prompt: {prompt}&quot;)
        print(&quot;Streaming response:&quot;)

        try:
            start_time = time.time()

            # Note: Streaming returns an async iterator
            stream = await client.complete_stream(
                prompt=prompt,
                max_tokens=300,
                temperature=0.7
            )

            full_response = &quot;&quot;
            async for chunk in stream:
                print(chunk, end='', flush=True)
                full_response += chunk

            duration = (time.time() - start_time) * 1000
            print(f&quot;\n\nStreaming completed in {duration:.2f}ms&quot;)
            print(f&quot;Total tokens: ~{len(full_response.split())}&quot;)

            return full_response
        except Exception as e:
            print(f&quot;Streaming completion failed: {e}&quot;)
            return None

    def test_client_warmup(self, provider: str = 'openai'):
        &quot;&quot;&quot;Test client warmup for improved performance.&quot;&quot;&quot;
        if provider not in self.clients:
            print(f&quot;Provider '{provider}' not available&quot;)
            return

        client = self.clients[provider]

        print(f&quot;\nTesting client warmup with {provider}...&quot;)

        try:
            # Warmup the client
            start_time = time.time()
            asyncio.run(client.warmup())
            warmup_duration = (time.time() - start_time) * 1000

            print(f&quot;Warmup completed in {warmup_duration:.2f}ms&quot;)

            # Test performance after warmup
            start_time = time.time()
            response = client.complete(&quot;Quick test after warmup&quot;, max_tokens=50)
            completion_duration = (time.time() - start_time) * 1000

            print(f&quot;Post-warmup completion: {completion_duration:.2f}ms&quot;)

        except Exception as e:
            print(f&quot;Warmup failed: {e}&quot;)

    def get_client_statistics(self, provider: str = 'openai'):
        &quot;&quot;&quot;Get detailed client statistics.&quot;&quot;&quot;
        if provider not in self.clients:
            print(f&quot;Provider '{provider}' not available&quot;)
            return None

        client = self.clients[provider]

        print(f&quot;\nGetting statistics for {provider}...&quot;)

        try:
            stats = client.get_stats()

            print(f&quot;Client Statistics for {provider}:&quot;)
            for key, value in stats.items():
                if isinstance(value, float):
                    print(f&quot;  {key}: {value:.3f}&quot;)
                else:
                    print(f&quot;  {key}: {value}&quot;)

            return stats
        except Exception as e:
            print(f&quot;Failed to get statistics: {e}&quot;)
            return None

    def reset_client_statistics(self, provider: str = 'openai'):
        &quot;&quot;&quot;Reset client statistics.&quot;&quot;&quot;
        if provider not in self.clients:
            print(f&quot;Provider '{provider}' not available&quot;)
            return

        client = self.clients[provider]

        try:
            client.reset_stats()
            print(f&quot;Statistics reset for {provider}&quot;)
        except Exception as e:
            print(f&quot;Failed to reset statistics: {e}&quot;)

    def compare_providers(self, prompt: str = &quot;Explain the concept of recursion in programming.&quot;):
        &quot;&quot;&quot;Compare responses from all available providers.&quot;&quot;&quot;
        print(f&quot;\nComparing providers...&quot;)
        print(f&quot;Prompt: {prompt}&quot;)

        results = {}

        for provider_name, client in self.clients.items():
            print(f&quot;\n--- Testing {provider_name} ---&quot;)
            try:
                start_time = time.time()
                response = client.complete(
                    prompt=prompt,
                    max_tokens=150,
                    temperature=0.7
                )
                duration = (time.time() - start_time) * 1000

                results[provider_name] = {
                    'response': response,
                    'duration_ms': duration,
                    'success': True
                }

                print(f&quot;{provider_name}: {duration:.2f}ms&quot;)
                print(f&quot;Response: {response[:100]}...&quot;)

            except Exception as e:
                results[provider_name] = {
                    'error': str(e),
                    'success': False
                }
                print(f&quot;{provider_name}: {e}&quot;)

        return results

# Performance-optimized clients
def create_performance_optimized_clients():
    &quot;&quot;&quot;Create clients optimized for different performance characteristics.&quot;&quot;&quot;

    graphbit.init()

    api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)
    if not api_key:
        print(&quot;OpenAI API key required for performance tests&quot;)
        return None

    # High-throughput client
    high_throughput_config = graphbit.LlmConfig.openai(api_key, &quot;gpt-4o-mini&quot;)
    high_throughput_client = graphbit.LlmClient(high_throughput_config, debug=False)

    # Low-latency client  
    low_latency_config = graphbit.LlmConfig.openai(api_key, &quot;gpt-4o-mini&quot;)
    low_latency_client = graphbit.LlmClient(low_latency_config, debug=False)

    return {
        'high_throughput': high_throughput_client,
        'low_latency': low_latency_client
    }

async def performance_benchmark():
    &quot;&quot;&quot;Benchmark different client configurations.&quot;&quot;&quot;

    clients = create_performance_optimized_clients()
    if not clients:
        return

    test_prompt = &quot;Summarize the benefits of renewable energy in one paragraph.&quot;

    print(&quot;\n🏃 Performance Benchmark&quot;)
    print(&quot;=&quot; * 50)

    for config_name, client in clients.items():
        print(f&quot;\nTesting {config_name} configuration...&quot;)

        # Single completion test
        start_time = time.time()
        try:
            response = client.complete(test_prompt, max_tokens=100)
            single_duration = (time.time() - start_time) * 1000
            print(f&quot;Single completion: {single_duration:.2f}ms&quot;)
        except Exception as e:
            print(f&quot;Single completion failed: {e}&quot;)
            continue

        # Batch test
        batch_prompts = [test_prompt] * 5
        start_time = time.time()
        try:
            batch_responses = await client.complete_batch(
                batch_prompts,
                max_tokens=100,
                max_concurrency=3
            )
            batch_duration = (time.time() - start_time) * 1000
            avg_per_prompt = batch_duration / len(batch_prompts)
            print(f&quot;Batch completion (5 prompts): {batch_duration:.2f}ms total, {avg_per_prompt:.2f}ms avg&quot;)
        except Exception as e:
            print(f&quot;Batch completion failed: {e}&quot;)

        # Get statistics
        try:
            stats = client.get_stats()
            print(f&quot;Stats: {stats.get('total_requests', 0)} requests, &quot;
                  f&quot;{stats.get('average_response_time_ms', 0):.2f}ms avg response time&quot;)
        except:
            pass

# Error handling and resilience examples
async def test_error_handling():
    &quot;&quot;&quot;Test error handling and resilience features.&quot;&quot;&quot;

    graphbit.init()

    # Test with invalid API key
    print(&quot;\nTesting Error Handling&quot;)
    print(&quot;=&quot; * 40)

    try:
        invalid_config = graphbit.LlmConfig.openai(&quot;invalid-key&quot;, &quot;gpt-4o-mini&quot;)
        invalid_client = graphbit.LlmClient(invalid_config)

        print(&quot;Testing with invalid API key...&quot;)
        response = invalid_client.complete(&quot;Test prompt&quot;, max_tokens=50)
        print(&quot;Expected error but got response&quot;)
    except Exception as e:
        print(f&quot;Correctly handled invalid API key: {type(e).__name__}&quot;)

    # Test timeout handling
    if os.getenv(&quot;OPENAI_API_KEY&quot;):
        try:
            config = graphbit.LlmConfig.openai(os.getenv(&quot;OPENAI_API_KEY&quot;), &quot;gpt-4o-mini&quot;)
            client = graphbit.LlmClient(config)

            print(&quot;\nTesting very long prompt (potential timeout)...&quot;)
            very_long_prompt = &quot;Write a comprehensive essay about &quot; + &quot;technology &quot; * 1000

            response = client.complete(very_long_prompt, max_tokens=2000)
            print(&quot;Long prompt handled successfully&quot;)
        except Exception as e:
            print(f&quot;Timeout/limit handled: {type(e).__name__}&quot;)

# System health monitoring
def monitor_llm_system_health():
    &quot;&quot;&quot;Monitor LLM system health and performance.&quot;&quot;&quot;

    graphbit.init()

    print(&quot;\nSystem Health Check&quot;)
    print(&quot;=&quot; * 40)

    # Check GraphBit health
    health = graphbit.health_check()
    print(&quot;GraphBit Health:&quot;)
    for key, value in health.items():
        status = &quot;Ok!&quot; if value else &quot;Not Ok!&quot;
        print(f&quot;  {status} {key}: {value}&quot;)

    # Get system information
    info = graphbit.get_system_info()
    print(f&quot;\nSystem Information:&quot;)
    print(f&quot;  Version: {info.get('version', 'unknown')}&quot;)
    print(f&quot;  Runtime threads: {info.get('runtime_worker_threads', 'unknown')}&quot;)
    print(f&quot;  Memory allocator: {info.get('memory_allocator', 'unknown')}&quot;)

    # Test provider connectivity
    print(f&quot;\nProvider Connectivity:&quot;)

    providers_to_test = [
        ('OpenAI', lambda: graphbit.LlmConfig.openai(os.getenv(&quot;OPENAI_API_KEY&quot;, &quot;test&quot;), &quot;gpt-4o-mini&quot;)),
        ('Anthropic', lambda: graphbit.LlmConfig.anthropic(os.getenv(&quot;ANTHROPIC_API_KEY&quot;, &quot;test&quot;), &quot;claude-3-5-sonnet-20241022&quot;)),
        ('Ollama', lambda: graphbit.LlmConfig.ollama(&quot;llama3.2&quot;))
    ]

    for provider_name, config_func in providers_to_test:
        try:
            config = config_func()
            client = graphbit.LlmClient(config)
            print(f&quot;  {provider_name}: Configuration valid&quot;)
        except Exception as e:
            print(f&quot;  {provider_name}: {str(e)[:50]}...&quot;)

# Example usage
async def main():
    &quot;&quot;&quot;Run comprehensive LLM system demonstration.&quot;&quot;&quot;

    print(&quot;GraphBit LLM Integration Demo&quot;)
    print(&quot;=&quot; * 60)

    try:
        # Initialize system
        llm_system = AdvancedLLMSystem()

        # Test basic functionality
        for provider in llm_system.clients.keys():
            llm_system.test_basic_completion(provider)
            await llm_system.test_async_completion(provider)
            break  # Just test first available provider for demo

        # Test advanced features with primary provider
        primary_provider = list(llm_system.clients.keys())[0]

        await llm_system.test_batch_completion(primary_provider)
        await llm_system.test_chat_optimized(primary_provider)

        # Test performance features
        llm_system.test_client_warmup(primary_provider)
        llm_system.get_client_statistics(primary_provider)

        # Compare providers if multiple available
        if len(llm_system.clients) &gt; 1:
            llm_system.compare_providers()

        # Performance benchmark
        await performance_benchmark()

        # Error handling tests
        await test_error_handling()

        # System health check
        monitor_llm_system_health()

        print(&quot;\nDemo completed successfully!&quot;)

    except Exception as e:
        print(f&quot;\nDemo failed: {e}&quot;)

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
</code></pre>
<h2 id="simplified-examples">Simplified Examples</h2>
<h3 id="quick-openai-integration">Quick OpenAI Integration</h3>
<pre><code class="language-python">import graphbit
import os

def quick_openai_example():
    &quot;&quot;&quot;Simple OpenAI integration example.&quot;&quot;&quot;

    # Initialize
    graphbit.init()

    # Configure and create client
    config = graphbit.LlmConfig.openai(
        api_key=os.getenv(&quot;OPENAI_API_KEY&quot;),
        model=&quot;gpt-4o-mini&quot;
    )
    client = graphbit.LlmClient(config)

    # Simple completion
    response = client.complete(
        &quot;Explain quantum computing in 3 sentences.&quot;,
        max_tokens=100,
        temperature=0.7
    )

    print(f&quot;Response: {response}&quot;)

    # Get statistics
    stats = client.get_stats()
    print(f&quot;Total requests: {stats.get('total_requests', 0)}&quot;)

# Usage
quick_openai_example()
</code></pre>
<h3 id="local-ollama-integration">Local Ollama Integration</h3>
<pre><code class="language-python">import graphbit

def quick_ollama_example():
    &quot;&quot;&quot;Simple Ollama integration example.&quot;&quot;&quot;

    # Initialize
    graphbit.init()

    # Configure Ollama (no API key needed)
    config = graphbit.LlmConfig.ollama(&quot;llama3.2&quot;)
    client = graphbit.LlmClient(config, debug=True)

    # Test completion
    try:
        response = client.complete(
            &quot;What are the benefits of local AI models?&quot;,
            max_tokens=150,
            temperature=0.8
        )
        print(f&quot;Ollama response: {response}&quot;)
    except Exception as e:
        print(f&quot;Ollama error (make sure Ollama is running): {e}&quot;)

# Usage
quick_ollama_example()
</code></pre>
<h3 id="anthropic-claude-integration">Anthropic Claude Integration</h3>
<pre><code class="language-python">import graphbit
import os

def quick_anthropic_example():
    &quot;&quot;&quot;Simple Anthropic Claude integration example.&quot;&quot;&quot;

    # Initialize
    graphbit.init()

    # Configure Anthropic
    config = graphbit.LlmConfig.anthropic(
        api_key=os.getenv(&quot;ANTHROPIC_API_KEY&quot;),
        model=&quot;claude-3-5-sonnet-20241022&quot;
    )
    client = graphbit.LlmClient(config)

    # Complex reasoning task
    response = client.complete(
        &quot;&quot;&quot;Analyze the pros and cons of remote work from both 
        employee and employer perspectives. Be balanced and thorough.&quot;&quot;&quot;,
        max_tokens=300,
        temperature=0.6
    )

    print(f&quot;Claude's analysis: {response}&quot;)

# Usage (requires ANTHROPIC_API_KEY)
# quick_anthropic_example()
</code></pre>
<h2 id="best-practices">Best Practices</h2>
<h3 id="configuration-management">Configuration Management</h3>
<pre><code class="language-python">def setup_production_llm_config():
    &quot;&quot;&quot;Set up production-ready LLM configuration.&quot;&quot;&quot;

    graphbit.init(log_level=&quot;warn&quot;, enable_tracing=False)

    # Primary provider with fallback
    providers = []

    if os.getenv(&quot;OPENAI_API_KEY&quot;):
        providers.append(('openai', graphbit.LlmConfig.openai(
            os.getenv(&quot;OPENAI_API_KEY&quot;),
            &quot;gpt-4o-mini&quot;
        )))

    if os.getenv(&quot;ANTHROPIC_API_KEY&quot;):
        providers.append(('anthropic', graphbit.LlmConfig.anthropic(
            os.getenv(&quot;ANTHROPIC_API_KEY&quot;),
            &quot;claude-3-5-sonnet-20241022&quot;
        )))

    # Add local fallback
    try:
        providers.append(('ollama', graphbit.LlmConfig.ollama(&quot;llama3.2&quot;)))
    except:
        pass

    if not providers:
        raise Exception(&quot;No LLM providers configured&quot;)

    return providers

def robust_completion(prompt: str, max_retries: int = 3):
    &quot;&quot;&quot;Completion with provider fallback.&quot;&quot;&quot;

    providers = setup_production_llm_config()

    for provider_name, config in providers:
        for attempt in range(max_retries):
            try:
                client = graphbit.LlmClient(config, debug=False)
                return client.complete(prompt, max_tokens=200)
            except Exception as e:
                print(f&quot;Attempt {attempt + 1} with {provider_name} failed: {e}&quot;)
                if attempt == max_retries - 1:
                    continue  # Try next provider
                time.sleep(2 ** attempt)  # Exponential backoff

    raise Exception(&quot;All providers failed&quot;)
</code></pre>
<h2 id="key-features">Key Features</h2>
<h3 id="provider-flexibility">Provider Flexibility</h3>
<ul>
<li><strong>Multiple Providers</strong>: OpenAI, Anthropic, Ollama support</li>
<li><strong>Easy Switching</strong>: Consistent API across providers</li>
<li><strong>Fallback Support</strong>: Automatic provider failover</li>
</ul>
<h3 id="performance-optimization">Performance Optimization</h3>
<ul>
<li><strong>Async Operations</strong>: Non-blocking completions</li>
<li><strong>Batch Processing</strong>: Efficient multiple prompt handling</li>
<li><strong>Streaming</strong>: Real-time response streaming</li>
<li><strong>Client Warmup</strong>: Improved initial response times</li>
</ul>
<h3 id="monitoring-and-reliability">Monitoring and Reliability</h3>
<ul>
<li><strong>Statistics Tracking</strong>: Detailed performance metrics</li>
<li><strong>Health Checks</strong>: System health monitoring</li>
<li><strong>Error Handling</strong>: Comprehensive error management</li>
<li><strong>Resilience Patterns</strong>: Circuit breakers and retry logic</li>
</ul>
<p>This example demonstrates GraphBit's comprehensive LLM integration capabilities for building production-ready AI applications. </p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.50899def.min.js"></script>
      
    
  </body>
</html>