{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GraphBit Documentation","text":"<p>Welcome to the comprehensive documentation for GraphBit - a high-performance AI agent workflow automation framework that combines Rust's performance with Python's ease of use.</p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ul> <li>Installation Guide - Install GraphBit on your system</li> <li>Quick Start Tutorial - Build your first workflow in 5 minutes</li> <li>Basic Examples - Simple examples to get you started</li> </ul>"},{"location":"#user-guide","title":"\ud83d\udcda User Guide","text":"<ul> <li>Core Concepts - Understand workflows, agents, and nodes</li> <li>Workflow Builder - Creating and connecting workflow nodes</li> <li>Agent Configuration - Setting up AI agents with different capabilities</li> <li>Text Splitters - Processing large documents with various splitting strategies</li> <li>LLM Providers - Working with OpenAI, Anthropic, Ollama, and more</li> <li>Dynamic Graph Generation - Auto-generating workflow structures</li> <li>Data Validation - Input validation and data quality checks</li> <li>Performance Optimization - Tuning for speed and efficiency</li> <li>Monitoring &amp; Observability - Metrics collection and debugging</li> <li>Embeddings &amp; Vector Search - Text embeddings and similarity search</li> <li>Reliability &amp; Fault Tolerance - Circuit breakers, retries, and error handling</li> </ul>"},{"location":"#api-reference","title":"\ud83d\udd27 API Reference","text":"<ul> <li>Python API - Complete Python API documentation</li> <li>Configuration Options - All configuration parameters</li> <li>Node Types - Agent, condition, transform, and delay nodes</li> <li>Execution Patterns - Sync, async, batch, and concurrent execution</li> </ul>"},{"location":"#advanced-topics","title":"\ud83c\udfaf Advanced Topics","text":"<ul> <li>Custom Extensions - Building custom node types and providers</li> <li>Plugin Development - Creating GraphBit plugins</li> <li>Advanced Patterns - Complex workflow design patterns</li> <li>Integration Guides - Connecting with external systems</li> </ul>"},{"location":"#development","title":"\ud83d\udee0\ufe0f Development","text":"<ul> <li>Architecture Overview - System design and components</li> <li>Contributing Guide - How to contribute to GraphBit</li> <li>Building from Source - Development setup and compilation</li> <li>Testing - Running tests and quality checks</li> </ul>"},{"location":"#examples-use-cases","title":"\ud83d\udccb Examples &amp; Use Cases","text":"<ul> <li>Content Generation Pipeline - Multi-agent content creation</li> <li>Data Processing Workflow - ETL pipelines with AI agents</li> <li>Code Review Automation - Automated code analysis</li> <li>Customer Support Bot - Multi-stage inquiry processing</li> </ul>"},{"location":"#what-is-graphbit","title":"What is GraphBit?","text":"<p>GraphBit is a declarative framework for building reliable AI agent workflows with strong type safety, comprehensive error handling, and predictable performance. It features:</p> <ul> <li>\ud83d\udd12 Type Safety - Strong typing throughout the execution pipeline</li> <li>\ud83d\udee1\ufe0f Reliability - Circuit breakers, retry policies, and error handling  </li> <li>\ud83e\udd16 Multi-LLM Support - OpenAI, Anthropic, Ollama, HuggingFace</li> <li>\u26a1 Performance - Rust core with Python bindings for optimal speed</li> <li>\ud83d\udcca Observability - Built-in metrics and execution tracing</li> <li>\ud83d\udd27 Resource Management - Concurrency controls and memory optimization</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>GraphBit uses a three-tier architecture:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Python API    \u2502  \u2190 PyO3 bindings with async support\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   CLI Tool      \u2502  \u2190 Project management and execution\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Rust Core     \u2502  \u2190 Workflow engine, agents, LLM providers\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: github.com/InfinitiBit/graphbit</li> <li>Issues: Report bugs and request features</li> <li>Discussions: Ask questions and share ideas</li> <li>Contributing: See our contributing guide</li> </ul>"},{"location":"#license","title":"License","text":"<p>GraphBit is released under the Proprietary License.</p> <p>Ready to build your first AI workflow? Start with our Quick Start Tutorial! </p>"},{"location":"api-reference/configuration/","title":"Configuration Options","text":"<p>GraphBit provides extensive configuration options to customize workflow execution, LLM providers, reliability features, and performance settings.</p>"},{"location":"api-reference/configuration/#library-initialization","title":"Library Initialization","text":""},{"location":"api-reference/configuration/#basic-initialization","title":"Basic Initialization","text":"<pre><code>import graphbit\n\n# Basic initialization\ngraphbit.init()\n</code></pre>"},{"location":"api-reference/configuration/#advanced-initialization","title":"Advanced Initialization","text":"<pre><code># With debugging and logging\ngraphbit.init(\n    log_level=\"info\",          # Log level: trace, debug, info, warn, error\n    enable_tracing=True,       # Enable detailed tracing\n    debug=True                 # Enable debug mode (alias for enable_tracing)\n)\n</code></pre>"},{"location":"api-reference/configuration/#runtime-configuration","title":"Runtime Configuration","text":"<p>Configure the runtime before initialization for advanced control:</p> <pre><code># Configure runtime (call before init)\ngraphbit.configure_runtime(\n    worker_threads=8,          # Number of worker threads\n    max_blocking_threads=16,   # Maximum blocking threads\n    thread_stack_size_mb=8     # Thread stack size in MB\n)\n\n# Then initialize\ngraphbit.init()\n</code></pre>"},{"location":"api-reference/configuration/#llm-configuration","title":"LLM Configuration","text":""},{"location":"api-reference/configuration/#openai-configuration","title":"OpenAI Configuration","text":"<pre><code># Basic OpenAI configuration\nconfig = graphbit.LlmConfig.openai(\n    api_key=\"your-api-key\",\n    model=\"gpt-4o-mini\"        # Optional, defaults to gpt-4o-mini\n)\n\n# With default model\nconfig = graphbit.LlmConfig.openai(\"your-api-key\")\n</code></pre>"},{"location":"api-reference/configuration/#anthropic-configuration","title":"Anthropic Configuration","text":"<pre><code># Basic Anthropic configuration\nconfig = graphbit.LlmConfig.anthropic(\n    api_key=\"your-anthropic-key\",\n    model=\"claude-3-5-sonnet-20241022\"  # Optional, defaults to claude-3-5-sonnet-20241022\n)\n\n# With default model\nconfig = graphbit.LlmConfig.anthropic(\"your-anthropic-key\")\n</code></pre>"},{"location":"api-reference/configuration/#perplexity-configuration","title":"Perplexity Configuration","text":"<pre><code># Basic Perplexity configuration\nconfig = graphbit.LlmConfig.perplexity(\n    api_key=\"your-perplexity-key\",\n    model=\"sonar\"             # Optional, defaults to sonar\n)\n\n# With default model\nconfig = graphbit.LlmConfig.perplexity(\"your-perplexity-key\")\n=======\n### DeepSeek Configuration\n\n```python\n# Basic DeepSeek configuration\nconfig = graphbit.LlmConfig.deepseek(\n    api_key=\"your-deepseek-key\",\n    model=\"deepseek-chat\"        # Optional, defaults to deepseek-chat\n)\n\n# With default model\nconfig = graphbit.LlmConfig.deepseek(\"your-deepseek-key\")\n\n# Different models for specific use cases\ncoding_config = graphbit.LlmConfig.deepseek(\"your-deepseek-key\", \"deepseek-coder\")\nreasoning_config = graphbit.LlmConfig.deepseek(\"your-deepseek-key\", \"deepseek-reasoner\")\n</code></pre>"},{"location":"api-reference/configuration/#ollama-configuration","title":"Ollama Configuration","text":"<pre><code># Local Ollama configuration\nconfig = graphbit.LlmConfig.ollama(\n    model=\"llama3.2\"          # Optional, defaults to llama3.2\n)\n\n# With default model\nconfig = graphbit.LlmConfig.ollama()\n</code></pre>"},{"location":"api-reference/configuration/#configuration-properties","title":"Configuration Properties","text":"<pre><code># Access configuration properties\nprovider = config.provider()  # \"openai\", \"anthropic\", \"perplexity\", \"ollama\"\n=======\nprovider = config.provider()  # \"openai\", \"anthropic\", \"deepseek\", \"ollama\"\nmodel = config.model()        # Model name\n</code></pre>"},{"location":"api-reference/configuration/#llm-client-configuration","title":"LLM Client Configuration","text":""},{"location":"api-reference/configuration/#basic-client","title":"Basic Client","text":"<pre><code># Simple client\nclient = graphbit.LlmClient(config)\n</code></pre>"},{"location":"api-reference/configuration/#client-with-debug-mode","title":"Client with Debug Mode","text":"<pre><code># Client with debugging enabled\nclient = graphbit.LlmClient(config, debug=True)\n</code></pre>"},{"location":"api-reference/configuration/#client-statistics-and-monitoring","title":"Client Statistics and Monitoring","text":"<pre><code># Get performance statistics\nstats = client.get_stats()\nprint(f\"Total requests: {stats['total_requests']}\")\nprint(f\"Success rate: {stats['success_rate']}\")\nprint(f\"Average response time: {stats['average_response_time_ms']}ms\")\n\n# Reset statistics\nclient.reset_stats()\n\n# Warm up client for better performance\nimport asyncio\nasyncio.run(client.warmup())\n</code></pre>"},{"location":"api-reference/configuration/#executor-configuration","title":"Executor Configuration","text":""},{"location":"api-reference/configuration/#basic-executor","title":"Basic Executor","text":"<pre><code># Simple executor\nexecutor = graphbit.Executor(llm_config)\n</code></pre>"},{"location":"api-reference/configuration/#executor-with-options","title":"Executor with Options","text":"<pre><code># Executor with configuration\nexecutor = graphbit.Executor(\n    config=llm_config,\n    lightweight_mode=False,    # Enable lightweight/low-latency mode\n    timeout_seconds=300,       # Execution timeout (1-3600 seconds)\n    debug=True                 # Enable debug mode\n)\n</code></pre>"},{"location":"api-reference/configuration/#specialized-executors","title":"Specialized Executors","text":""},{"location":"api-reference/configuration/#high-throughput-executor","title":"High Throughput Executor","text":"<pre><code># Optimized for high throughput\nexecutor = graphbit.Executor.new_high_throughput(\n    llm_config,\n    timeout_seconds=600,       # Optional timeout override\n    debug=False                # Optional debug mode\n)\n</code></pre>"},{"location":"api-reference/configuration/#low-latency-executor","title":"Low Latency Executor","text":"<pre><code># Optimized for low latency\nexecutor = graphbit.Executor.new_low_latency(\n    llm_config,\n    timeout_seconds=30,        # Shorter timeout for low latency\n    debug=False\n)\n</code></pre>"},{"location":"api-reference/configuration/#memory-optimized-executor","title":"Memory Optimized Executor","text":"<pre><code># Optimized for memory usage\nexecutor = graphbit.Executor.new_memory_optimized(\n    llm_config,\n    timeout_seconds=300,\n    debug=False\n)\n</code></pre>"},{"location":"api-reference/configuration/#runtime-configuration_1","title":"Runtime Configuration","text":"<pre><code># Configure executor settings\nexecutor.configure(\n    timeout_seconds=600,       # Execution timeout (1-3600 seconds)\n    max_retries=5,            # Maximum retries (0-10)\n    enable_metrics=True,      # Enable performance metrics\n    debug=False               # Debug mode\n)\n\n# Legacy configuration methods\nexecutor.set_lightweight_mode(True)  # Enable lightweight mode\nis_lightweight = executor.is_lightweight_mode()  # Check mode\n</code></pre>"},{"location":"api-reference/configuration/#executor-statistics","title":"Executor Statistics","text":"<pre><code># Get execution statistics\nstats = executor.get_stats()\nprint(f\"Total executions: {stats['total_executions']}\")\nprint(f\"Success rate: {stats['success_rate']}\")\nprint(f\"Average duration: {stats['average_duration_ms']}ms\")\nprint(f\"Execution mode: {stats['execution_mode']}\")\n\n# Reset statistics\nexecutor.reset_stats()\n\n# Get current execution mode\nmode = executor.get_execution_mode()  # Returns: HighThroughput, LowLatency, etc.\n</code></pre>"},{"location":"api-reference/configuration/#embeddings-configuration","title":"Embeddings Configuration","text":""},{"location":"api-reference/configuration/#openai-embeddings","title":"OpenAI Embeddings","text":"<pre><code># OpenAI embeddings configuration\nembed_config = graphbit.EmbeddingConfig.openai(\n    api_key=\"your-api-key\",\n    model=\"text-embedding-3-small\"  # Optional, defaults to text-embedding-3-small\n)\n\n# With default model\nembed_config = graphbit.EmbeddingConfig.openai(\"your-api-key\")\n</code></pre>"},{"location":"api-reference/configuration/#huggingface-embeddings","title":"HuggingFace Embeddings","text":"<pre><code># HuggingFace embeddings configuration\nembed_config = graphbit.EmbeddingConfig.huggingface(\n    api_key=\"your-hf-token\",\n    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n</code></pre>"},{"location":"api-reference/configuration/#embeddings-client","title":"Embeddings Client","text":"<pre><code># Create embeddings client\nembed_client = graphbit.EmbeddingClient(embed_config)\n\n# Generate embeddings\nembedding = embed_client.embed(\"Hello world\")\nembeddings = embed_client.embed_many([\"Text 1\", \"Text 2\"])\n\n# Calculate similarity\nsimilarity = graphbit.EmbeddingClient.similarity(embedding1, embedding2)\n</code></pre>"},{"location":"api-reference/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"api-reference/configuration/#required-environment-variables","title":"Required Environment Variables","text":"<pre><code># OpenAI\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\n# HuggingFace\nexport HUGGINGFACE_API_KEY=\"your-huggingface-token\"\n</code></pre>"},{"location":"api-reference/configuration/#graphbit-specific-environment-variables","title":"GraphBit-Specific Environment Variables","text":"<pre><code># Runtime configuration\nexport GRAPHBIT_WORKER_THREADS=\"8\"\nexport GRAPHBIT_MAX_BLOCKING_THREADS=\"16\"\n\n# Logging\nexport GRAPHBIT_LOG_LEVEL=\"INFO\"\nexport GRAPHBIT_DEBUG=\"true\"\n</code></pre>"},{"location":"api-reference/configuration/#system-information-and-health","title":"System Information and Health","text":""},{"location":"api-reference/configuration/#system-information","title":"System Information","text":"<pre><code># Get comprehensive system information\ninfo = graphbit.get_system_info()\nprint(f\"Version: {info['version']}\")\nprint(f\"CPU count: {info['cpu_count']}\")\nprint(f\"Runtime initialized: {info['runtime_initialized']}\")\nprint(f\"Worker threads: {info['runtime_worker_threads']}\")\nprint(f\"Memory allocator: {info['memory_allocator']}\")\n</code></pre>"},{"location":"api-reference/configuration/#health-checks","title":"Health Checks","text":"<pre><code># Perform health check\nhealth = graphbit.health_check()\nif health['overall_healthy']:\n    print(\"\u2705 System is healthy\")\n    print(f\"Memory healthy: {health['memory_healthy']}\")\n    print(f\"Runtime healthy: {health['runtime_healthy']}\")\nelse:\n    print(\"\u274c System has issues\")\n</code></pre>"},{"location":"api-reference/configuration/#version-information","title":"Version Information","text":"<pre><code># Get current version\nversion = graphbit.version()\nprint(f\"GraphBit version: {version}\")\n</code></pre>"},{"location":"api-reference/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"api-reference/configuration/#development-configuration","title":"Development Configuration","text":"<pre><code>def create_dev_config():\n    \"\"\"Configuration for development environment.\"\"\"\n\n    # Initialize with debugging\n    graphbit.init(debug=True, log_level=\"info\")\n\n    # Use faster, cheaper model for development\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    # Low-latency executor for development\n    executor = graphbit.Executor.new_low_latency(\n        config, \n        timeout_seconds=60,\n        debug=True\n    )\n\n    return executor\n</code></pre>"},{"location":"api-reference/configuration/#production-configuration","title":"Production Configuration","text":"<pre><code>def create_prod_config():\n    \"\"\"Configuration for production environment.\"\"\"\n\n    # Initialize without debugging\n    graphbit.init(debug=False, log_level=\"warn\")\n\n    # High-quality model for production\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    # High-throughput executor for production\n    executor = graphbit.Executor.new_high_throughput(\n        config,\n        timeout_seconds=300,\n        debug=False\n    )\n\n    # Configure for production reliability\n    executor.configure(\n        timeout_seconds=300,\n        max_retries=3,\n        enable_metrics=True,\n        debug=False\n    )\n\n    return executor\n</code></pre>"},{"location":"api-reference/configuration/#high-volume-configuration","title":"High-Volume Configuration","text":"<pre><code>def create_high_volume_config():\n    \"\"\"Configuration for high-volume processing.\"\"\"\n\n    # Configure runtime for high throughput\n    graphbit.configure_runtime(\n        worker_threads=16,\n        max_blocking_threads=32\n    )\n    graphbit.init(debug=False)\n\n    # Fast, cost-effective model\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    # Memory-optimized executor\n    executor = graphbit.Executor.new_memory_optimized(\n        config,\n        timeout_seconds=180,\n        debug=False\n    )\n\n    return executor\n</code></pre>"},{"location":"api-reference/configuration/#local-development-configuration","title":"Local Development Configuration","text":"<pre><code>def create_local_config():\n    \"\"\"Configuration for local development with Ollama.\"\"\"\n\n    graphbit.init(debug=True, log_level=\"debug\")\n\n    # Use local Ollama\n    config = graphbit.LlmConfig.ollama(\"llama3.2\")\n\n    # Low-latency for quick iteration\n    executor = graphbit.Executor.new_low_latency(\n        config,\n        timeout_seconds=180,  # Longer timeout for local inference\n        debug=True\n    )\n\n    return executor\n</code></pre>"},{"location":"api-reference/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"api-reference/configuration/#environment-validation","title":"Environment Validation","text":"<pre><code>def validate_environment():\n    \"\"\"Validate environment setup.\"\"\"\n\n    import os\n\n    # Check required environment variables\n    required_vars = {\n        \"OPENAI_API_KEY\": \"OpenAI API key\",\n        # Add others as needed\n    }\n\n    missing_vars = []\n    for var, description in required_vars.items():\n        if not os.getenv(var):\n            missing_vars.append(f\"{var} ({description})\")\n\n    if missing_vars:\n        raise ValueError(f\"Missing environment variables: {', '.join(missing_vars)}\")\n\n    print(\"\u2705 Environment validation passed\")\n</code></pre>"},{"location":"api-reference/configuration/#configuration-testing","title":"Configuration Testing","text":"<pre><code>def test_configuration(config):\n    \"\"\"Test LLM configuration.\"\"\"\n\n    try:\n        # Create client\n        client = graphbit.LlmClient(config)\n\n        # Test simple completion\n        response = client.complete(\"Say 'Configuration test successful'\")\n\n        if \"successful\" in response.lower():\n            print(\"Configuration test passed\")\n            return True\n        else:\n            print(\"Configuration test failed - unexpected response\")\n            return False\n\n    except Exception as e:\n        print(f\"Configuration test failed: {e}\")\n        return False\n</code></pre>"},{"location":"api-reference/configuration/#health-check-function","title":"Health Check Function","text":"<pre><code>def comprehensive_health_check():\n    \"\"\"Comprehensive system health check.\"\"\"\n\n    # Check system health\n    health = graphbit.health_check()\n    if not health['overall_healthy']:\n        print(\"System health check failed\")\n        return False\n\n    # Check system info\n    info = graphbit.get_system_info()\n    if not info['runtime_initialized']:\n        print(\"Runtime not initialized\")\n        return False\n\n    # Test basic functionality\n    try:\n        config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"))\n        client = graphbit.LlmClient(config)\n\n        # Quick test\n        response = client.complete(\"Test\")\n        if not response:\n            print(\"LLM test failed\")\n            return False\n\n    except Exception as e:\n        print(f\"\u274c LLM test failed: {e}\")\n        return False\n\n    print(\"\u2705 Comprehensive health check passed\")\n    return True\n</code></pre>"},{"location":"api-reference/configuration/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/configuration/#1-environment-based-configuration","title":"1. Environment-Based Configuration","text":"<pre><code>def get_config_for_environment():\n    \"\"\"Get configuration based on environment.\"\"\"\n\n    env = os.getenv(\"ENVIRONMENT\", \"development\")\n\n    if env == \"production\":\n        return create_prod_config()\n    elif env == \"staging\":\n        return create_staging_config()\n    elif env == \"local\":\n        return create_local_config()\n    else:\n        return create_dev_config()\n</code></pre>"},{"location":"api-reference/configuration/#2-secure-configuration","title":"2. Secure Configuration","text":"<pre><code>def secure_config_setup():\n    \"\"\"Set up configuration securely.\"\"\"\n\n    # Validate API key exists\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"OPENAI_API_KEY environment variable required\")\n\n    # Validate API key format (basic check)\n    if len(api_key) &lt; 20:\n        raise ValueError(\"Invalid API key format\")\n\n    config = graphbit.LlmConfig.openai(api_key)\n    return config\n</code></pre>"},{"location":"api-reference/configuration/#3-performance-monitoring","title":"3. Performance Monitoring","text":"<pre><code>def monitor_performance(executor):\n    \"\"\"Monitor executor performance.\"\"\"\n\n    import time\n\n    # Get initial stats\n    initial_stats = executor.get_stats()\n\n    start_time = time.time()\n\n    # Your workflow execution here\n    # result = executor.execute(workflow)\n\n    end_time = time.time()\n    execution_time = end_time - start_time\n\n    # Get final stats\n    final_stats = executor.get_stats()\n\n    # Log performance metrics\n    print(f\"Execution time: {execution_time:.2f}s\")\n    print(f\"Total executions: {final_stats['total_executions']}\")\n    print(f\"Success rate: {final_stats['success_rate']:.2%}\")\n\n    # Alert on performance issues\n    if execution_time &gt; 60:  # 60 second threshold\n        print(\"Slow execution detected - consider tuning configuration\")\n\n    if final_stats['success_rate'] &lt; 0.95:  # 95% success rate threshold\n        print(\"Low success rate - check configuration and API health\")\n</code></pre>"},{"location":"api-reference/configuration/#4-graceful-error-handling","title":"4. Graceful Error Handling","text":"<pre><code>def robust_config_creation():\n    \"\"\"Create configuration with fallback options.\"\"\"\n\n    try:\n        # Primary configuration\n        config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"))\n        executor = graphbit.Executor.new_high_throughput(config)\n\n        # Test configuration\n        if test_configuration(config):\n            return executor\n        else:\n            raise Exception(\"Configuration test failed\")\n\n    except Exception as e:\n        print(f\"Primary configuration failed: {e}\")\n\n        try:\n            # Fallback to local Ollama\n            print(\"Falling back to local Ollama...\")\n            fallback_config = graphbit.LlmConfig.ollama()\n            fallback_executor = graphbit.Executor.new_low_latency(fallback_config)\n\n            if test_configuration(fallback_config):\n                return fallback_executor\n            else:\n                raise Exception(\"Fallback configuration failed\")\n\n        except Exception as fallback_error:\n            print(f\"Fallback configuration failed: {fallback_error}\")\n            raise Exception(\"All configuration options exhausted\")\n</code></pre>"},{"location":"api-reference/configuration/#5-resource-cleanup","title":"5. Resource Cleanup","text":"<pre><code>def cleanup_resources():\n    \"\"\"Clean up GraphBit resources.\"\"\"\n\n    try:\n        # Shutdown GraphBit (for testing/cleanup)\n        graphbit.shutdown()\n        print(\"Resources cleaned up successfully\")\n    except Exception as e:\n        print(f\"Error during cleanup: {e}\")\n</code></pre>"},{"location":"api-reference/configuration/#configuration-troubleshooting","title":"Configuration Troubleshooting","text":""},{"location":"api-reference/configuration/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"api-reference/configuration/#1-api-key-issues","title":"1. API Key Issues","text":"<pre><code># Check API key validity\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    print(\"OPENAI_API_KEY not set\")\nelif len(api_key) &lt; 20:\n    print(\"API key appears invalid (too short)\")\nelif not api_key.startswith(\"sk-\"):\n    print(\"OpenAI API key should start with 'sk-'\")\nelse:\n    print(\"API key format looks correct\")\n</code></pre>"},{"location":"api-reference/configuration/#2-runtime-issues","title":"2. Runtime Issues","text":"<pre><code># Check runtime status\ninfo = graphbit.get_system_info()\nif not info['runtime_initialized']:\n    print(\"Runtime not initialized - call graphbit.init()\")\nelse:\n    print(f\"Runtime initialized with {info['runtime_worker_threads']} workers\")\n</code></pre>"},{"location":"api-reference/configuration/#3-memory-issues","title":"3. Memory Issues","text":"<pre><code># Check memory status\nhealth = graphbit.health_check()\nif not health['memory_healthy']:\n    print(f\"Low memory: {health['available_memory_mb']}MB available\")\n    print(\"Consider using memory-optimized executor\")\nelse:\n    print(\"Memory status OK\")\n</code></pre> <p>Proper configuration is essential for optimal GraphBit performance. Choose settings that match your use case, environment, and performance requirements.</p>"},{"location":"api-reference/node-types/","title":"Node Types Reference","text":"<p>GraphBit workflows are built using different types of nodes, each serving a specific purpose. This reference covers all available node types and their usage patterns.</p>"},{"location":"api-reference/node-types/#node-type-categories","title":"Node Type Categories","text":"<ol> <li>Agent Nodes - AI-powered processing nodes</li> <li>Condition Nodes - Decision and branching logic</li> <li>Transform Nodes - Data transformation and processing</li> <li>Delay Nodes - Timing and rate limiting</li> <li>Document Loader Nodes - Document processing</li> </ol>"},{"location":"api-reference/node-types/#agent-nodes","title":"Agent Nodes","text":"<p>Agent nodes are the core AI-powered components that interact with LLM providers.</p>"},{"location":"api-reference/node-types/#basic-agent-node","title":"Basic Agent Node","text":"<pre><code>agent = graphbit.Node.agent(\n    name=\"Content Analyzer\",\n    prompt=\"Analyze the following content: {input}\",\n    agent_id=\"analyzer\"  # Optional, auto-generated if not provided\n)\n</code></pre> <p>Parameters: - <code>name</code> (str): Human-readable node name - <code>prompt</code> (str): LLM prompt template with variable placeholders - <code>agent_id</code> (str, optional): Unique identifier for the agent. Auto-generated if not provided</p>"},{"location":"api-reference/node-types/#advanced-agent-node","title":"Advanced Agent Node","text":"<pre><code>agent = graphbit.PyWorkflowNode.agent_node_with_config(\n    name=\"Creative Writer\", \n    description=\"Writes creative content\",\n    agent_id=\"writer\",\n    prompt=\"Write a story about: {topic}\",\n    max_tokens=2000,\n    temperature=0.8\n)\n</code></pre> <p>Additional Parameters: - <code>max_tokens</code> (int): Maximum tokens to generate - <code>temperature</code> (float): Creativity/randomness level (0.0-1.0)</p>"},{"location":"api-reference/node-types/#agent-node-examples","title":"Agent Node Examples","text":""},{"location":"api-reference/node-types/#text-analysis-agent","title":"Text Analysis Agent","text":"<pre><code>sentiment_analyzer = graphbit.Node.agent(\n    name=\"Sentiment Analyzer\",\n    prompt=\"\"\"\n    Analyze the sentiment of this text: \"{text}\"\n\n    Provide:\n    - Overall sentiment (positive/negative/neutral)\n    - Confidence score (0-1)\n    - Key emotional indicators\n    \"\"\",\n    agent_id=\"sentiment_analyzer\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#code-review-agent","title":"Code Review Agent","text":"<pre><code>code_reviewer = graphbit.Node.agent(\n    name=\"Code Reviewer\",\n    prompt=\"\"\"\n    Review this code for quality and security issues:\n\n    {code}\n\n    Check for:\n    - Security vulnerabilities\n    - Performance issues\n    - Code style problems\n    - Best practices violations\n    \"\"\",\n    agent_id=\"code_reviewer\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#data-processing-agent","title":"Data Processing Agent","text":"<pre><code>data_processor = graphbit.Node.agent(\n    name=\"Data Processor\",\n    prompt=\"\"\"\n    Process this dataset and provide insights:\n\n    Data: {dataset}\n\n    Include:\n    1. Statistical summary\n    2. Key trends\n    3. Anomalies\n    4. Recommendations\n    \"\"\",\n    agent_id=\"data_processor\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#content-generation-agent","title":"Content Generation Agent","text":"<pre><code>content_writer = graphbit.Node.agent(\n    name=\"Content Writer\",\n    prompt=\"\"\"\n    Write engaging content about: {topic}\n\n    Requirements:\n    - Target audience: {audience}\n    - Tone: {tone}\n    - Length: {word_count} words\n    - Include call-to-action\n    \"\"\",\n    agent_id=\"content_writer\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#research-assistant-agent","title":"Research Assistant Agent","text":"<pre><code>research_assistant = graphbit.Node.agent(\n    name=\"Research Assistant\",\n    prompt=\"\"\"\n    Research the following topic: {research_topic}\n\n    Provide:\n    - Key findings (3-5 points)\n    - Supporting evidence\n    - Potential implications\n    - Areas for further investigation\n\n    Focus on: {focus_area}\n    \"\"\",\n    agent_id=\"research_assistant\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#condition-nodes","title":"Condition Nodes","text":"<p>Condition nodes enable branching logic and decision-making in workflows.</p>"},{"location":"api-reference/node-types/#basic-condition-node","title":"Basic Condition Node","text":"<pre><code>condition = graphbit.Node.condition(\n    name=\"Quality Gate\",\n    expression=\"quality_score &gt; 0.8\"\n)\n</code></pre> <p>Parameters: - <code>name</code> (str): Node name - <code>expression</code> (str): Boolean expression to evaluate</p>"},{"location":"api-reference/node-types/#condition-expressions","title":"Condition Expressions","text":"<p>Condition nodes support various comparison operators:</p>"},{"location":"api-reference/node-types/#numeric-comparisons","title":"Numeric Comparisons","text":"<pre><code># Greater than\nhigh_score = graphbit.Node.condition(\n    name=\"High Score Check\", \n    expression=\"score &gt; 80\"\n)\n\n# Range checks\nvalid_range = graphbit.Node.condition(\n    name=\"Range Validator\", \n    expression=\"value &gt;= 10 &amp;&amp; value &lt;= 100\"\n)\n\n# Multiple conditions\ncomplex_check = graphbit.Node.condition(\n    name=\"Complex Check\", \n    expression=\"(score &gt; 75 &amp;&amp; confidence &gt; 0.8) || priority == 'high'\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#string-comparisons","title":"String Comparisons","text":"<pre><code># Equality\nstatus_check = graphbit.Node.condition(\n    name=\"Status Check\", \n    expression=\"status == 'approved'\"\n)\n\n# Contains check\ncontent_check = graphbit.Node.condition(\n    name=\"Content Check\", \n    expression=\"content.contains('urgent')\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#boolean-logic","title":"Boolean Logic","text":"<pre><code># AND conditions\napproval_gate = graphbit.Node.condition(\n    name=\"Approval Gate\", \n    expression=\"technical_approved == true &amp;&amp; business_approved == true\"\n)\n\n# OR conditions  \npriority_check = graphbit.Node.condition(\n    name=\"Priority Check\", \n    expression=\"priority == 'high' || severity == 'critical'\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#condition-node-examples","title":"Condition Node Examples","text":""},{"location":"api-reference/node-types/#quality-assurance","title":"Quality Assurance","text":"<pre><code>qa_gate = graphbit.Node.condition(\n    name=\"QA Gate\",\n    expression=\"quality_rating &gt;= 8 &amp;&amp; error_count == 0\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#content-moderation","title":"Content Moderation","text":"<pre><code>content_filter = graphbit.Node.condition(\n    name=\"Content Filter\", \n    expression=\"toxicity_score &lt; 0.1 &amp;&amp; sentiment != 'very_negative'\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#business-rules","title":"Business Rules","text":"<pre><code>business_rule = graphbit.Node.condition(\n    name=\"Business Rule\",\n    expression=\"budget_remaining &gt; cost &amp;&amp; approval_level &gt;= required_level\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#threshold-checking","title":"Threshold Checking","text":"<pre><code>performance_threshold = graphbit.Node.condition(\n    name=\"Performance Threshold\",\n    expression=\"response_time &lt; 1000 &amp;&amp; error_rate &lt; 0.01\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#data-validation","title":"Data Validation","text":"<pre><code>data_validator = graphbit.Node.condition(\n    name=\"Data Validator\",\n    expression=\"data_completeness &gt; 0.95 &amp;&amp; data_accuracy &gt; 0.9\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#transform-nodes","title":"Transform Nodes","text":"<p>Transform nodes perform data processing and format conversions.</p>"},{"location":"api-reference/node-types/#basic-transform-node","title":"Basic Transform Node","text":"<pre><code>transformer = graphbit.Node.transform(\n    name=\"Text Transformer\",\n    transformation=\"uppercase\"\n)\n</code></pre> <p>Parameters: - <code>name</code> (str): Node name - <code>transformation</code> (str): Transformation type</p>"},{"location":"api-reference/node-types/#available-transformations","title":"Available Transformations","text":""},{"location":"api-reference/node-types/#text-transformations","title":"Text Transformations","text":"<pre><code># Convert to uppercase\nupper = graphbit.Node.transform(\n    name=\"Uppercase Converter\", \n    transformation=\"uppercase\"\n)\n\n# Convert to lowercase  \nlower = graphbit.Node.transform(\n    name=\"Lowercase Converter\", \n    transformation=\"lowercase\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#data-extraction","title":"Data Extraction","text":"<pre><code># Extract JSON from text\njson_extractor = graphbit.Node.transform(\n    name=\"JSON Extractor\", \n    transformation=\"json_extract\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#text-processing","title":"Text Processing","text":"<pre><code># Split text\ntext_splitter = graphbit.Node.transform(\n    name=\"Text Splitter\", \n    transformation=\"split\"\n)\n\n# Join text\ntext_joiner = graphbit.Node.transform(\n    name=\"Text Joiner\", \n    transformation=\"join\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#transform-node-examples","title":"Transform Node Examples","text":""},{"location":"api-reference/node-types/#data-cleaning-pipeline","title":"Data Cleaning Pipeline","text":"<pre><code># Clean and format text\ntext_cleaner = graphbit.Node.transform(\n    name=\"Text Cleaner\", \n    transformation=\"lowercase\"\n)\n\n# Normalize data format\ndata_normalizer = graphbit.Node.transform(\n    name=\"Data Normalizer\",\n    transformation=\"uppercase\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#format-conversion","title":"Format Conversion","text":"<pre><code># Convert response to structured format\nformatter = graphbit.Node.transform(\n    name=\"Response Formatter\",\n    transformation=\"lowercase\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#delay-nodes","title":"Delay Nodes","text":"<p>Delay nodes add timing controls and rate limiting to workflows.</p>"},{"location":"api-reference/node-types/#basic-delay-node","title":"Basic Delay Node","text":"<pre><code>delay = graphbit.PyWorkflowNode.delay_node(\n    name=\"Rate Limiter\",\n    description=\"Prevents API rate limiting\", \n    duration_seconds=5\n)\n</code></pre> <p>Parameters: - <code>name</code> (str): Node name - <code>description</code> (str): Node description - <code>duration_seconds</code> (int): Delay duration in seconds</p>"},{"location":"api-reference/node-types/#delay-node-examples","title":"Delay Node Examples","text":""},{"location":"api-reference/node-types/#rate-limiting","title":"Rate Limiting","text":"<pre><code># API rate limiting\napi_delay = graphbit.PyWorkflowNode.delay_node(\n    name=\"API Rate Limit\",\n    description=\"Waits to respect API limits\",\n    duration_seconds=2\n)\n\n# Batch processing delay\nbatch_delay = graphbit.PyWorkflowNode.delay_node(\n    name=\"Batch Delay\", \n    description=\"Delays between batch items\",\n    duration_seconds=1\n)\n</code></pre>"},{"location":"api-reference/node-types/#system-cooldown","title":"System Cooldown","text":"<pre><code># Cool-down period\ncooldown = graphbit.PyWorkflowNode.delay_node(\n    name=\"System Cooldown\",\n    description=\"Allows system recovery time\",\n    duration_seconds=30\n)\n</code></pre>"},{"location":"api-reference/node-types/#scheduled-processing","title":"Scheduled Processing","text":"<pre><code># Wait for scheduled time\nscheduler_delay = graphbit.PyWorkflowNode.delay_node(\n    name=\"Schedule Delay\",\n    description=\"Waits for next processing window\", \n    duration_seconds=300  # 5 minutes\n)\n</code></pre>"},{"location":"api-reference/node-types/#document-loader-nodes","title":"Document Loader Nodes","text":"<p>Document loader nodes process and load various document types.</p>"},{"location":"api-reference/node-types/#basic-document-loader","title":"Basic Document Loader","text":"<pre><code>loader = graphbit.PyWorkflowNode.document_loader_node(\n    name=\"PDF Loader\",\n    description=\"Loads PDF documents\",\n    document_type=\"pdf\",\n    source_path=\"/path/to/document.pdf\"\n)\n</code></pre> <p>Parameters: - <code>name</code> (str): Node name - <code>description</code> (str): Node description - <code>document_type</code> (str): Document type (\"pdf\", \"txt\", \"docx\", etc.) - <code>source_path</code> (str): Path to the document</p>"},{"location":"api-reference/node-types/#supported-document-types","title":"Supported Document Types","text":""},{"location":"api-reference/node-types/#pdf-documents","title":"PDF Documents","text":"<pre><code>pdf_loader = graphbit.PyWorkflowNode.document_loader_node(\n    name=\"PDF Document Loader\",\n    description=\"Loads and processes PDF files\",\n    document_type=\"pdf\",\n    source_path=\"documents/report.pdf\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#text-files","title":"Text Files","text":"<pre><code>text_loader = graphbit.PyWorkflowNode.document_loader_node(\n    name=\"Text File Loader\", \n    description=\"Loads plain text files\",\n    document_type=\"txt\",\n    source_path=\"data/content.txt\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#word-documents","title":"Word Documents","text":"<pre><code>docx_loader = graphbit.PyWorkflowNode.document_loader_node(\n    name=\"Word Document Loader\",\n    description=\"Loads Microsoft Word documents\",\n    document_type=\"docx\", \n    source_path=\"documents/specification.docx\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#node-connection-patterns","title":"Node Connection Patterns","text":""},{"location":"api-reference/node-types/#sequential-connections","title":"Sequential Connections","text":"<p>Connect nodes for sequential processing:</p> <pre><code>workflow = graphbit.Workflow(\"Sequential Pipeline\")\n\n# Add nodes\nnode1_id = workflow.add_node(input_processor)\nnode2_id = workflow.add_node(analyzer)\nnode3_id = workflow.add_node(output_formatter)\n\n# Connect sequentially\nworkflow.connect(node1_id, node2_id)\nworkflow.connect(node2_id, node3_id)\n</code></pre>"},{"location":"api-reference/node-types/#conditional-connections","title":"Conditional Connections","text":"<p>Use condition nodes for branching:</p> <pre><code>workflow = graphbit.Workflow(\"Conditional Pipeline\")\n\n# Add nodes\nanalyzer_id = workflow.add_node(analyzer)\ncondition_id = workflow.add_node(quality_condition)\nsuccess_path_id = workflow.add_node(success_handler)\nfailure_path_id = workflow.add_node(failure_handler)\n\n# Connect with conditions\nworkflow.connect(analyzer_id, condition_id)\nworkflow.connect(condition_id, success_path_id)  # Connect to success path\nworkflow.connect(condition_id, failure_path_id)  # Connect to failure path\n</code></pre>"},{"location":"api-reference/node-types/#parallel-processing","title":"Parallel Processing","text":"<p>Process multiple branches simultaneously:</p> <pre><code>workflow = graphbit.Workflow(\"Parallel Processing\")\n\n# Add input and processors\ninput_id = workflow.add_node(input_processor)\nprocessor1_id = workflow.add_node(sentiment_analyzer)\nprocessor2_id = workflow.add_node(topic_extractor)\nprocessor3_id = workflow.add_node(summary_generator)\naggregator_id = workflow.add_node(result_aggregator)\n\n# Fan-out to parallel processors\nworkflow.connect(input_id, processor1_id)\nworkflow.connect(input_id, processor2_id)\nworkflow.connect(input_id, processor3_id)\n\n# Fan-in to aggregator\nworkflow.connect(processor1_id, aggregator_id)\nworkflow.connect(processor2_id, aggregator_id)\nworkflow.connect(processor3_id, aggregator_id)\n</code></pre>"},{"location":"api-reference/node-types/#advanced-node-patterns","title":"Advanced Node Patterns","text":""},{"location":"api-reference/node-types/#validation-chain","title":"Validation Chain","text":"<pre><code>def create_validation_chain():\n    workflow = graphbit.Workflow(\"Validation Chain\")\n\n    # Input validator\n    input_validator = graphbit.Node.condition(\n        name=\"Input Validator\", \n        expression=\"input_valid == true\"\n    )\n\n    # Content processor\n    processor = graphbit.Node.agent(\n        name=\"Content Processor\",\n        prompt=\"Process this validated content: {input}\",\n        agent_id=\"processor\"\n    )\n\n    # Output validator\n    output_validator = graphbit.Node.condition(\n        name=\"Output Validator\", \n        expression=\"output_quality &gt; 0.7\"\n    )\n\n    # Connect validation chain\n    input_id = workflow.add_node(input_validator)\n    proc_id = workflow.add_node(processor)\n    output_id = workflow.add_node(output_validator)\n\n    workflow.connect(input_id, proc_id)\n    workflow.connect(proc_id, output_id)\n\n    return workflow\n</code></pre>"},{"location":"api-reference/node-types/#error-handling-pattern","title":"Error Handling Pattern","text":"<pre><code>def create_error_handling_workflow():\n    workflow = graphbit.Workflow(\"Error Handling\")\n\n    # Main processor\n    main_processor = graphbit.Node.agent(\n        name=\"Main Processor\",\n        prompt=\"Process: {input}\",\n        agent_id=\"main\"\n    )\n\n    # Error detector\n    error_detector = graphbit.Node.condition(\n        name=\"Error Detector\", \n        expression=\"error_occurred == false\"\n    )\n\n    # Error handler\n    error_handler = graphbit.Node.agent(\n        name=\"Error Handler\",\n        prompt=\"Handle this error: {error_message}\",\n        agent_id=\"error_handler\"\n    )\n\n    # Success handler\n    success_handler = graphbit.Node.agent(\n        name=\"Success Handler\",\n        prompt=\"Finalize successful result: {result}\",\n        agent_id=\"success_handler\"\n    )\n\n    # Build error handling flow\n    main_id = workflow.add_node(main_processor)\n    detector_id = workflow.add_node(error_detector)\n    error_id = workflow.add_node(error_handler)\n    success_id = workflow.add_node(success_handler)\n\n    workflow.connect(main_id, detector_id)\n    workflow.connect(detector_id, error_id)   # Error path\n    workflow.connect(detector_id, success_id) # Success path\n\n    return workflow\n</code></pre>"},{"location":"api-reference/node-types/#multi-step-analysis-pipeline","title":"Multi-Step Analysis Pipeline","text":"<pre><code>def create_analysis_pipeline():\n    workflow = graphbit.Workflow(\"Multi-Step Analysis\")\n\n    # Step 1: Initial analysis\n    initial_analyzer = graphbit.Node.agent(\n        name=\"Initial Analyzer\",\n        prompt=\"Perform initial analysis of: {input}\",\n        agent_id=\"initial_analyzer\"\n    )\n\n    # Step 2: Quality check\n    quality_check = graphbit.Node.condition(\n        name=\"Quality Check\",\n        expression=\"initial_quality &gt; 0.6\"\n    )\n\n    # Step 3: Deep analysis (if quality is good)\n    deep_analyzer = graphbit.Node.agent(\n        name=\"Deep Analyzer\",\n        prompt=\"Perform deep analysis of: {analyzed_content}\",\n        agent_id=\"deep_analyzer\"\n    )\n\n    # Step 4: Final formatter\n    formatter = graphbit.Node.transform(\n        name=\"Result Formatter\",\n        transformation=\"uppercase\"\n    )\n\n    # Connect the pipeline\n    initial_id = workflow.add_node(initial_analyzer)\n    quality_id = workflow.add_node(quality_check)\n    deep_id = workflow.add_node(deep_analyzer)\n    format_id = workflow.add_node(formatter)\n\n    workflow.connect(initial_id, quality_id)\n    workflow.connect(quality_id, deep_id)\n    workflow.connect(deep_id, format_id)\n\n    return workflow\n</code></pre>"},{"location":"api-reference/node-types/#node-properties-and-methods","title":"Node Properties and Methods","text":"<p>All nodes share common properties:</p> <pre><code># Access node properties\nnode_id = node.id()    # Unique identifier\nnode_name = node.name() # Human-readable name\n</code></pre>"},{"location":"api-reference/node-types/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/node-types/#1-descriptive-names","title":"1. Descriptive Names","text":"<p>Use clear, descriptive names for all nodes:</p> <pre><code># Good\nemail_sentiment_analyzer = graphbit.Node.agent(\n    name=\"Email Sentiment Analyzer\",\n    prompt=\"Analyze sentiment of customer emails: {email_content}\",\n    agent_id=\"email_sentiment\"\n)\n\n# Avoid\nnode1 = graphbit.Node.agent(\n    name=\"Node1\", \n    prompt=\"Do: {input}\",\n    agent_id=\"n1\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#2-single-responsibility","title":"2. Single Responsibility","text":"<p>Each node should have one clear purpose:</p> <pre><code># Good - focused on one task\nspam_detector = graphbit.Node.agent(\n    name=\"Spam Detector\",\n    prompt=\"Is this email spam? {email}\",\n    agent_id=\"spam_detector\"\n)\n\n# Avoid - too many responsibilities  \neverything_processor = graphbit.Node.agent(\n    name=\"Everything Processor\",\n    prompt=\"Do everything with: {input}\",\n    agent_id=\"everything\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#3-appropriate-node-types","title":"3. Appropriate Node Types","text":"<p>Choose the right node type for each task:</p> <ul> <li>Agent Nodes: AI/LLM processing tasks</li> <li>Condition Nodes: Decision making and branching</li> <li>Transform Nodes: Data format conversion</li> <li>Delay Nodes: Timing and rate control</li> <li>Document Loaders: File processing</li> </ul>"},{"location":"api-reference/node-types/#4-error-handling","title":"4. Error Handling","text":"<p>Include appropriate error handling and validation:</p> <pre><code># Validation before processing\nvalidator = graphbit.Node.condition(\n    name=\"Input Validator\", \n    expression=\"data_valid == true\"\n)\n\n# Error recovery\nerror_handler = graphbit.Node.agent(\n    name=\"Error Handler\",\n    prompt=\"Safely handle this error: {error}\",\n    agent_id=\"error_handler\"\n)\n</code></pre>"},{"location":"api-reference/node-types/#5-clear-prompt-design","title":"5. Clear Prompt Design","text":"<p>Write clear, specific prompts for agent nodes:</p> <pre><code># Good - specific and clear\nsummarizer = graphbit.Node.agent(\n    name=\"Document Summarizer\",\n    prompt=\"\"\"\n    Summarize this document in exactly 3 paragraphs:\n\n    Document: {document_content}\n\n    Requirements:\n    - Paragraph 1: Main topic and purpose\n    - Paragraph 2: Key findings or arguments\n    - Paragraph 3: Conclusions and implications\n    \"\"\",\n    agent_id=\"summarizer\"\n)\n\n# Avoid - vague and unclear\nbad_summarizer = graphbit.Node.agent(\n    name=\"Summarizer\",\n    prompt=\"Summarize: {input}\",\n    agent_id=\"summarizer\"\n)\n</code></pre> <p>Understanding these node types and their usage patterns enables you to build sophisticated, reliable workflows that handle complex AI processing tasks effectively. Choose the appropriate node type for each step in your workflow, and connect them in logical patterns to achieve your processing goals.</p>"},{"location":"api-reference/python-api/","title":"Python API Reference","text":"<p>Complete reference for GraphBit's Python API. This document covers all classes, methods, and their usage based on the actual Python binding implementation.</p>"},{"location":"api-reference/python-api/#module-graphbit","title":"Module: <code>graphbit</code>","text":""},{"location":"api-reference/python-api/#core-functions","title":"Core Functions","text":""},{"location":"api-reference/python-api/#initlog_levelnone-enable_tracingnone-debugnone","title":"<code>init(log_level=None, enable_tracing=None, debug=None)</code>","text":"<p>Initialize the GraphBit library with optional configuration.</p> <pre><code>import graphbit\n\n# Basic initialization\ngraphbit.init()\n\n# With debugging enabled\ngraphbit.init(debug=True)\n\n# With custom log level and tracing\ngraphbit.init(log_level=\"info\", enable_tracing=True)\n</code></pre> <p>Parameters: - <code>log_level</code> (str, optional): Log level (\"trace\", \"debug\", \"info\", \"warn\", \"error\"). Default: \"warn\" - <code>enable_tracing</code> (bool, optional): Enable tracing. Default: False - <code>debug</code> (bool, optional): Enable debug mode (alias for enable_tracing). Default: False</p> <p>Returns: <code>None</code> Raises: <code>RuntimeError</code> if initialization fails</p>"},{"location":"api-reference/python-api/#version","title":"<code>version()</code>","text":"<p>Get the current GraphBit version.</p> <pre><code>version = graphbit.version()\nprint(f\"GraphBit version: {version}\")\n</code></pre> <p>Returns: <code>str</code> - Version string (e.g., \"0.1.0\")</p>"},{"location":"api-reference/python-api/#get_system_info","title":"<code>get_system_info()</code>","text":"<p>Get comprehensive system information and health status.</p> <pre><code>info = graphbit.get_system_info()\nprint(f\"CPU count: {info['cpu_count']}\")\nprint(f\"Runtime initialized: {info['runtime_initialized']}\")\n</code></pre> <p>Returns: <code>dict</code> - Dictionary containing: - <code>version</code>: GraphBit version - <code>python_binding_version</code>: Python binding version - <code>runtime_uptime_seconds</code>: Runtime uptime - <code>runtime_worker_threads</code>: Number of worker threads - <code>cpu_count</code>: Number of CPU cores - <code>runtime_initialized</code>: Runtime initialization status - <code>memory_allocator</code>: Memory allocator type - <code>build_target</code>: Build target - <code>build_profile</code>: Build profile (debug/release)</p>"},{"location":"api-reference/python-api/#health_check","title":"<code>health_check()</code>","text":"<p>Perform comprehensive health checks.</p> <pre><code>health = graphbit.health_check()\nif health['overall_healthy']:\n    print(\"System is healthy\")\nelse:\n    print(\"System has issues\")\n</code></pre> <p>Returns: <code>dict</code> - Dictionary containing health status information</p>"},{"location":"api-reference/python-api/#configure_runtimeworker_threadsnone-max_blocking_threadsnone-thread_stack_size_mbnone","title":"<code>configure_runtime(worker_threads=None, max_blocking_threads=None, thread_stack_size_mb=None)</code>","text":"<p>Configure the global runtime with custom settings (advanced).</p> <pre><code># Configure runtime before init()\ngraphbit.configure_runtime(worker_threads=8, max_blocking_threads=16)\ngraphbit.init()\n</code></pre> <p>Parameters: - <code>worker_threads</code> (int, optional): Number of worker threads - <code>max_blocking_threads</code> (int, optional): Maximum blocking threads - <code>thread_stack_size_mb</code> (int, optional): Thread stack size in MB</p>"},{"location":"api-reference/python-api/#shutdown","title":"<code>shutdown()</code>","text":"<p>Gracefully shutdown the library (for testing and cleanup).</p> <pre><code>graphbit.shutdown()\n</code></pre>"},{"location":"api-reference/python-api/#llm-configuration","title":"LLM Configuration","text":""},{"location":"api-reference/python-api/#llmconfig","title":"<code>LlmConfig</code>","text":"<p>Configuration class for Large Language Model providers.</p>"},{"location":"api-reference/python-api/#static-methods","title":"Static Methods","text":""},{"location":"api-reference/python-api/#llmconfigopenaiapi_key-modelnone","title":"<code>LlmConfig.openai(api_key, model=None)</code>","text":"<p>Create OpenAI provider configuration.</p> <pre><code>config = graphbit.LlmConfig.openai(\"sk-...\", \"gpt-4o-mini\")\n# With default model\nconfig = graphbit.LlmConfig.openai(\"sk-...\")  # Uses gpt-4o-mini\n</code></pre> <p>Parameters: - <code>api_key</code> (str): OpenAI API key - <code>model</code> (str, optional): Model name. Default: \"gpt-4o-mini\"</p> <p>Returns: <code>LlmConfig</code> instance</p>"},{"location":"api-reference/python-api/#llmconfiganthropicapi_key-modelnone","title":"<code>LlmConfig.anthropic(api_key, model=None)</code>","text":"<p>Create Anthropic provider configuration.</p> <pre><code>config = graphbit.LlmConfig.anthropic(\"sk-ant-...\", \"claude-3-5-sonnet-20241022\")\n# With default model\nconfig = graphbit.LlmConfig.anthropic(\"sk-ant-...\")  # Uses claude-3-5-sonnet-20241022\n</code></pre> <p>Parameters: - <code>api_key</code> (str): Anthropic API key - <code>model</code> (str, optional): Model name. Default: \"claude-3-5-sonnet-20241022\"</p> <p>Returns: <code>LlmConfig</code> instance</p>"},{"location":"api-reference/python-api/#llmconfigdeepseekapi_key-modelnone","title":"<code>LlmConfig.deepseek(api_key, model=None)</code>","text":"<p>Create DeepSeek provider configuration.</p> <pre><code>config = graphbit.LlmConfig.deepseek(\"your-deepseek-api-key\", \"deepseek-chat\")\n# With default model\nconfig = graphbit.LlmConfig.deepseek(\"your-deepseek-api-key\")  # Uses deepseek-chat\n</code></pre> <p>Parameters: - <code>api_key</code> (str): DeepSeek API key - <code>model</code> (str, optional): Model name. Default: \"deepseek-chat\"</p> <p>Available Models: - <code>deepseek-chat</code>: General conversation and instruction following - <code>deepseek-coder</code>: Specialized for code generation and programming tasks - <code>deepseek-reasoner</code>: Advanced reasoning and mathematical problem solving</p> <p>Returns: <code>LlmConfig</code> instance</p>"},{"location":"api-reference/python-api/#llmconfighuggingfaceapi_key-modelnone-base_urlnone","title":"<code>LlmConfig.huggingface(api_key, model=None, base_url=None)</code>","text":"<p>Create HuggingFace provider configuration.</p> <pre><code>config = graphbit.LlmConfig.huggingface(\"hf_...\", \"microsoft/DialoGPT-medium\")\n# With default model\nconfig = graphbit.LlmConfig.huggingface(\"hf_...\")  # Uses microsoft/DialoGPT-medium\n# With custom endpoint\nconfig = graphbit.LlmConfig.huggingface(\"hf_...\", \"mistralai/Mistral-7B-Instruct-v0.1\", \n                                        base_url=\"https://my-endpoint.huggingface.co\")\n</code></pre> <p>Parameters: - <code>api_key</code> (str): HuggingFace API key - <code>model</code> (str, optional): Model name. Default: \"microsoft/DialoGPT-medium\" - <code>base_url</code> (str, optional): Custom API endpoint. Default: HuggingFace Inference API</p> <p>Returns: <code>LlmConfig</code> instance</p>"},{"location":"api-reference/python-api/#llmconfigollamamodelnone","title":"<code>LlmConfig.ollama(model=None)</code>","text":"<p>Create Ollama provider configuration.</p> <pre><code>config = graphbit.LlmConfig.ollama(\"llama3.2\")\n# With default model\nconfig = graphbit.LlmConfig.ollama()  # Uses llama3.2\n</code></pre> <p>Parameters: - <code>model</code> (str, optional): Model name. Default: \"llama3.2\"</p> <p>Returns: <code>LlmConfig</code> instance</p>"},{"location":"api-reference/python-api/#instance-methods","title":"Instance Methods","text":""},{"location":"api-reference/python-api/#provider","title":"<code>provider()</code>","text":"<p>Get the provider name.</p> <pre><code>provider = config.provider()  # \"openai\", \"anthropic\", \"ollama\"\n</code></pre>"},{"location":"api-reference/python-api/#model","title":"<code>model()</code>","text":"<p>Get the model name.</p> <pre><code>model = config.model()  # \"gpt-4o-mini\", \"claude-3-5-sonnet-20241022\", etc.\n</code></pre>"},{"location":"api-reference/python-api/#llm-client","title":"LLM Client","text":""},{"location":"api-reference/python-api/#llmclient","title":"<code>LlmClient</code>","text":"<p>Production-grade LLM client with resilience patterns.</p>"},{"location":"api-reference/python-api/#constructor","title":"Constructor","text":""},{"location":"api-reference/python-api/#llmclientconfig-debugnone","title":"<code>LlmClient(config, debug=None)</code>","text":"<p>Create a new LLM client.</p> <pre><code>client = graphbit.LlmClient(config)\n# With debugging\nclient = graphbit.LlmClient(config, debug=True)\n</code></pre> <p>Parameters: - <code>config</code> (LlmConfig): LLM configuration - <code>debug</code> (bool, optional): Enable debug mode. Default: False</p>"},{"location":"api-reference/python-api/#methods","title":"Methods","text":""},{"location":"api-reference/python-api/#completeprompt-max_tokensnone-temperaturenone","title":"<code>complete(prompt, max_tokens=None, temperature=None)</code>","text":"<p>Synchronous completion with resilience.</p> <pre><code>response = client.complete(\"Write a short story about a robot\")\nprint(response)\n\n# With parameters\nresponse = client.complete(\n    \"Explain quantum computing\",\n    max_tokens=500,\n    temperature=0.7\n)\n</code></pre> <p>Parameters: - <code>prompt</code> (str): Input prompt - <code>max_tokens</code> (int, optional): Maximum tokens to generate (1-100000) - <code>temperature</code> (float, optional): Sampling temperature (0.0-2.0)</p> <p>Returns: <code>str</code> - Generated text Raises: <code>ValueError</code> for invalid parameters</p>"},{"location":"api-reference/python-api/#complete_asyncprompt-max_tokensnone-temperaturenone","title":"<code>complete_async(prompt, max_tokens=None, temperature=None)</code>","text":"<p>Asynchronous completion with full resilience.</p> <pre><code>import asyncio\n\nasync def generate():\n    response = await client.complete_async(\"Tell me a joke\")\n    return response\n\nresult = asyncio.run(generate())\n</code></pre> <p>Parameters: Same as <code>complete()</code> Returns: <code>Awaitable[str]</code> - Generated text</p>"},{"location":"api-reference/python-api/#complete_batchprompts-max_tokensnone-temperaturenone-max_concurrencynone","title":"<code>complete_batch(prompts, max_tokens=None, temperature=None, max_concurrency=None)</code>","text":"<p>Ultra-fast batch processing with controlled concurrency.</p> <pre><code>import asyncio\n\nprompts = [\n    \"Summarize AI trends\",\n    \"Explain blockchain\",\n    \"Describe quantum computing\"\n]\n\nasync def batch_generate():\n    responses = await client.complete_batch(prompts, max_concurrency=5)\n    return responses\n\nresults = asyncio.run(batch_generate())\n</code></pre> <p>Parameters: - <code>prompts</code> (List[str]): List of prompts (max 1000) - <code>max_tokens</code> (int, optional): Maximum tokens per prompt - <code>temperature</code> (float, optional): Sampling temperature - <code>max_concurrency</code> (int, optional): Maximum concurrent requests. Default: CPU count * 2</p> <p>Returns: <code>Awaitable[List[str]]</code> - List of generated responses</p>"},{"location":"api-reference/python-api/#chat_optimizedmessages-max_tokensnone-temperaturenone","title":"<code>chat_optimized(messages, max_tokens=None, temperature=None)</code>","text":"<p>Optimized chat completion with message validation.</p> <pre><code>import asyncio\n\nmessages = [\n    (\"system\", \"You are a helpful assistant\"),\n    (\"user\", \"What is Python?\"),\n    (\"assistant\", \"Python is a programming language\"),\n    (\"user\", \"Tell me more about its features\")\n]\n\nasync def chat():\n    response = await client.chat_optimized(messages)\n    return response\n\nresult = asyncio.run(chat())\n</code></pre> <p>Parameters: - <code>messages</code> (List[Tuple[str, str]]): List of (role, content) tuples - <code>max_tokens</code> (int, optional): Maximum tokens to generate - <code>temperature</code> (float, optional): Sampling temperature</p> <p>Returns: <code>Awaitable[str]</code> - Generated response</p>"},{"location":"api-reference/python-api/#complete_streamprompt-max_tokensnone-temperaturenone","title":"<code>complete_stream(prompt, max_tokens=None, temperature=None)</code>","text":"<p>Stream completion (alias for async complete).</p> <pre><code>import asyncio\n\nasync def stream():\n    response = await client.complete_stream(\"Write a poem\")\n    return response\n\nresult = asyncio.run(stream())\n</code></pre>"},{"location":"api-reference/python-api/#get_stats","title":"<code>get_stats()</code>","text":"<p>Get comprehensive client statistics.</p> <pre><code>stats = client.get_stats()\nprint(f\"Total requests: {stats['total_requests']}\")\nprint(f\"Success rate: {stats['success_rate']}\")\nprint(f\"Average response time: {stats['average_response_time_ms']}ms\")\n</code></pre> <p>Returns: <code>dict</code> - Dictionary containing performance metrics</p>"},{"location":"api-reference/python-api/#warmup","title":"<code>warmup()</code>","text":"<p>Warm up the client to avoid initialization overhead.</p> <pre><code>import asyncio\n\nasync def prepare():\n    await client.warmup()\n    print(\"Client warmed up\")\n\nasyncio.run(prepare())\n</code></pre>"},{"location":"api-reference/python-api/#reset_stats","title":"<code>reset_stats()</code>","text":"<p>Reset client statistics.</p> <pre><code>client.reset_stats()\n</code></pre>"},{"location":"api-reference/python-api/#embeddings","title":"Embeddings","text":""},{"location":"api-reference/python-api/#embeddingconfig","title":"<code>EmbeddingConfig</code>","text":"<p>Configuration for embedding providers.</p>"},{"location":"api-reference/python-api/#static-methods_1","title":"Static Methods","text":""},{"location":"api-reference/python-api/#embeddingconfigopenaiapi_key-modelnone","title":"<code>EmbeddingConfig.openai(api_key, model=None)</code>","text":"<p>Create OpenAI embeddings configuration.</p> <pre><code>config = graphbit.EmbeddingConfig.openai(\"sk-...\", \"text-embedding-3-small\")\n# With default model\nconfig = graphbit.EmbeddingConfig.openai(\"sk-...\")  # Uses text-embedding-3-small\n</code></pre> <p>Parameters: - <code>api_key</code> (str): OpenAI API key - <code>model</code> (str, optional): Model name. Default: \"text-embedding-3-small\"</p>"},{"location":"api-reference/python-api/#embeddingconfighuggingfaceapi_key-model","title":"<code>EmbeddingConfig.huggingface(api_key, model)</code>","text":"<p>Create HuggingFace embeddings configuration.</p> <pre><code>config = graphbit.EmbeddingConfig.huggingface(\"hf_...\", \"sentence-transformers/all-MiniLM-L6-v2\")\n</code></pre> <p>Parameters: - <code>api_key</code> (str): HuggingFace API token - <code>model</code> (str): Model name from HuggingFace hub</p>"},{"location":"api-reference/python-api/#embeddingclient","title":"<code>EmbeddingClient</code>","text":"<p>Client for generating text embeddings.</p>"},{"location":"api-reference/python-api/#constructor_1","title":"Constructor","text":""},{"location":"api-reference/python-api/#embeddingclientconfig","title":"<code>EmbeddingClient(config)</code>","text":"<p>Create embedding client.</p> <pre><code>client = graphbit.EmbeddingClient(config)\n</code></pre>"},{"location":"api-reference/python-api/#methods_1","title":"Methods","text":""},{"location":"api-reference/python-api/#embedtext","title":"<code>embed(text)</code>","text":"<p>Generate embedding for single text.</p> <pre><code>embedding = client.embed(\"Hello world\")\nprint(f\"Embedding dimension: {len(embedding)}\")\n</code></pre> <p>Parameters: - <code>text</code> (str): Input text</p> <p>Returns: <code>List[float]</code> - Embedding vector</p>"},{"location":"api-reference/python-api/#embed_manytexts","title":"<code>embed_many(texts)</code>","text":"<p>Generate embeddings for multiple texts.</p> <pre><code>texts = [\"Text 1\", \"Text 2\", \"Text 3\"]\nembeddings = client.embed_many(texts)\nprint(f\"Generated {len(embeddings)} embeddings\")\n</code></pre> <p>Parameters: - <code>texts</code> (List[str]): List of input texts</p> <p>Returns: <code>List[List[float]]</code> - List of embedding vectors</p>"},{"location":"api-reference/python-api/#similaritya-b-static","title":"<code>similarity(a, b)</code> (static)","text":"<p>Calculate cosine similarity between two embeddings.</p> <pre><code>similarity = graphbit.EmbeddingClient.similarity(embed1, embed2)\nprint(f\"Similarity: {similarity}\")\n</code></pre> <p>Parameters: - <code>a</code> (List[float]): First embedding vector - <code>b</code> (List[float]): Second embedding vector</p> <p>Returns: <code>float</code> - Cosine similarity (-1.0 to 1.0)</p>"},{"location":"api-reference/python-api/#workflow-components","title":"Workflow Components","text":""},{"location":"api-reference/python-api/#node","title":"<code>Node</code>","text":"<p>Factory class for creating different types of workflow nodes.</p>"},{"location":"api-reference/python-api/#static-methods_2","title":"Static Methods","text":""},{"location":"api-reference/python-api/#nodeagentname-prompt-agent_idnone","title":"<code>Node.agent(name, prompt, agent_id=None)</code>","text":"<p>Create an AI agent node.</p> <pre><code>agent = graphbit.Node.agent(\n    name=\"Content Analyzer\",\n    prompt=\"Analyze the sentiment of: {input}\",\n    agent_id=\"analyzer\"  # Optional, auto-generated if not provided\n)\n</code></pre> <p>Parameters: - <code>name</code> (str): Human-readable node name - <code>prompt</code> (str): LLM prompt template with variables - <code>agent_id</code> (str, optional): Unique agent identifier. Auto-generated if not provided</p> <p>Returns: <code>Node</code> instance</p>"},{"location":"api-reference/python-api/#nodetransformname-transformation","title":"<code>Node.transform(name, transformation)</code>","text":"<p>Create a data transformation node.</p> <pre><code>transformer = graphbit.Node.transform(\n    name=\"Uppercase\",\n    transformation=\"uppercase\"\n)\n</code></pre> <p>Parameters: - <code>name</code> (str): Node name - <code>transformation</code> (str): Transformation type</p> <p>Returns: <code>Node</code> instance</p>"},{"location":"api-reference/python-api/#nodeconditionname-expression","title":"<code>Node.condition(name, expression)</code>","text":"<p>Create a condition node for branching logic.</p> <pre><code>condition = graphbit.Node.condition(\n    name=\"Quality Check\",\n    expression=\"quality_score &gt; 0.8\"\n)\n</code></pre> <p>Parameters: - <code>name</code> (str): Node name - <code>expression</code> (str): Boolean expression to evaluate</p> <p>Returns: <code>Node</code> instance</p>"},{"location":"api-reference/python-api/#instance-methods_1","title":"Instance Methods","text":""},{"location":"api-reference/python-api/#id","title":"<code>id()</code>","text":"<p>Get the node ID.</p>"},{"location":"api-reference/python-api/#name","title":"<code>name()</code>","text":"<p>Get the node name.</p>"},{"location":"api-reference/python-api/#workflow","title":"<code>Workflow</code>","text":"<p>Represents a complete workflow.</p>"},{"location":"api-reference/python-api/#constructor_2","title":"Constructor","text":""},{"location":"api-reference/python-api/#workflowname","title":"<code>Workflow(name)</code>","text":"<p>Create a new workflow.</p> <pre><code>workflow = graphbit.Workflow(\"My Workflow\")\n</code></pre>"},{"location":"api-reference/python-api/#methods_2","title":"Methods","text":""},{"location":"api-reference/python-api/#add_nodenode","title":"<code>add_node(node)</code>","text":"<p>Add a node to the workflow.</p> <pre><code>node_id = workflow.add_node(my_node)\nprint(f\"Added node with ID: {node_id}\")\n</code></pre> <p>Parameters: - <code>node</code> (Node): Node to add</p> <p>Returns: <code>str</code> - Unique node ID</p>"},{"location":"api-reference/python-api/#connectfrom_id-to_id","title":"<code>connect(from_id, to_id)</code>","text":"<p>Connect two nodes.</p> <pre><code>workflow.connect(node1_id, node2_id)\n</code></pre> <p>Parameters: - <code>from_id</code> (str): Source node ID - <code>to_id</code> (str): Target node ID</p>"},{"location":"api-reference/python-api/#validate","title":"<code>validate()</code>","text":"<p>Validate the workflow structure.</p> <pre><code>try:\n    workflow.validate()\n    print(\"Workflow is valid\")\nexcept Exception as e:\n    print(f\"Invalid workflow: {e}\")\n</code></pre>"},{"location":"api-reference/python-api/#workflowresult","title":"<code>WorkflowResult</code>","text":"<p>Contains workflow execution results.</p>"},{"location":"api-reference/python-api/#methods_3","title":"Methods","text":""},{"location":"api-reference/python-api/#is_success","title":"<code>is_success()</code>","text":"<p>Check if workflow completed successfully.</p> <pre><code>if result.is_success():\n    print(\"Workflow completed successfully\")\n</code></pre>"},{"location":"api-reference/python-api/#is_failed","title":"<code>is_failed()</code>","text":"<p>Check if workflow failed.</p> <pre><code>if result.is_failed():\n    print(\"Workflow failed\")\n</code></pre>"},{"location":"api-reference/python-api/#state","title":"<code>state()</code>","text":"<p>Get the workflow state.</p> <pre><code>state = result.state()\nprint(f\"Workflow state: {state}\")\n</code></pre>"},{"location":"api-reference/python-api/#execution_time_ms","title":"<code>execution_time_ms()</code>","text":"<p>Get execution time in milliseconds.</p> <pre><code>time_ms = result.execution_time_ms()\nprint(f\"Executed in {time_ms}ms\")\n</code></pre>"},{"location":"api-reference/python-api/#get_variablekey","title":"<code>get_variable(key)</code>","text":"<p>Get a variable value.</p> <pre><code>output = result.get_variable(\"output\")\nif output:\n    print(f\"Result: {output}\")\n</code></pre>"},{"location":"api-reference/python-api/#get_all_variables","title":"<code>get_all_variables()</code>","text":"<p>Get all variables as a dictionary.</p> <pre><code>all_vars = result.get_all_variables()\nfor key, value in all_vars.items():\n    print(f\"{key}: {value}\")\n</code></pre>"},{"location":"api-reference/python-api/#variables","title":"<code>variables()</code>","text":"<p>Get all variables as a list of tuples.</p> <pre><code>vars_list = result.variables()\nfor key, value in vars_list:\n    print(f\"{key}: {value}\")\n</code></pre>"},{"location":"api-reference/python-api/#workflow-execution","title":"Workflow Execution","text":""},{"location":"api-reference/python-api/#executor","title":"<code>Executor</code>","text":"<p>Production-grade workflow executor with comprehensive features.</p>"},{"location":"api-reference/python-api/#constructors","title":"Constructors","text":""},{"location":"api-reference/python-api/#executorconfig-lightweight_modenone-timeout_secondsnone-debugnone","title":"<code>Executor(config, lightweight_mode=None, timeout_seconds=None, debug=None)</code>","text":"<p>Create a basic executor.</p> <pre><code>executor = graphbit.Executor(llm_config)\n# With configuration\nexecutor = graphbit.Executor(\n    llm_config, \n    lightweight_mode=False,\n    timeout_seconds=300,\n    debug=True\n)\n</code></pre> <p>Parameters: - <code>config</code> (LlmConfig): LLM configuration - <code>lightweight_mode</code> (bool, optional): Enable lightweight mode (low latency) - <code>timeout_seconds</code> (int, optional): Execution timeout (1-3600 seconds) - <code>debug</code> (bool, optional): Enable debug mode</p>"},{"location":"api-reference/python-api/#executornew_high_throughputllm_config-timeout_secondsnone-debugnone-static","title":"<code>Executor.new_high_throughput(llm_config, timeout_seconds=None, debug=None)</code> (static)","text":"<p>Create executor optimized for high throughput.</p> <pre><code>executor = graphbit.Executor.new_high_throughput(llm_config)\n</code></pre>"},{"location":"api-reference/python-api/#executornew_low_latencyllm_config-timeout_secondsnone-debugnone-static","title":"<code>Executor.new_low_latency(llm_config, timeout_seconds=None, debug=None)</code> (static)","text":"<p>Create executor optimized for low latency.</p> <pre><code>executor = graphbit.Executor.new_low_latency(llm_config, timeout_seconds=30)\n</code></pre>"},{"location":"api-reference/python-api/#executornew_memory_optimizedllm_config-timeout_secondsnone-debugnone-static","title":"<code>Executor.new_memory_optimized(llm_config, timeout_seconds=None, debug=None)</code> (static)","text":"<p>Create executor optimized for memory usage.</p> <pre><code>executor = graphbit.Executor.new_memory_optimized(llm_config)\n</code></pre>"},{"location":"api-reference/python-api/#configuration-methods","title":"Configuration Methods","text":""},{"location":"api-reference/python-api/#configuretimeout_secondsnone-max_retriesnone-enable_metricsnone-debugnone","title":"<code>configure(timeout_seconds=None, max_retries=None, enable_metrics=None, debug=None)</code>","text":"<p>Configure the executor with new settings.</p> <pre><code>executor.configure(\n    timeout_seconds=600,\n    max_retries=5,\n    enable_metrics=True,\n    debug=False\n)\n</code></pre>"},{"location":"api-reference/python-api/#set_lightweight_modeenabled","title":"<code>set_lightweight_mode(enabled)</code>","text":"<p>Legacy method for backward compatibility.</p> <pre><code>executor.set_lightweight_mode(True)\n</code></pre>"},{"location":"api-reference/python-api/#is_lightweight_mode","title":"<code>is_lightweight_mode()</code>","text":"<p>Check if lightweight mode is enabled.</p> <pre><code>is_lightweight = executor.is_lightweight_mode()\n</code></pre>"},{"location":"api-reference/python-api/#execution-methods","title":"Execution Methods","text":""},{"location":"api-reference/python-api/#executeworkflow","title":"<code>execute(workflow)</code>","text":"<p>Execute a workflow synchronously.</p> <pre><code>result = executor.execute(workflow)\nif result.is_success():\n    print(\"Success!\")\n</code></pre> <p>Parameters: - <code>workflow</code> (Workflow): Workflow to execute</p> <p>Returns: <code>WorkflowResult</code> - Execution result</p>"},{"location":"api-reference/python-api/#run_asyncworkflow","title":"<code>run_async(workflow)</code>","text":"<p>Execute a workflow asynchronously.</p> <pre><code>import asyncio\n\nasync def run_workflow():\n    result = await executor.run_async(workflow)\n    return result\n\nresult = asyncio.run(run_workflow())\n</code></pre>"},{"location":"api-reference/python-api/#statistics-methods","title":"Statistics Methods","text":""},{"location":"api-reference/python-api/#get_stats_1","title":"<code>get_stats()</code>","text":"<p>Get comprehensive execution statistics.</p> <pre><code>stats = executor.get_stats()\nprint(f\"Total executions: {stats['total_executions']}\")\nprint(f\"Success rate: {stats['success_rate']}\")\nprint(f\"Average duration: {stats['average_duration_ms']}ms\")\n</code></pre>"},{"location":"api-reference/python-api/#reset_stats_1","title":"<code>reset_stats()</code>","text":"<p>Reset execution statistics.</p> <pre><code>executor.reset_stats()\n</code></pre>"},{"location":"api-reference/python-api/#get_execution_mode","title":"<code>get_execution_mode()</code>","text":"<p>Get the current execution mode.</p> <pre><code>mode = executor.get_execution_mode()\nprint(f\"Execution mode: {mode}\")\n</code></pre>"},{"location":"api-reference/python-api/#error-handling","title":"Error Handling","text":"<p>GraphBit uses standard Python exceptions:</p> <ul> <li><code>ValueError</code> - Invalid parameters or workflow structure</li> <li><code>RuntimeError</code> - Execution errors</li> <li><code>TimeoutError</code> - Operation timeouts</li> </ul> <pre><code>try:\n    result = executor.execute(workflow)\nexcept ValueError as e:\n    print(f\"Invalid workflow: {e}\")\nexcept RuntimeError as e:\n    print(f\"Execution failed: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api-reference/python-api/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/python-api/#basic-workflow","title":"Basic Workflow","text":"<pre><code>import graphbit\nimport os\n\n# Initialize\ngraphbit.init()\n\n# Configure LLM\nconfig = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"))\n\n# Create workflow\nworkflow = graphbit.Workflow(\"Analysis Workflow\")\nagent = graphbit.Node.agent(\n    \"Analyzer\", \n    \"Analyze the following text: {input}\"\n)\nnode_id = workflow.add_node(agent)\nworkflow.validate()\n\n# Execute\nexecutor = graphbit.Executor(config)\nresult = executor.execute(workflow)\n\nif result.is_success():\n    output = result.get_variable(\"output\")\n    print(f\"Analysis result: {output}\")\n</code></pre>"},{"location":"api-reference/python-api/#advanced-llm-usage","title":"Advanced LLM Usage","text":"<pre><code># Create client with debugging\nclient = graphbit.LlmClient(config, debug=True)\n\n# Batch processing\nprompts = [\n    \"Summarize: AI is transforming industries\",\n    \"Explain: Machine learning algorithms\",\n    \"Analyze: The future of automation\"\n]\n\nimport asyncio\nasync def process_batch():\n    responses = await client.complete_batch(prompts, max_concurrency=3)\n    return responses\n\nresults = asyncio.run(process_batch())\n\n# Chat conversation\nmessages = [\n    (\"system\", \"You are a helpful AI assistant\"),\n    (\"user\", \"What is machine learning?\"),\n    (\"assistant\", \"Machine learning is a subset of AI...\"),\n    (\"user\", \"Can you give me an example?\")\n]\n\nasync def chat():\n    response = await client.chat_optimized(messages, temperature=0.7)\n    return response\n\nchat_result = asyncio.run(chat())\n</code></pre>"},{"location":"api-reference/python-api/#high-performance-execution","title":"High-Performance Execution","text":"<pre><code># Create high-throughput executor\nexecutor = graphbit.Executor.new_high_throughput(\n    llm_config, \n    timeout_seconds=600,\n    debug=False\n)\n\n# Configure for production\nexecutor.configure(\n    timeout_seconds=300,\n    max_retries=3,\n    enable_metrics=True\n)\n\n# Execute workflow\nresult = executor.execute(workflow)\n\n# Monitor performance\nstats = executor.get_stats()\nprint(f\"Execution stats: {stats}\")\n</code></pre>"},{"location":"api-reference/python-api/#embeddings-usage","title":"Embeddings Usage","text":"<pre><code># Configure embeddings\nembed_config = graphbit.EmbeddingConfig.openai(os.getenv(\"OPENAI_API_KEY\"))\nembed_client = graphbit.EmbeddingClient(embed_config)\n\n# Generate embeddings\ntexts = [\"Hello world\", \"Goodbye world\", \"Machine learning\"]\nembeddings = embed_client.embed_many(texts)\n\n# Calculate similarity\nsimilarity = graphbit.EmbeddingClient.similarity(embeddings[0], embeddings[1])\nprint(f\"Similarity between texts: {similarity}\")\n</code></pre>"},{"location":"connector/aws_boto3/","title":"AWS Boto3 Integration with Graphbit","text":""},{"location":"connector/aws_boto3/#overview","title":"Overview","text":"<p>This guide demonstrates how to integrate AWS services with the Graphbit ecosystem using the <code>boto3</code> library. We provide a comprehensive example using S3 (object storage) and DynamoDB (NoSQL database), including file upload, download, update, and content extraction for embeddings. You can use similar patterns to connect and interact with other AWS services through Boto3 in your Graphbit-powered workflows.</p>"},{"location":"connector/aws_boto3/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS account with access to the services you want to use (e.g., S3, DynamoDB, etc.)</li> <li>AWS credentials configured (via environment variables, AWS CLI, or IAM roles)</li> <li>Python environment with <code>boto3</code> and <code>graphbit</code> installed:   <code>bash   pip install boto3 graphbit</code></li> <li>OpenAI API Key for embeddings</li> <li>Environment variable for your OpenAI API key:   <code>bash   OPENAI_API_KEY=\"your_openai_api_key_here\"</code></li> </ul>"},{"location":"connector/aws_boto3/#step-1-connect-to-aws-services-with-boto3","title":"Step 1: Connect to AWS Services with Boto3","text":"<p>Below, we show how to connect to S3 and DynamoDB. You can use the same approach to connect to other AWS services supported by Boto3.</p> <pre><code>import boto3\n\nREGION = '&lt;your-region&gt;'\nBUCKET_NAME = '&lt;your-bucket-name&gt;'\nTABLE_NAME = '&lt;your-table-name&gt;'\n\n# Connect to S3\ns3 = boto3.client('s3', region_name=REGION)\nprint(f\"Connected to S3 in region '{REGION}'.\")\n\n# Connect to DynamoDB\ndynamodb = boto3.resource('dynamodb', region_name=REGION)\ntable = dynamodb.Table(TABLE_NAME)\nprint(f\"Connected to DynamoDB table '{TABLE_NAME}' in region '{REGION}'.\")\n</code></pre> <p>Note: For other AWS services, refer to the Boto3 documentation for the appropriate client or resource initialization.</p>"},{"location":"connector/aws_boto3/#step-2-s3-file-upload-download-update-and-listing","title":"Step 2: S3 File Upload, Download, Update, and Listing","text":"<p>This example demonstrates uploading a file to S3, listing objects, downloading and reading a file, appending new content, and re-uploading the updated file. These patterns can be used for other S3 operations as well.</p> <pre><code># Create and upload a test file\nwith open('test_file.txt', 'w') as f:\n    f.write('Hello from Graphbit integration demo!')\n\ns3.upload_file('test_file.txt', BUCKET_NAME, 'test_file.txt')\nprint(f\"Uploaded 'test_file.txt' to S3 bucket '{BUCKET_NAME}' as 'test_file.txt'.\")\n\n# List objects in S3 bucket\nresponse = s3.list_objects_v2(Bucket=BUCKET_NAME)\nfor obj in response.get('Contents', []):\n    print(\" -\", obj['Key'])\n\n# Download and read the file from S3\ns3.download_file(BUCKET_NAME, 'test_file.txt', 'downloaded_file.txt')\nwith open('downloaded_file.txt', 'r') as f:\n    content = f.read()\n    print(\"Content of file from S3:\")\n    print(content)\n\n# Append new lines to the file and re-upload\nbatch_texts = [\n    \"This is a sample document for vector search.\",\n    \"Graph databases are great for relationships.\",\n    \"Vector search enables semantic retrieval.\",\n    \"OpenAI provides powerful embedding models.\",\n]\nwith open('downloaded_file.txt', 'a') as f:\n    for line in batch_texts:\n        f.write('\\n' + line)\ns3.upload_file('downloaded_file.txt', BUCKET_NAME, 'test_file.txt')\nprint(\"Updated file uploaded to S3.\")\n\n# Download and print updated file\ns3.download_file(BUCKET_NAME, 'test_file.txt', 'downloaded_file.txt')\nwith open('downloaded_file.txt', 'r') as f:\n    print(\"\\nContent of file from S3 after update:\")\n    print(f.read())\n</code></pre>"},{"location":"connector/aws_boto3/#step-3-embedding-with-graphbit-and-storing-in-dynamodb","title":"Step 3: Embedding with Graphbit and Storing in DynamoDB","text":"<p>You can use Graphbit to generate vector embeddings and store them in DynamoDB. In this workflow, we extract the second line from the file downloaded from S3 for a single embedding.</p> <pre><code>\nimport os\nfrom decimal import Decimal\nfrom graphbit import EmbeddingConfig as gb_ecg, EmbeddingClient as gb_ect\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n\nembedding_config = gb_ecg.openai(OPENAI_API_KEY, \"text-embedding-3-small\")\nembedding_client = gb_ect(embedding_config)\n\ndef float_list_to_decimal(lst):\n    return [Decimal(str(x)) for x in lst]\n\n# Extract the second line for single embedding\nwith open('downloaded_file.txt', 'r') as f:\n    lines = f.readlines()\nif len(lines) &gt;= 2:\n    text = lines[1].strip()\n    print(\"Extracted second sentence:\", text)\nelse:\n    print(\"File does not have a second sentence.\")\nembedding = embedding_client.embed(text)\ntable.put_item(Item={\n    \"itemID\": \"item-embedding-1\",\n    \"embedding\": float_list_to_decimal(embedding),\n    \"metadata\": {\"category\": \"test\"}\n})\nprint(\"Stored single embedding in DynamoDB.\")\n</code></pre>"},{"location":"connector/aws_boto3/#step-4-batch-embedding-example-extracting-from-file","title":"Step 4: Batch Embedding Example (Extracting from File)","text":"<p>This example demonstrates how to generate and store multiple embeddings in DynamoDB in a batch. Here, we extract lines 3 to 5 from the file downloaded from S3 for batch embedding.</p> <pre><code># Extract lines 3 to 5 for batch embedding\nwith open('downloaded_file.txt', 'r') as f:\n    lines = f.readlines()\nif len(lines) &gt;= 5:\n    batch_texts = [lines[2].strip(), lines[3].strip(), lines[4].strip()]\n    print(\"batch_texts =\", batch_texts)\nelse:\n    print(\"File does not have enough lines.\")\nbatch_embeddings = embedding_client.embed_many(batch_texts)\nbatch_items = [\n    {\n        \"itemID\": f\"batch_{i}\",\n        \"embedding\": float_list_to_decimal(emb),\n        \"metadata\": {\"text\": text}\n    }\n    for i, (text, emb) in enumerate(zip(batch_texts, batch_embeddings))\n]\nwith table.batch_writer() as batch:\n    for item in batch_items:\n        batch.put_item(Item=item)\nprint(f\"Inserted {len(batch_items)} batch embeddings.\")\n</code></pre>"},{"location":"connector/aws_boto3/#step-5-vector-search-using-graphbit","title":"Step 5: Vector Search using Graphbit","text":"<p>This example shows how to perform a simple vector similarity search using embeddings stored in DynamoDB. You can adapt this logic for other AWS data stores or search services.</p> <pre><code>query_embedding = embedding_client.embed(\"Find documents related to vector search.\")\nscan_resp = table.scan()\nitems = scan_resp.get('Items', [])\nbest_score = -1\nbest_item = None\nfor item in items:\n    if 'embedding' in item:\n        score = gb_ect.similarity(query_embedding, item['embedding'])\n        if score &gt; best_score:\n            best_score = score\n            best_item = item\nif best_item:\n    print(f\"Most similar itemID: {best_item['itemID']} (score: {best_score:.4f})\")\nelse:\n    print(\"No embeddings found in table.\")\n</code></pre> <p>This connector pattern enables you to use AWS Boto3 to integrate Graphbit with a wide range of AWS services. While this guide uses S3 and DynamoDB as examples, you can extend these patterns to other AWS services to build powerful, cloud-native AI workflows. </p>"},{"location":"connector/chromadb_integration/","title":"ChromaDB Integration with Graphbit","text":""},{"location":"connector/chromadb_integration/#overview","title":"Overview","text":"<p>This guide explains how to use Graphbit to generate embeddings and perform similarity search in ChromaDB, a fast, open-source embedding database for AI applications. You can use ChromaDB to store, index, and search high-dimensional vectors for semantic search and retrieval-augmented generation.</p>"},{"location":"connector/chromadb_integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>ChromaDB installed and running (see ChromaDB documentation).</li> <li>OpenAI API Key (or another supported embedding provider).</li> <li>Graphbit installed and configured (see installation guide).</li> <li>Python environment with <code>chromadb</code>, <code>graphbit</code>, and optionally <code>python-dotenv</code> installed.</li> <li>.env file in your project root with the following variables:   <code>env   OPENAI_API_KEY=your_openai_api_key_here</code></li> </ul>"},{"location":"connector/chromadb_integration/#step-1-connect-to-chromadb-and-create-collection","title":"Step 1: Connect to ChromaDB and Create Collection","text":"<p>Set up the ChromaDB client and ensure the collection exists:</p> <pre><code>import os\nfrom chromadb import Client\nfrom chromadb.config import Settings\n\nchromadb_client = Client()\n\nif \"chromadb_integration\" in [c.name for c in chromadb_client.list_collections()]:\n    collection = chromadb_client.get_collection(name=\"chromadb_integration\")\nelse:\n    collection = chromadb_client.create_collection(\n        name=\"chromadb_integration\",\n        metadata={\"hnsw:space\": \"cosine\"}\n    )\n</code></pre>"},{"location":"connector/chromadb_integration/#step-2-generate-embeddings-using-graphbit","title":"Step 2: Generate Embeddings using Graphbit","text":"<p>Use Graphbit to generate embeddings for your texts:</p> <pre><code>import graphbit\n\ngraphbit.init()\nembedding_client = graphbit.EmbeddingClient(\n    graphbit.EmbeddingConfig.openai(\n        model=\"text-embedding-3-small\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n    )\n)\n\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"ChromaDB is a fast, open-source embedding database for AI applications.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeds = embedding_client.embed_many(texts)\n</code></pre>"},{"location":"connector/chromadb_integration/#step-3-insert-embeddings-into-chromadb","title":"Step 3: Insert Embeddings into ChromaDB","text":"<p>Insert the generated embeddings into the ChromaDB collection:</p> <pre><code>collection.add(\n    documents=texts,\n    embeddings=embeds,\n    ids=[f\"doc_{i}\" for i in range(len(texts))],\n    metadatas=[{\"source\": \"initial_knowledge\", \"chunk_id\": i} for i in range(len(texts))]\n)\n</code></pre>"},{"location":"connector/chromadb_integration/#step-4-vector-search-similarity-search","title":"Step 4: Vector Search (Similarity Search)","text":"<p>Embed your query and search for similar vectors in ChromaDB:</p> <pre><code>query = \"What is GraphBit?\"\nquery_embedding = embedding_client.embed(query)\n\nquery_result = collection.query(\n    query_embeddings=[query_embedding],\n    n_results=3,\n    include=[\"documents\", \"metadatas\", \"distances\"],\n)\n\nids = query_result[\"ids\"][0]\ndocs = query_result[\"documents\"][0]\ndistances = query_result[\"distances\"][0]\nscores = [1 - d for d in distances]\n\nfor doc_id, text, score in zip(ids, docs, scores):\n    print(f\"ID: {doc_id}\\nScore: {score:.4f}\\nText: {text}\\n---\")\n</code></pre>"},{"location":"connector/chromadb_integration/#full-example","title":"Full Example","text":"<pre><code>import os\nfrom chromadb import Client\nimport graphbit\n\ngraphbit.init()\nembedding_client = graphbit.EmbeddingClient(\n    graphbit.EmbeddingConfig.openai(\n        model=\"text-embedding-3-small\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n    )\n)\n\nchromadb_client = Client()\nif \"chromadb_integration\" in [c.name for c in chromadb_client.list_collections()]:\n    collection = chromadb_client.get_collection(name=\"chromadb_integration\")\nelse:\n    collection = chromadb_client.create_collection(\n        name=\"chromadb_integration\",\n        metadata={\"hnsw:space\": \"cosine\"}\n    )\n\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"ChromaDB is a fast, open-source embedding database for AI applications.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeds = embedding_client.embed_many(texts)\n\ncollection.add(\n    documents=texts,\n    embeddings=embeds,\n    ids=[f\"doc_{i}\" for i in range(len(texts))],\n    metadatas=[{\"source\": \"initial_knowledge\", \"chunk_id\": i} for i in range(len(texts))]\n)\n\nquery = \"What is GraphBit?\"\nquery_embedding = embedding_client.embed(query)\n\nquery_result = collection.query(\n    query_embeddings=[query_embedding],\n    n_results=3,\n    include=[\"documents\", \"metadatas\", \"distances\"],\n)\n\nids = query_result[\"ids\"][0]\ndocs = query_result[\"documents\"][0]\ndistances = query_result[\"distances\"][0]\nscores = [1 - d for d in distances]\n\nfor doc_id, text, score in zip(ids, docs, scores):\n    print(f\"ID: {doc_id}\\nScore: {score:.4f}\\nText: {text}\\n---\")\n</code></pre> <p>This integration enables you to leverage Graphbit's embedding capabilities with ChromaDB for scalable, open-source semantic search and retrieval workflows. </p>"},{"location":"connector/faiss_integration/","title":"FAISS Integration with Graphbit","text":""},{"location":"connector/faiss_integration/#overview","title":"Overview","text":"<p>This guide explains how to use Graphbit to generate embeddings and perform similarity search using FAISS (Facebook AI Similarity Search), a library for efficient similarity search and clustering of dense vectors. You can use FAISS to store, index, and search high-dimensional vectors for semantic search and retrieval-augmented generation.</p>"},{"location":"connector/faiss_integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenAI API Key (or another supported embedding provider).</li> <li>Graphbit installed and configured (see installation guide).</li> <li>Python environment with <code>faiss-cpu</code>, <code>numpy</code>, <code>graphbit</code>, and optionally installed.</li> </ul>"},{"location":"connector/faiss_integration/#step-1-initialize-graphbit","title":"Step 1: Initialize Graphbit","text":"<p>Set up Graphbit:</p> <pre><code>from graphbit import EmbeddingConfig, EmbeddingClient\n\nembedding_client = EmbeddingClient(\n    EmbeddingConfig.openai(\n        model=\"text-embedding-3-small\",\n        api_key=\"openai_api_key\",\n    )\n)\n</code></pre>"},{"location":"connector/faiss_integration/#-","title":"---","text":""},{"location":"connector/faiss_integration/#step-2-generate-embeddings","title":"Step 2: Generate Embeddings","text":"<p>Generate embeddings for your texts:</p> <pre><code>import numpy as np\n\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"FAISS is a library for efficient similarity search and clustering of dense vectors.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeddings = embedding_client.embed_many(texts)\nembeddings = np.array(embeddings).astype('float32')\n</code></pre>"},{"location":"connector/faiss_integration/#step-3-create-faiss-index","title":"Step 3: Create FAISS Index","text":"<p>Create a FAISS index for similarity search:</p> <pre><code>import faiss\n\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatIP(dimension)\n</code></pre>"},{"location":"connector/faiss_integration/#step-4-add-embeddings-to-faiss-index","title":"Step 4: Add Embeddings to FAISS Index","text":"<p>Add the generated embeddings to the FAISS index:</p> <pre><code>index.add(embeddings)\n</code></pre>"},{"location":"connector/faiss_integration/#step-5-vector-search-similarity-search","title":"Step 5: Vector Search (Similarity Search)","text":"<p>Embed your query and search for similar vectors in FAISS:</p> <pre><code>query = \"What is GraphBit?\"\nquery_embedding = embedding_client.embed(query)\nquery_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)\n\nscores, indices = index.search(query_embedding, k=3)\n</code></pre>"},{"location":"connector/faiss_integration/#full-example","title":"Full Example","text":"<pre><code>import faiss\nimport numpy as np\nfrom graphbit import EmbeddingConfig, EmbeddingClient\n\nembedding_client = EmbeddingClient(\n    EmbeddingConfig.openai(\n        model=\"text-embedding-3-small\",\n        api_key=\"openai_api_key\",\n    )\n)\n\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"FAISS is a library for efficient similarity search and clustering of dense vectors.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeddings = embedding_client.embed_many(texts)\nembeddings = np.array(embeddings).astype('float32')\n\ndimension = embeddings.shape[1]\nindex = faiss.IndexFlatIP(dimension)\nindex.add(embeddings)\n\nquery = \"What is GraphBit?\"\nquery_embedding = embedding_client.embed(query)\nquery_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)\n\nscores, indices = index.search(query_embedding, k=3)\n\nfor idx, score in zip(indices[0], scores[0]):\n    print(f\"ID: doc_{idx}\\nScore: {score:.4f}\\nText: {texts[idx]}\\n---\")\n</code></pre> <p>This integration enables you to leverage Graphbit's embedding capabilities with FAISS for efficient, scalable semantic search and retrieval workflows. </p>"},{"location":"connector/google_search_api/","title":"Google Search API Integration with Graphbit","text":""},{"location":"connector/google_search_api/#overview","title":"Overview","text":"<p>This guideline explains how to connect the Google Search API to Graphbit, enabling Graphbit to orchestrate the retrieval, processing, and utilization of web search results in your AI workflows. This integration allows you to automate research, enrich LLM prompts, and build intelligent pipelines that leverage real-time web data.</p>"},{"location":"connector/google_search_api/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Custom Search API Key: Obtain from Google Cloud Console.</li> <li>Custom Search Engine (CSE) ID: Set up a CSE at Google CSE and ensure it is configured to search the public web.</li> <li>OpenAI API Key: For LLM summarization (or another supported LLM provider).</li> <li>Graphbit installed and configured (see installation guide).</li> <li>Python environment with <code>requests</code>, <code>python-dotenv</code>, and <code>graphbit</code> installed.</li> <li>.env file in your project root with the following variables:   <code>env   GOOGLE_API_KEY=your_google_api_key_here   GOOGLE_CSE_ID=your_search_engine_id_here   OPENAI_API_KEY=your_openai_api_key_here</code></li> </ul>"},{"location":"connector/google_search_api/#step-1-implement-the-google-search-connector","title":"Step 1: Implement the Google Search Connector","text":"<p>Define a function to query the Google Search API, loading credentials from environment variables:</p> <pre><code>import requests\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\nGOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n\ndef google_search(query):\n    url = \"https://www.googleapis.com/customsearch/v1\"\n    params = {\n        \"key\": GOOGLE_API_KEY,\n        \"cx\": GOOGLE_CSE_ID,\n        \"q\": query\n    }\n    response = requests.get(url, params=params)\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"connector/google_search_api/#step-2-process-the-search-results","title":"Step 2: Process the Search Results","text":"<p>Extract relevant information (title, link, and snippet) from the search results for downstream use. By default, only the top 3 results are included, but you can override this by specifying the max_snippets parameter:</p> <pre><code>def process_search_results(results, max_snippets=3):\n    \"\"\"\n    Extracts up to max_snippets search results (default: 3) as formatted strings.\n    \"\"\"\n    items = results.get(\"items\", [])[:max_snippets]\n    snippets = [\n        f\"{item['title']} ({item['link']}): {item['snippet']}\"\n        for item in items\n    ]\n    return \"\\n\\n\".join(snippets)\n</code></pre> <ul> <li>If you call <code>process_search_results(results)</code>, it will use the default of 3 results.</li> <li>To use a different number, call <code>process_search_results(results, max_snippets=10)</code> (for example).</li> </ul>"},{"location":"connector/google_search_api/#step-3-build-the-graphbit-workflow","title":"Step 3: Build the Graphbit Workflow","text":"<ol> <li> <p>Run the Google Search and process the results:</p> <p><code>python search_results = google_search(\"Graphbit open source\") snippets_text = process_search_results(search_results, max_snippets=10)</code></p> </li> <li> <p>Create a Graphbit agent node for summarization:</p> <p>```python from graphbit import Node, Workflow</p> <p>agent = Node.agent(     name=\"Summarizer\",     prompt=f\"Summarize these search results: {snippets_text}\" ) workflow = Workflow(\"Google Search Workflow\") workflow.add_node(agent) ```</p> </li> </ol>"},{"location":"connector/google_search_api/#step-4-orchestrate-and-execute-with-graphbit","title":"Step 4: Orchestrate and Execute with Graphbit","text":"<ol> <li> <p>Initialize Graphbit and configure your LLM:</p> <p><code>python import graphbit from dotenv import load_dotenv import os load_dotenv() graphbit.init() llm_config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\")) executor = graphbit.Executor(llm_config)</code></p> </li> <li> <p>Run the workflow and retrieve the summary:</p> <p><code>python result = executor.execute(workflow) if result.is_success():     print(\"Summary:\", result.get_variable(\"node_result_1\")) else:     print(\"Workflow failed:\", result.state())</code></p> </li> </ol>"},{"location":"connector/google_search_api/#full-example","title":"Full Example","text":"<pre><code>import requests\nimport graphbit\nfrom graphbit import Node, Workflow\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\nGOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\ndef google_search(query):\n    url = \"https://www.googleapis.com/customsearch/v1\"\n    params = {\"key\": GOOGLE_API_KEY, \"cx\": GOOGLE_CSE_ID, \"q\": query}\n    response = requests.get(url, params=params)\n    response.raise_for_status()\n    return response.json()\n\ndef process_search_results(results, max_snippets=10):\n    items = results.get(\"items\", [])[:max_snippets]\n    snippets = [\n        f\"{item['title']} ({item['link']}): {item['snippet']}\"\n        for item in items\n    ]\n    return \"\\n\\n\".join(snippets)\n\nsearch_results = google_search(\"Graphbit open source\")\nsnippets_text = process_search_results(search_results, max_snippets=10)\n\nagent = Node.agent(\n    name=\"Summarizer\",\n    prompt=f\"Summarize these search results: {snippets_text}\"\n)\nworkflow = Workflow(\"Google Search Workflow\")\nworkflow.add_node(agent)\n\ngraphbit.init()\nllm_config = graphbit.LlmConfig.openai(OPENAI_API_KEY)\nexecutor = graphbit.Executor(llm_config)\n\nresult = executor.execute(workflow)\nif result.is_success():\n    print(\"Summary:\", result.get_variable(\"node_result_1\"))\nelse:\n    print(\"Workflow failed:\", result.state())\n</code></pre> <p>This connector pattern enables you to seamlessly blend external web data into your AI workflows, orchestrated by Graphbit. </p>"},{"location":"connector/mariadb_integration/","title":"MariaDB Integration with Graphbit","text":""},{"location":"connector/mariadb_integration/#overview","title":"Overview","text":"<p>This guide explains how to use Graphbit to generate embeddings and perform similarity search in MariaDB using its native VECTOR type and vector functions. You can use both MariaDB's built-in vector search and manual similarity search in Python for debugging or prototyping.</p>"},{"location":"connector/mariadb_integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>MariaDB 11.4+ with native VECTOR support enabled.</li> <li>OpenAI API Key (or another supported embedding provider).</li> <li>Graphbit installed and configured (see installation guide).</li> <li>Python environment with <code>mariadb</code>, <code>graphbit</code>, and <code>numpy</code> installed.</li> <li>A MariaDB database and user with appropriate permissions.</li> </ul>"},{"location":"connector/mariadb_integration/#step-1-connect-to-mariadb","title":"Step 1: Connect to MariaDB","text":"<p>Establish a connection to your MariaDB instance:</p> <pre><code>import os\nimport mariadb\n\nconn = mariadb.connect(\n    user=os.getenv(\"DB_USER\", \"root\"),\n    password=os.getenv(\"DB_PASSWORD\", \"12345\"),\n    host=os.getenv(\"DB_HOST\", \"localhost\"),\n    port=3306,\n    database=os.getenv(\"DB_NAME\", \"vector_db\"),\n)\n</code></pre>"},{"location":"connector/mariadb_integration/#step-2-create-table","title":"Step 2: Create Table","text":"<p>Create a table with native VECTOR support:</p> <pre><code>cursor = conn.cursor()\n\ncursor.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS graphbit_vector (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    text TEXT NOT NULL,\n    embedding VECTOR(1536) NOT NULL,\n    metadata JSON\n);\n\"\"\")\nconn.commit()\n</code></pre>"},{"location":"connector/mariadb_integration/#step-3-generate-embeddings-using-graphbit","title":"Step 3: Generate Embeddings using Graphbit","text":"<p>Use Graphbit to generate embeddings for your texts:</p> <pre><code>import graphbit\n\ngraphbit.init()\nembedding_client = graphbit.EmbeddingClient(\n    graphbit.EmbeddingConfig.openai(\n        model=\"text-embedding-3-small\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n    )\n)\n\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"MariaDB supports native vector search for efficient AI similarity queries.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeds = embedding_client.embed_many(texts)\n</code></pre>"},{"location":"connector/mariadb_integration/#step-4-insert-embeddings-into-mariadb","title":"Step 4: Insert Embeddings into MariaDB","text":"<p>Insert the generated embeddings into the MariaDB table:</p> <pre><code>import array\nimport json\n\nfor txt, vec in zip(texts, embeds):\n    vec_bytes = array.array('f', vec).tobytes()\n    cursor.execute(\n        \"INSERT INTO graphbit_vector (text, embedding, metadata) VALUES (?, ?, ?)\",\n        (txt, vec_bytes, json.dumps({\"source\": \"graphbit\"}))\n    )\nconn.commit()\n</code></pre>"},{"location":"connector/mariadb_integration/#step-5-vector-search-using-sql-native-mariadb-vector-search","title":"Step 5: Vector Search using SQL (Native MariaDB Vector Search)","text":"<p>Embed your query and search for similar vectors in MariaDB using SQL:</p> <pre><code>query_vec = embedding_client.embed(\"What is GraphBit?\")\nquery_vec_bytes = array.array('f', query_vec).tobytes()\n\ncursor.execute(\"\"\"\nSELECT id, text, VEC_DISTANCE_COSINE(embedding, ?) AS score\nFROM graphbit_vector\nORDER BY score\nLIMIT 2;\n\"\"\", (query_vec_bytes,))\n\nrows = cursor.fetchall()\nfor id_, text, score in rows:\n    print(f\"ID: {id_}\\nSimilarity Score: {(1-score):.4f}\\nText: {text}\\n---\")\n</code></pre>"},{"location":"connector/mariadb_integration/#step-6-vector-search-example-graphbit-manual-similarity","title":"Step 6: Vector Search Example (Graphbit Manual Similarity)","text":"<p>You can also fetch all embeddings from MariaDB and perform similarity search in Python using Graphbit. This is useful for debugging or comparing results.</p> <pre><code>import ast\n\ncursor.execute(\"SELECT id, text, embedding, metadata FROM graphbit_vector;\")\nall_rows = cursor.fetchall()\n\nbest_score = -1\nbest_item = None\n\nfor row_id, text, embedding_vec_raw, metadata in all_rows:\n    if isinstance(embedding_vec_raw, str):\n        embedding_vec = ast.literal_eval(embedding_vec_raw)\n    elif isinstance(embedding_vec_raw, (bytes, bytearray)):\n        embedding_vec = array.array('f')\n        embedding_vec.frombytes(embedding_vec_raw)\n        embedding_vec = embedding_vec.tolist()\n\n    if len(embedding_vec) != len(query_vec):\n        print(f\"Skipping row {row_id} due to dimension mismatch.\")\n        continue\n\n    score = embedding_client.similarity(query_vec, embedding_vec)\n    print(f\"Row ID: {row_id}, Score: {score:.4f}, Text: {text}\")\n</code></pre>"},{"location":"connector/mariadb_integration/#full-example","title":"Full Example","text":"<pre><code>import os\nimport array\nimport json\nimport numpy as np\nimport mariadb\nimport graphbit\n\n# Step 1: Connect to MariaDB\nconn = mariadb.connect(\n    user=os.getenv(\"DB_USER\", \"root\"),\n    password=os.getenv(\"DB_PASSWORD\", \"12345\"),\n    host=os.getenv(\"DB_HOST\", \"localhost\"),\n    port=3306,\n    database=os.getenv(\"DB_NAME\", \"vector_db\"),\n)\ncursor = conn.cursor()\n\n# Step 2: Create Table\ncursor.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS graphbit_vector (\n    id INT AUTO_INCREMENT PRIMARY KEY,\n    text TEXT NOT NULL,\n    embedding VECTOR(1536) NOT NULL,\n    metadata JSON\n);\n\"\"\")\nconn.commit()\n\n# Step 3: Generate Embeddings using Graphbit\ngraphbit.init()\nembedding_client = graphbit.EmbeddingClient(\n    graphbit.EmbeddingConfig.openai(\n        model=\"text-embedding-3-small\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n    )\n)\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"Qdrant is an open-source vector database for similarity search.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeds = embedding_client.embed_many(texts)\n\n# Step 4: Insert Embeddings into MariaDB\nfor txt, vec in zip(texts, embeds):\n    vec_bytes = array.array('f', vec).tobytes()\n    cursor.execute(\n        \"INSERT INTO graphbit_vector (text, embedding, metadata) VALUES (?, ?, ?)\",\n        (txt, vec_bytes, json.dumps({\"source\": \"graphbit\"}))\n    )\nconn.commit()\n\n# Step 5: SQL vector search\nquery_vec = embedding_client.embed(\"What is GraphBit?\")\nquery_vec_bytes = array.array('f', query_vec).tobytes()\ncursor.execute(\"\"\"\nSELECT id, text, VEC_DISTANCE_COSINE(embedding, ?) AS score\nFROM graphbit_vector\nORDER BY score\nLIMIT 2;\n\"\"\", (query_vec_bytes,))\nrows = cursor.fetchall()\nfor id_, text, score in rows:\n    print(f\"[SQL] ID: {id_}\\nSimilarity Score: {(1-score):.4f}\\nText: {text}\\n---\")\n\n# Step 6: Manual similarity search\ncursor.execute(\"SELECT id, text, embedding FROM graphbit_vector\")\nrows = cursor.fetchall()\ndef decode_vector(vec_bytes):\n    arr = array.array('f')\n    arr.frombytes(vec_bytes)\n    return np.array(arr)\ntexts = []\nvectors = []\nids = []\nfor id_, text, vec_bytes in rows:\n    ids.append(id_)\n    texts.append(text)\n    vectors.append(decode_vector(vec_bytes))\nquery_vec = np.array(embedding_client.embed(\"What is GraphBit?\"))\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\nscores = [cosine_similarity(query_vec, vec) for vec in vectors]\nranked = sorted(zip(ids, texts, scores), key=lambda x: x[2], reverse=True)\nfor id_, text, score in ranked[:2]:\n    print(f\"[Manual] ID: {id_}\\nScore: {score:.4f}\\nText: {text}\\n---\")\n\n# Closing Mariadb\ncursor.close()\nconn.close()\n</code></pre> <p>This integration enables you to leverage Graphbit's embedding capabilities with MariaDB's native vector support for scalable, production-grade semantic search and retrieval workflows. </p>"},{"location":"connector/milvus_integration/","title":"Milvus Integration with Graphbit","text":""},{"location":"connector/milvus_integration/#overview","title":"Overview","text":"<p>This guide demonstrates how to connect Milvus, an open-source vector database, with Graphbit. Through this integration, you can store, index, and perform similarity search on high-dimensional embeddings generated by LLMs or embedding models. This enables use cases like semantic search, retrieval-augmented generation (RAG), and other AI-powered vector search workflows.</p>"},{"location":"connector/milvus_integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Milvus running locally or remotely (see Milvus documentation).</li> <li>OpenAI API Key: For embedding generation (or another supported embedding provider).</li> <li>Graphbit installed and configured (see installation guide).</li> <li>Python environment with <code>pymilvus</code>, <code>graphbit</code> installed.</li> </ul>"},{"location":"connector/milvus_integration/#step-1-configure-graphbit-embedding","title":"Step 1: Configure Graphbit Embedding","text":"<p>Configure Graphbit Embedding:</p> <pre><code>import os\nfrom graphbit import EmbeddingConfig, EmbeddingClient\n\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\nembedding_client = EmbeddingClient(\n    EmbeddingConfig.openai(\n        model=\"text-embedding-3-small\",\n        api_key=openai_api_key,\n    )\n)\n</code></pre>"},{"location":"connector/milvus_integration/#step-2-generate-embeddings","title":"Step 2: Generate Embeddings","text":"<p>Generate Embeddings:</p> <pre><code>texts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"FAISS is a library for efficient similarity search and clustering of dense vectors.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeddings = embedding_client.embed_many(texts)\n</code></pre>"},{"location":"connector/milvus_integration/#step-3-initialize-milvus-client-and-create-collection","title":"Step 3: Initialize Milvus Client and Create Collection","text":"<p>Set up the Milvus client and ensure the collection is created:</p> <pre><code>from pymilvus import MilvusClient\n\nvectordb_client = MilvusClient(\"graphbit_vector.db\")\ndimension = len(embeddings[0])\n\ncollection_name = \"graphbit_collection\"\nif vectordb_client.has_collection(collection_name=collection_name):\n    vectordb_client.drop_collection(collection_name=collection_name)\n\nvectordb_client.create_collection(collection_name, dimension=dimension)\n</code></pre>"},{"location":"connector/milvus_integration/#step-4-insert-data-into-milvus","title":"Step 4: Insert Data into Milvus","text":"<p>Prepare and insert your embeddings with metadata into Milvus:</p> <pre><code>vectors = [\n    {\"id\": i, \"vector\": embedding, \"text\": texts[i], \"subject\": \"graphbit\"} \n    for i, embedding in enumerate(embeddings)\n]\n\nvectordb_client.insert(collection_name=collection_name, data=vectors)\n</code></pre>"},{"location":"connector/milvus_integration/#step-5-perform-similarity-search","title":"Step 5: Perform Similarity Search","text":"<p>Embed your query and search for similar vectors in Milvus:</p> <pre><code>queries = [\"What is GraphBit?\", \"What is Milvus?\"]\nquery_embeddings = embedding_client.embed_many(queries)\n\nsearch_results = vectordb_client.search(\n    collection_name=collection_name,\n    data=query_embeddings,\n    limit=3,\n    output_fields=[\"text\"],\n)\n\nfor idx, result in enumerate(search_results):\n    print(f\"Query {idx}: {queries[idx]}\")\n    for item in result:\n        print(f\"  id: {item['id']}, Text: {item.get('entity', {}).get('text', '')}, Score: {item.get('distance', 0):.4f}\")\n\n</code></pre>"},{"location":"connector/milvus_integration/#full-example","title":"Full Example","text":"<pre><code>import os\nfrom pymilvus import MilvusClient\nfrom graphbit import EmbeddingConfig, EmbeddingClient\n\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n\nembedding_client = EmbeddingClient(\n    EmbeddingConfig.openai(\n        model=\"text-embedding-3-small\",\n        api_key=openai_api_key,\n    )\n)\n\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"Milvus is an open-source vector database for scalable similarity search and AI applications.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeddings = embedding_client.embed_many(texts)\n\nvectordb_client = MilvusClient(\"graphbit_vector.db\")\ndimension = len(embeddings[0])\n\ncollection_name = \"graphbit_collection\"\nif vectordb_client.has_collection(collection_name=collection_name):\n    vectordb_client.drop_collection(collection_name=collection_name)\n\nvectordb_client.create_collection(collection_name, dimension=dimension)\n\nvectors = [\n    {\"id\": i, \"vector\": embedding, \"text\": texts[i]} for i, embedding in enumerate(embeddings)\n]\nvectordb_client.insert(collection_name=collection_name, data=vectors)\n\n# Query\nqueries = [\"What is GraphBit?\", \"What is Milvus?\"]\nquery_embeddings = embedding_client.embed_many(queries)\n\nsearch_results = vectordb_client.search(\n    collection_name=collection_name,\n    data=query_embeddings,\n    limit=3,\n    output_fields=[\"text\"],\n)\n\nfor idx, result in enumerate(search_results):\n    print(f\"Query {idx}: {queries[idx]}\")\n    for item in result:\n        print(f\"  id: {item['id']}, Text: {item.get('entity', {}).get('text', '')}, Score: {item.get('distance', 0):.4f}\")\n</code></pre>"},{"location":"connector/milvus_integration/#key-features","title":"Key Features","text":"<ul> <li>Local Storage: Milvus can run locally with file-based storage (<code>graphbit_vector.db</code>)</li> <li>Metadata Support: Store additional fields like text content and subject tags</li> <li>Flexible Queries: Search with custom limits and output field selection</li> <li>Automatic Dimension Detection: Collection dimension is automatically set based on embedding size</li> <li>Clean Slate Option: Drop and recreate collections for fresh starts</li> </ul>"},{"location":"connector/mongodb/","title":"MongoDB Integration with Graphbit","text":""},{"location":"connector/mongodb/#overview","title":"Overview","text":"<p>This guideline explains how to use MongoDB as both a general-purpose and a vector database within the Graphbit ecosystem, leveraging OpenAI embeddings. You will learn how to connect, store, and search data and vectors.</p>"},{"location":"connector/mongodb/#prerequisites","title":"Prerequisites","text":"<ul> <li>MongoDB running locally or in the cloud (e.g., MongoDB Atlas)</li> <li>Python environment with <code>pymongo</code> and <code>graphbit</code> installed:   <code>bash   pip install pymongo graphbit</code></li> <li>OpenAI API Key for embeddings</li> <li>Environment variable for your OpenAI API key:   <code>bash   export OPENAI_API_KEY=sk-...</code></li> </ul>"},{"location":"connector/mongodb/#step-1-connect-to-mongodb","title":"Step 1: Connect to MongoDB","text":"<p>You can connect to either a local MongoDB instance or a cloud-hosted MongoDB Atlas cluster by changing the <code>MONGO_URI</code>. Here\u2019s how to do both:</p> <pre><code>from pymongo import MongoClient\nimport os\n\n# For local MongoDB\nMONGO_URI = \"mongodb://localhost:27017\"\n\n# For MongoDB Atlas (replace &lt;username&gt;, &lt;password&gt;, and &lt;cluster-url&gt; with your details)\n# MONGO_URI = \"mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;cluster-url&gt;/\"\n\ntry:\n    client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)\n    client.server_info()  # Force connection\nexcept Exception as e:\n    print(f\"Failed to connect to MongoDB: {e}\")\n    exit(1)\n\ndb = client[\"graphbit_demo\"]\n</code></pre> <p>Tip: To use MongoDB Atlas, simply comment out the local URI and uncomment the Atlas URI, filling in your credentials.</p>"},{"location":"connector/mongodb/#step-2-general-purpose-crud-operations","title":"Step 2: General-purpose CRUD Operations","text":"<pre><code>general_collection = db[\"general_data\"]\n\n# CREATE: Insert a document\ndoc = {\"name\": \"Alice\", \"role\": \"engineer\", \"age\": 30}\ninsert_result = general_collection.insert_one(doc)\nprint(f\"Inserted document ID: {insert_result.inserted_id}\")\n\n# READ: Find a single document\nfound_doc = general_collection.find_one({\"name\": \"Alice\"})\nprint(f\"Found document: {found_doc}\")\n\n# READ: Find all documents (returns a cursor)\nall_docs = list(general_collection.find({}))\nprint(f\"All documents: {all_docs}\")\n\n# UPDATE: Update a document\ngeneral_collection.update_one({\"name\": \"Alice\"}, {\"$set\": {\"age\": 31}})\nupdated_doc = general_collection.find_one({\"name\": \"Alice\"})\nprint(f\"Updated document: {updated_doc}\")\n\n# DELETE: Delete a document\ngeneral_collection.delete_one({\"name\": \"Alice\"})\nprint(f\"Document deleted. Remaining: {list(general_collection.find({}))}\")\n\n</code></pre>"},{"location":"connector/mongodb/#step-3-store-and-search-vectors-with-openai-embeddings","title":"Step 3: Store and Search Vectors with OpenAI Embeddings","text":""},{"location":"connector/mongodb/#31-generate-and-store-an-embedding","title":"3.1. Generate and Store an Embedding","text":"<pre><code>import graphbit\n\ngraphbit.init()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n\nembedding_config = graphbit.EmbeddingConfig.openai(OPENAI_API_KEY, \"text-embedding-3-small\")\nembedding_client = graphbit.EmbeddingClient(embedding_config)\n\ntext = \"This is a sample document for vector search.\"\nembedding = embedding_client.embed(text)\n\nvector_collection = db[\"vector_data\"]\nvector_doc = {\"item_id\": \"item123\", \"embedding\": embedding, \"metadata\": {\"category\": \"test\"}}\nvector_collection.insert_one(vector_doc)\n</code></pre>"},{"location":"connector/mongodb/#32-vector-search-example","title":"3.2. Vector Search Example","text":"<pre><code>query_text = \"Find documents related to vector search.\"\nquery_embedding = embedding_client.embed(query_text)\n\nresults = vector_collection.find({})\nbest_score = -1\nbest_doc = None\nfor doc in results:\n    score = graphbit.EmbeddingClient.similarity(query_embedding, doc[\"embedding\"])\n    if score &gt; best_score:\n        best_score = score\n        best_doc = doc\nif best_doc is not None:\n    print(f\"Most similar document: {best_doc['item_id']} with score {best_score:.4f}\")\nelse:\n    print(\"No documents found in vector collection.\")\n</code></pre>"},{"location":"connector/mongodb/#step-4-batch-embedding-example","title":"Step 4: Batch Embedding Example","text":"<pre><code>batch_texts = [\n    \"Graph databases are great for relationships.\",\n    \"Vector search enables semantic retrieval.\",\n    \"OpenAI provides powerful embedding models.\",\n]\nbatch_embeddings = embedding_client.embed_many(batch_texts)\n\ndocs = [\n    {\"item_id\": f\"batch_{idx}\", \"embedding\": emb, \"metadata\": {\"text\": text}}\n    for idx, (text, emb) in enumerate(zip(batch_texts, batch_embeddings))\n]\nvector_collection.insert_many(docs)\nprint(f\"Inserted {len(batch_texts)} documents with OpenAI embeddings.\")\n</code></pre>"},{"location":"connector/mongodb/#full-example","title":"Full Example","text":"<pre><code>import os\nfrom pymongo import MongoClient\nimport graphbit\n\ngraphbit.init()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nMONGO_URI = \"mongodb://localhost:27017\"\nclient = MongoClient(MONGO_URI)\ndb = client[\"graphbit_demo\"]\n\n# General CRUD\ncol = db[\"general_data\"]\ncol.insert_one({\"name\": \"Alice\", \"role\": \"engineer\", \"age\": 30})\nprint(col.find_one({\"name\": \"Alice\"}))\ncol.update_one({\"name\": \"Alice\"}, {\"$set\": {\"age\": 31}})\nprint(col.find_one({\"name\": \"Alice\"}))\ncol.delete_one({\"name\": \"Alice\"})\n\n# Vector storage and search\nembedding_config = graphbit.EmbeddingConfig.openai(OPENAI_API_KEY, \"text-embedding-3-small\")\nembedding_client = graphbit.EmbeddingClient(embedding_config)\ntext = \"This is a sample document for vector search.\"\nembedding = embedding_client.embed(text)\nvec_col = db[\"vector_data\"]\nvec_col.insert_one({\"item_id\": \"item123\", \"embedding\": embedding})\n\nquery_embedding = embedding_client.embed(\"Find documents related to vector search.\")\nbest_doc = max(vec_col.find({}), key=lambda doc: graphbit.EmbeddingClient.similarity(query_embedding, doc[\"embedding\"]), default=None)\nif best_doc:\n    print(f\"Most similar document: {best_doc['item_id']}\")\n\n# Batch insert\nbatch_texts = [\n    \"Graph databases are great for relationships.\",\n    \"Vector search enables semantic retrieval.\",\n    \"OpenAI provides powerful embedding models.\",\n]\nbatch_embeddings = embedding_client.embed_many(batch_texts)\nvec_col.insert_many([\n    {\"item_id\": f\"batch_{i}\", \"embedding\": emb, \"metadata\": {\"text\": text}}\n    for i, (text, emb) in enumerate(zip(batch_texts, batch_embeddings))\n])\n</code></pre> <p>This connector pattern enables you to use MongoDB as both a general-purpose and vector database in your AI workflows, orchestrated by Graphbit. </p>"},{"location":"connector/pgvector/","title":"PGVector Integration with Graphbit","text":""},{"location":"connector/pgvector/#overview","title":"Overview","text":"<p>This guideline explains how to use PostgreSQL with the PGVector extension as a vector database within the Graphbit ecosystem, leveraging OpenAI embeddings. You will learn how to connect, store, and search data and vectors.</p>"},{"location":"connector/pgvector/#prerequisites","title":"Prerequisites","text":"<ul> <li>PostgreSQL with the PGVector extension installed and enabled</li> <li>Python environment with <code>psycopg2</code> and <code>graphbit</code> installed:   <code>bash   pip install psycopg2 graphbit</code></li> <li>OpenAI API Key for embeddings</li> <li>Environment variable for your OpenAI API key:   <code>bash   export OPENAI_API_KEY=sk-...</code></li> <li>A PostgreSQL database (e.g., <code>vector_db</code>) and user with appropriate permissions</li> </ul>"},{"location":"connector/pgvector/#step-1-connect-to-postgresql-and-ensure-table-exists","title":"Step 1: Connect to PostgreSQL and Ensure Table Exists","text":"<pre><code>import psycopg2\nimport os\n\n# Connect to PostgreSQL\nconn = psycopg2.connect(\n    dbname=\"vector_db\",\n    user=\"postgres\",\n    password=\"your_password\",\n    host=\"localhost\",\n    port=5432\n)\ncur = conn.cursor()\n\n# Ensure PGVector extension and table exist\ncur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\ncur.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS vector_data (\n    id SERIAL PRIMARY KEY,\n    item_id TEXT,\n    embedding VECTOR(1536),\n    metadata JSONB\n);\n\"\"\")\ncur.execute(\"\"\"\nCREATE INDEX IF NOT EXISTS idx_embedding_vector ON vector_data USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n\"\"\")\nconn.commit()\n</code></pre> <p>Note: The dimension in <code>VECTOR(1536)</code> must match your embedding model\u2019s output.</p> <p>Common Graphbit-supported Openai embedding models:</p> Model Name Dimension text-embedding-ada-002 1536 text-embedding-3-small 1536 text-embedding-3-large 3072 <p>If you use a different model, check its documentation for the correct dimension.</p>"},{"location":"connector/pgvector/#step-2-store-and-search-vectors-with-openai-embeddings","title":"Step 2: Store and Search Vectors with OpenAI Embeddings","text":""},{"location":"connector/pgvector/#21-generate-and-store-an-embedding","title":"2.1. Generate and Store an Embedding","text":"<pre><code>import graphbit\nimport json\n\ngraphbit.init()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nembedding_config = graphbit.EmbeddingConfig.openai(OPENAI_API_KEY, \"text-embedding-3-small\")\nembedding_client = graphbit.EmbeddingClient(embedding_config)\n\n# Generate embedding from text\ndoc_text = \"This is a sample document for vector search.\"\nembedding = embedding_client.embed(doc_text)\n\n# Insert embedding into PGVector table\ncur.execute(\n    \"\"\"\n    INSERT INTO vector_data (item_id, embedding, metadata)\n    VALUES (%s, %s, %s)\n    \"\"\",\n    (\"item123\", embedding, json.dumps({\"category\": \"test\"}))\n)\nconn.commit()\nprint(\"Inserted embedding for item123.\")\n</code></pre>"},{"location":"connector/pgvector/#22-vector-search-example-sqlpgvector","title":"2.2. Vector Search Example (SQL/PGVector)","text":"<pre><code>query_text = \"Find documents related to vector search.\"\nquery_embedding = embedding_client.embed(query_text)\ncur.execute(\n    \"\"\"\n    SELECT item_id, metadata, embedding &lt;#&gt; %s::vector AS distance\n    FROM vector_data\n    ORDER BY embedding &lt;#&gt; %s::vector ASC\n    LIMIT 1;\n    \"\"\",\n    (query_embedding, query_embedding)\n)\nresult = cur.fetchone()\nif result:\n    print(f\"Most similar item: {result[0]}, distance: {result[2]:.4f}\")\nelse:\n    print(\"No similar items found.\")\n</code></pre>"},{"location":"connector/pgvector/#23-vector-search-example-graphbit-manual-similarity","title":"2.3. Vector Search Example (Graphbit Manual Similarity)","text":"<pre><code>import ast\ncur.execute(\"SELECT item_id, embedding, metadata FROM vector_data;\")\nall_rows = cur.fetchall()\nbest_score = -1\nbest_item = None\nfor item_id, embedding_vec, metadata in all_rows:\n    # Convert the embedding from string to list if needed\n    if isinstance(embedding_vec, str):\n        embedding_vec = ast.literal_eval(embedding_vec)\n    score = embedding_client.similarity(query_embedding, embedding_vec)\n    if score &gt; best_score:\n        best_score = score\n        best_item = (item_id, metadata)\nif best_item is not None:\n    print(f\"Most similar document: {best_item[0]} with score {best_score:.4f}\")\nelse:\n    print(\"No documents found in vector table.\")\n</code></pre>"},{"location":"connector/pgvector/#step-3-batch-embedding-example","title":"Step 3: Batch Embedding Example","text":"<pre><code>batch_texts = [\n    \"Graph databases are great for relationships.\",\n    \"Vector search enables semantic retrieval.\",\n    \"OpenAI provides powerful embedding models.\",\n]\nbatch_embeddings = embedding_client.embed_many(batch_texts)\nfor idx, (text, emb) in enumerate(zip(batch_texts, batch_embeddings)):\n    cur.execute(\n        \"\"\"\n        INSERT INTO vector_data (item_id, embedding, metadata)\n        VALUES (%s, %s, %s)\n        \"\"\",\n        (f\"batch_{idx}\", emb, json.dumps({\"text\": text}))\n    )\nconn.commit()\nprint(f\"Inserted {len(batch_texts)} documents with embeddings.\")\n</code></pre>"},{"location":"connector/pgvector/#full-example","title":"Full Example","text":"<pre><code>import os\nimport psycopg2\nimport graphbit\nimport json\nimport ast\n\ngraphbit.init()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nembedding_config = graphbit.EmbeddingConfig.openai(OPENAI_API_KEY, \"text-embedding-3-small\")\nembedding_client = graphbit.EmbeddingClient(embedding_config)\n\nconn = psycopg2.connect(\n    dbname=\"vector_db\",\n    user=\"postgres\",\n    password=\"your_password\",\n    host=\"localhost\",\n    port=5432\n)\ncur = conn.cursor()\n\n# Ensure PGVector extension and table exist\ncur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\ncur.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS vector_data (\n    id SERIAL PRIMARY KEY,\n    item_id TEXT,\n    embedding VECTOR(1536),\n    metadata JSONB\n);\n\"\"\")\ncur.execute(\"\"\"\nCREATE INDEX IF NOT EXISTS idx_embedding_vector ON vector_data USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n\"\"\")\nconn.commit()\n\n# Insert a single embedding\ndoc_text = \"This is a sample document for vector search.\"\nembedding = embedding_client.embed(doc_text)\ncur.execute(\n    \"\"\"\n    INSERT INTO vector_data (item_id, embedding, metadata)\n    VALUES (%s, %s, %s)\n    \"\"\",\n    (\"item123\", embedding, json.dumps({\"category\": \"test\"}))\n)\nconn.commit()\n\n# Vector search (SQL/PGVector)\nquery_text = \"Find documents related to vector search.\"\nquery_embedding = embedding_client.embed(query_text)\ncur.execute(\n    \"\"\"\n    SELECT item_id, metadata, embedding &lt;#&gt; %s::vector AS distance\n    FROM vector_data\n    ORDER BY embedding &lt;#&gt; %s::vector ASC\n    LIMIT 1;\n    \"\"\",\n    (query_embedding, query_embedding)\n)\nresult = cur.fetchone()\nif result:\n    print(f\"Most similar item: {result[0]}, distance: {result[2]:.4f}\")\nelse:\n    print(\"No similar items found.\")\n\n# Vector search (Graphbit manual similarity)\ncur.execute(\"SELECT item_id, embedding, metadata FROM vector_data;\")\nall_rows = cur.fetchall()\nbest_score = -1\nbest_item = None\nfor item_id, embedding_vec, metadata in all_rows:\n    if isinstance(embedding_vec, str):\n        embedding_vec = ast.literal_eval(embedding_vec)\n    score = embedding_client.similarity(query_embedding, embedding_vec)\n    if score &gt; best_score:\n        best_score = score\n        best_item = (item_id, metadata)\nif best_item is not None:\n    print(f\"Most similar document: {best_item[0]} with score {best_score:.4f}\")\nelse:\n    print(\"No documents found in vector table.\")\n\n# Batch insert\nbatch_texts = [\n    \"Graph databases are great for relationships.\",\n    \"Vector search enables semantic retrieval.\",\n    \"OpenAI provides powerful embedding models.\",\n]\nbatch_embeddings = embedding_client.embed_many(batch_texts)\nfor idx, (text, emb) in enumerate(zip(batch_texts, batch_embeddings)):\n    cur.execute(\n        \"\"\"\n        INSERT INTO vector_data (item_id, embedding, metadata)\n        VALUES (%s, %s, %s)\n        \"\"\",\n        (f\"batch_{idx}\", emb, json.dumps({\"text\": text}))\n    )\nconn.commit()\nprint(f\"Inserted {len(batch_texts)} documents with embeddings.\")\n\n# Cleanup\ncur.close()\nconn.close()\nprint(\"Done.\")\n</code></pre> <p>This connector pattern enables you to use PostgreSQL with PGVector as a vector database in your AI workflows, orchestrated by Graphbit. </p>"},{"location":"connector/pinecone_integration/","title":"Pinecone Integration with Graphbit","text":""},{"location":"connector/pinecone_integration/#overview","title":"Overview","text":"<p>This guide explains how to connect Pinecone to Graphbit, enabling you to perform vector similarity search over high-dimensional embeddings generated by LLMs. With this integration, you can store, index, and search embeddings for tasks like semantic search, retrieval-augmented generation, and more.</p>"},{"location":"connector/pinecone_integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Pinecone API Key: Obtain from Pinecone Console.</li> <li>OpenAI API Key: For LLM and embedding generation (or another supported embedding provider).</li> <li>Graphbit installed and configured (see installation guide).</li> <li>Python environment with <code>pinecone</code>, <code>graphbit</code>, and optionally <code>python-dotenv</code> installed.</li> <li>.env file in your project root with the following variables:   <code>env   OPENAI_API_KEY=your_openai_api_key_here   PINECONE_API_KEY=your_pinecone_api_key_here</code></li> </ul>"},{"location":"connector/pinecone_integration/#step-1-initialize-pinecone-and-graphbit","title":"Step 1: Initialize Pinecone and Graphbit","text":"<p>Set up Pinecone and Graphbit, and ensure the index exists and is ready:</p> <pre><code>import os\nimport time\nimport uuid\nfrom pinecone import Pinecone, ServerlessSpec\nimport graphbit\nfrom dotenv import load_dotenv\n\nload_dotenv()\nINDEX_NAME = \"graphbit-vector\"\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n\npinecone_client = Pinecone(api_key=PINECONE_API_KEY)\nindex_list = pinecone_client.list_indexes()\n\npinecone_client.create_index(\n    name=INDEX_NAME,\n    vector_type=\"dense\",\n    dimension=1536,\n    metric=\"cosine\",\n    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n)\n\nindex = pinecone_client.Index(INDEX_NAME)\n\ngraphbit.init()\n</code></pre>"},{"location":"connector/pinecone_integration/#step-2-generate-and-upsert-embeddings","title":"Step 2: Generate and Upsert Embeddings","text":"<p>Use Graphbit's embedding client to generate embeddings and upsert them into Pinecone:</p> <pre><code>OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nEMBEDDING_MODEL = \"text-embedding-3-small\"\n\nembedding_config = graphbit.EmbeddingConfig.openai(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)\nembedding_client = graphbit.EmbeddingClient(embedding_config)\n\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"Pinecone enables vector search over high-dimensional embeddings.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeddings = embedding_client.embed_many(texts)\n\n# Prepare vectors for upsert: (id, values, metadata)\nvectors = [\n    (str(uuid.uuid4()), emb, {\"text\": txt})\n    for emb, txt in zip(embeddings, texts)\n]\nupsert_response = index.upsert(vectors=vectors)\nprint(\"Upsert response:\", upsert_response)\n</code></pre>"},{"location":"connector/pinecone_integration/#step-3-perform-similarity-search","title":"Step 3: Perform Similarity Search","text":"<p>Embed your query and search for similar vectors in Pinecone:</p> <pre><code>query = \"What is GraphBit?\"\nembed_query = embedding_client.embed(query)\nprint(\"Query embedding shape:\", len(embed_query))\n\n# Standard Pinecone query\nresp = index.query(vector=embed_query, top_k=2, include_metadata=True)\nprint(\"Query response:\", resp)\n\n# Pretty-print results\nfor match in resp[\"matches\"]:\n    print(f\"Score: {match['score']:.4f}\")\n    print(f\"Text: {match['metadata']['text']}\")\n    print(\"---\")\n</code></pre>"},{"location":"connector/pinecone_integration/#full-example","title":"Full Example","text":"<pre><code>import os\nimport time\nimport uuid\n\nfrom pinecone import Pinecone, ServerlessSpec\n\nimport graphbit\n\nINDEX_NAME = \"graphbit-vector\"\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n\npinecone_client = Pinecone(api_key=PINECONE_API_KEY)\nindex_name = INDEX_NAME\n\nindex_list = pinecone_client.list_indexes()\n# Check if the index exists\nif index_name not in [idx[\"name\"] for idx in index_list]:\n    print(f\"Index {index_name} does not exist. Creating it...\")\n    pinecone_client.create_index(\n        name=index_name,\n        vector_type=\"dense\",\n        dimension=1536,\n        metric=\"cosine\",\n        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n    )\n    # Wait for index to be ready\n    while True:\n        status = pinecone_client.describe_index(index_name)\n        if status[\"status\"][\"ready\"]:\n            break\n        print(\"Waiting for index to be ready...\")\n        time.sleep(2)\n    index = pinecone_client.Index(index_name)\nelse:\n    index = pinecone_client.Index(index_name)\n\ngraphbit.init()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nEMBEDDING_MODEL = \"text-embedding-3-small\"\nembedding_config = graphbit.EmbeddingConfig.openai(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)\nembedding_client = graphbit.EmbeddingClient(embedding_config)\n\ntext = [\"GraphBit is a framework for LLM workflows and agent orchestration.\", \"Pinecone enables vector search over high-dimensional embeddings.\", \"OpenAI offers tools for LLMs and embeddings.\"]\nembeddings = embedding_client.embed_many(text)\nvectors = [(str(uuid.uuid4()), emb, {\"text\": txt}) for emb, txt in zip(embeddings, text)]\n\nupsert_response = index.upsert(vectors=vectors)\nprint(\"Upsert response:\", upsert_response)\n\n# Waiting until the vector is available in the index\nNAMESPACE = \"__default__\"\nattempt = 0\ntarget_id = vectors[0][0]\nwhile True:\n    response = index.fetch(ids=[target_id], namespace=NAMESPACE)\n    if response.vectors and target_id in response.vectors:\n        print(f\"Confirmed upsert: vector {target_id} is available.\")\n        break\n    print(f\"Waiting for upsert completion... attempt {attempt}\")\n    attempt += 1\n    time.sleep(2)\n\nquery = \"What is GraphBit?\"\nembed_query = embedding_client.embed(query)\nprint(\"Query embedding shape:\", len(embed_query))\n\nresults = index.query(vector=embed_query, top_k=2, include_metadata=True)\nprint(results)\n\n# Pretty-print results\nfor match in results[\"matches\"]:\n    print(f\"Score: {match['score']:.4f}\")\n    print(f\"Text: {match['metadata']['text']}\")\n    print(\"---\")\n</code></pre> <p>This integration enables you to leverage Graphbit's embedding capabilities with Pinecone's vector database for scalable, production-grade semantic search and retrieval workflows. </p>"},{"location":"connector/qdrant_integration/","title":"Qdrant Integration with Graphbit","text":""},{"location":"connector/qdrant_integration/#overview","title":"Overview","text":"<p>This guide explains how to connect Qdrant, an open-source vector database, to Graphbit. With this integration, you can store, index, and search high-dimensional embeddings generated by LLMs for semantic search, retrieval-augmented generation, and more.</p>"},{"location":"connector/qdrant_integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Qdrant running locally or remotely (see Qdrant documentation).</li> <li>OpenAI API Key: For embedding generation (or another supported embedding provider).</li> <li>Graphbit installed and configured (see installation guide).</li> <li>Python environment with <code>qdrant-client</code>, <code>graphbit</code>, and optionally <code>python-dotenv</code> installed.</li> <li>.env file in your project root with the following variables:   <code>env   OPENAI_API_KEY=your_openai_api_key_here</code></li> </ul>"},{"location":"connector/qdrant_integration/#step-1-connect-to-qdrant-and-create-collection","title":"Step 1: Connect to Qdrant and Create Collection","text":"<p>Set up the Qdrant client and ensure the collection is created:</p> <pre><code>import os\nimport uuid\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import VectorParams, Distance, PointStruct\n\nCOLLECTION = \"graphbit-vector\"\nDIMENSION = 1536\n\nclient = QdrantClient(host=\"localhost\", port=6333)\n\nif not client.collection_exists(COLLECTION):\n    client.create_collection(\n        collection_name=COLLECTION,\n        vectors_config=VectorParams(size=DIMENSION, distance=Distance.COSINE),\n    )\n</code></pre>"},{"location":"connector/qdrant_integration/#step-2-generate-and-upsert-embeddings-using-graphbit","title":"Step 2: Generate and Upsert Embeddings using Graphbit","text":"<p>Use Graphbit's embedding client to generate embeddings and upsert them into Qdrant:</p> <pre><code>import graphbit\n\ngraphbit.init()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nEMBEDDING_MODEL = \"text-embedding-3-small\"\nembedding_client = graphbit.EmbeddingClient(\n    graphbit.EmbeddingConfig.openai(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)\n)\n\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"Qdrant is an open-source vector database for similarity search.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeds = embedding_client.embed_many(texts)\n\npoints = [\n    PointStruct(id=str(uuid.uuid4()), vector=vec, payload={\"text\": txt})\n    for vec, txt in zip(embeds, texts)\n]\nclient.upsert(collection_name=COLLECTION, points=points, wait=True)\n</code></pre>"},{"location":"connector/qdrant_integration/#step-3-perform-similarity-search","title":"Step 3: Perform Similarity Search","text":"<p>Embed your query and search for similar vectors in Qdrant:</p> <pre><code>query = \"What is GraphBit?\"\nquery_vec = embedding_client.embed(query)\nresponse = client.query_points(\n    collection_name=COLLECTION,\n    query=query_vec,\n    limit=2,\n    with_payload=True\n)\n</code></pre>"},{"location":"connector/qdrant_integration/#full-example","title":"Full Example","text":"<pre><code>import os\nimport uuid\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import VectorParams, Distance, PointStruct\nimport graphbit\n\nCOLLECTION = \"graphbit-vector\"\nDIMENSION = 1536\n\nclient = QdrantClient(host=\"localhost\", port=6333)\nif not client.collection_exists(COLLECTION):\n    client.create_collection(\n        collection_name=COLLECTION,\n        vectors_config=VectorParams(size=DIMENSION, distance=Distance.COSINE),\n    )\n\ngraphbit.init()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nEMBEDDING_MODEL = \"text-embedding-3-small\"\nembedding_client = graphbit.EmbeddingClient(\n    graphbit.EmbeddingConfig.openai(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)\n)\n\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"Qdrant is an open-source vector database for similarity search.\",\n    \"OpenAI offers tools for LLMs and embeddings.\"\n]\nembeds = embedding_client.embed_many(texts)\n\npoints = [\n    PointStruct(id=str(uuid.uuid4()), vector=vec, payload={\"text\": txt})\n    for vec, txt in zip(embeds, texts)\n]\nclient.upsert(collection_name=COLLECTION, points=points, wait=True)\n\n# Retrieve one point to double-check (optional)\ncheck_id = points[0].id\nretrieved = client.retrieve(\n    collection_name=COLLECTION,\n    ids=[check_id],\n    with_payload=True\n)\nassert retrieved, \"Upsert failed!\"\n\nquery = \"What is GraphBit?\"\nquery_vec = embedding_client.embed(query)\nresponse = client.query_points(\n    collection_name=COLLECTION,\n    query=query_vec,\n    limit=2,\n    with_payload=True\n)\n\n# Process response into a more readable format\nresponse_points = response.points\nresult = []\nfor point in response_points:\n    result.append({\n        \"id\": point.id,\n        \"text\": point.payload.get(\"text\", \"\"),\n        \"score\": point.score\n    })\nprint(result)\n</code></pre> <p>This integration enables you to leverage Graphbit's embedding capabilities with Qdrant's vector database for scalable, open-source semantic search and retrieval workflows. </p>"},{"location":"connector/weaviate_integration/","title":"Weaviate Integration with Graphbit","text":""},{"location":"connector/weaviate_integration/#overview","title":"Overview","text":"<p>This guide explains how to connect Weaviate, a vector database for AI applications, to Graphbit. With this integration, you can store, index, and search high-dimensional embeddings generated by LLMs for semantic search, retrieval-augmented generation, and more.</p>"},{"location":"connector/weaviate_integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Weaviate Instance: A Weaviate Cloud Service (WCS) account or a self-hosted Weaviate instance.     Obtain the <code>Weaviate URL</code> (e.g., <code>https://&lt;cluster-id&gt;.weaviate.network</code>) and <code>API key</code> (for WCS).</li> <li>OpenAI API Key: For embedding generation (or another supported embedding provider).</li> <li>Graphbit installed and configured (see installation guide).</li> <li>Python environment with <code>weaviate-client</code>, <code>graphbit</code></li> </ul>"},{"location":"connector/weaviate_integration/#step-1-connect-to-weaviate","title":"Step 1: Connect to Weaviate","text":"<p>Set up the connection to your Weaviate instance:</p> <pre><code>import os\nfrom weaviate.classes.init import Auth\nfrom weaviate import connect_to_weaviate_cloud\n\nweaviate_url = os.environ[\"WEAVIATE_URL\"]\nweaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n\n# Connect to Weaviate Cloud Service\nvectordb_config = connect_to_weaviate_cloud(\n    cluster_url=weaviate_url,\n    auth_credentials=Auth.api_key(weaviate_api_key),\n)\n\n# Check if Weaviate is ready\nprint(\"Weaviate ready:\", vectordb_config.is_ready())\n</code></pre>"},{"location":"connector/weaviate_integration/#step-2-create-collection","title":"Step 2: Create Collection","text":"<p>Create a collection to store your vectors:</p> <pre><code>import uuid\n\nCOLLECTION_NAME = \"Graphbit_VectorDB\"\n\n# Get or create collection\nvectordb_client = vectordb_config.collections.get(COLLECTION_NAME)\nif vectordb_client is None:\n    vectordb_client = vectordb_config.collections.create(\n        name=COLLECTION_NAME,\n        vector_config={\n            \"vectorizer\": \"none\",\n            \"vectorIndexConfig\": {\n                \"distance\": \"cosine\",\n            }\n        },\n    )\n</code></pre>"},{"location":"connector/weaviate_integration/#step-3-initialize-embedding-client","title":"Step 3: Initialize Embedding Client","text":"<p>Set up Graphbit and initialize the embedding client:</p> <pre><code>from graphbit import EmbeddingClient, EmbeddingConfig\n\n# Initialize embedding client\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n\nembedding_config = EmbeddingConfig.openai(model=\"text-embedding-3-small\", api_key=openai_api_key)\nembedding_client = EmbeddingClient(embedding_config)\n</code></pre>"},{"location":"connector/weaviate_integration/#step-4-generate-embeddings","title":"Step 4: Generate Embeddings","text":"<p>Generate embeddings for your text data:</p> <pre><code># Sample texts to embed\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"Weaviate is a vector database for AI applications.\",\n    \"OpenAI provides APIs for embeddings and LLMs.\"\n]\n\n# Generate embeddings\nembeddings = embedding_client.embed_many(texts)\n</code></pre>"},{"location":"connector/weaviate_integration/#step-5-insert-embeddings-into-weaviate","title":"Step 5: Insert Embeddings into Weaviate","text":"<p>Insert the generated embeddings into your Weaviate collection:</p> <pre><code>for i, (text, vector) in enumerate(zip(texts, embeddings)):\n    vectordb_client.data.insert(\n        properties={\"text\": text},\n        vector=vector,\n        uuid=uuid.uuid4(),\n    )\n</code></pre>"},{"location":"connector/weaviate_integration/#step-6-perform-similarity-search","title":"Step 6: Perform Similarity Search","text":"<p>Embed your query and search for similar vectors in Weaviate:</p> <pre><code>query = \"What is GraphBit?\"\nquery_embedding = embedding_client.embed(query)\n\n# Perform similarity search\nresults = vectordb_client.query.near_vector(\n    near_vector=query_embedding,\n    limit=3,\n    return_metadata=[\"distance\"],\n    return_properties=[\"text\"]\n)\n</code></pre>"},{"location":"connector/weaviate_integration/#full-example","title":"Full Example","text":"<pre><code>import uuid\nfrom weaviate.classes.init import Auth\nfrom weaviate import connect_to_weaviate_cloud\nfrom graphbit import EmbeddingConfig, EmbeddingClient\n\nCOLLECTION_NAME = \"Graphbit_VectorDB\"\n\n# Step 1: Connect to Weaviate Cloud Service\nweaviate_url = os.environ[\"WEAVIATE_URL\"]\nweaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n\nvectordb_config = connect_to_weaviate_cloud(\n    cluster_url=weaviate_url,\n    auth_credentials=Auth.api_key(weaviate_api_key),\n)\n\nprint(\"Weaviate ready:\", vectordb_config.is_ready())\n\n# Step 2: Get or create collection\nvectordb_client = vectordb_config.collections.get(COLLECTION_NAME)\nif vectordb_client is None:\n    vectordb_client = vectordb_config.collections.create(\n        name=COLLECTION_NAME,\n        vector_config={\n            \"vectorizer\": \"none\",\n            \"vectorIndexConfig\": {\n                \"distance\": \"cosine\",\n            }\n        },\n    )\n\n# Step 3: Initialize Graphbit and embedding client\nembedding_config = EmbeddingConfig.openai(model=\"text-embedding-3-small\", api_key=os.getenv(\"OPENAI_API_KEY\", \"\"))\nembedding_client = EmbeddingClient(embedding_config)\n\n# Step 4: Generate embeddings\ntexts = [\n    \"GraphBit is a framework for LLM workflows and agent orchestration.\",\n    \"Weaviate is a vector database for AI applications.\",\n    \"OpenAI provides APIs for embeddings and LLMs.\"\n]\n\nembeddings = embedding_client.embed_many(texts)\n\n# Step 5: Insert embeddings into Weaviate\nfor i, (text, vector) in enumerate(zip(texts, embeddings)):\n    vectordb_client.data.insert(\n        properties={\"text\": text},\n        vector=vector,\n        uuid=uuid.uuid4(),\n    )\n\n# Step 6: Perform similarity search\nquery = \"What is GraphBit?\"\nquery_embedding = embedding_client.embed(query)\n\nresults = vectordb_client.query.near_vector(\n    near_vector=query_embedding,\n    limit=3,\n    return_metadata=[\"distance\"],\n    return_properties=[\"text\"]\n)\n\n# Display results\nprint(\"Search Results:\")\nfor obj in results.objects:\n    print(\"Text:\", obj.properties[\"text\"])\n    print(\"Distance:\", obj.metadata.distance)\n    print(\"Score:\", 1 - obj.metadata.distance)\n    print(\"---\")\n\n# Clean up (optional)\nvectordb_config.close()\n</code></pre> <p>This integration enables you to leverage Graphbit's embedding capabilities with Weaviate's vector database for scalable, production-grade semantic search and retrieval workflows with advanced filtering and metadata capabilities. </p>"},{"location":"development/architecture/","title":"Architecture Overview","text":"<p>GraphBit is a high-performance AI agent workflow automation framework built with a three-tier architecture that combines the performance of Rust with the accessibility of Python.</p>"},{"location":"development/architecture/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Python API Layer                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    PyO3 Bindings                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Rust Core Engine                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     LLM Providers     \u2502     Storage      \u2502    Telemetry   \u2502\n\u2502  OpenAI | Anthropic   \u2502  Memory | Disk   \u2502  Tracing | Logs \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"development/architecture/#core-components","title":"Core Components","text":""},{"location":"development/architecture/#1-python-api-layer","title":"1. Python API Layer","text":"<p>The Python API provides a user-friendly interface for creating and managing workflows:</p> <pre><code># High-level Python interface\nimport graphbit\n\ngraphbit.init()\nbuilder = graphbit.PyWorkflowBuilder(\"My Workflow\")\n# ... build workflow\nexecutor = graphbit.PyWorkflowExecutor(config)\nresult = executor.execute(workflow)\n</code></pre> <p>Key Classes: - <code>PyWorkflowBuilder</code> - Fluent workflow construction - <code>PyWorkflowExecutor</code> - Workflow execution engine - <code>PyWorkflowNode</code> - Node creation and management - <code>PyLlmConfig</code> - LLM provider configuration</p>"},{"location":"development/architecture/#2-pyo3-bindings-layer","title":"2. PyO3 Bindings Layer","text":"<p>PyO3 provides seamless interoperability between Python and Rust with production-grade features:</p> <pre><code>// Python class exposed via PyO3\n#[pyclass]\npub struct LlmClient {\n    provider: Arc&lt;RwLock&lt;Box&lt;dyn LlmProviderTrait&gt;&gt;&gt;,\n    circuit_breaker: Arc&lt;CircuitBreaker&gt;,\n    config: ClientConfig,\n    stats: Arc&lt;RwLock&lt;ClientStats&gt;&gt;,\n}\n\n#[pymethods]\nimpl LlmClient {\n    #[new]\n    fn new(config: LlmConfig, debug: Option&lt;bool&gt;) -&gt; PyResult&lt;Self&gt; {\n        // Production-grade initialization with error handling\n    }\n\n    fn complete(&amp;self, prompt: String, max_tokens: Option&lt;u32&gt;) -&gt; PyResult&lt;String&gt; {\n        // High-performance completion with resilience patterns\n    }\n}\n</code></pre> <p>Key Features: - Type Safety: Comprehensive input validation and type checking - Memory Management: Proper resource cleanup across language boundaries - Error Handling: Structured error types mapped to Python exceptions - Async Support: Full async/await compatibility with Tokio runtime - Performance: Zero-copy operations and optimized data structures - Observability: Built-in metrics, tracing, and health monitoring</p> <p>Module Structure: - <code>lib.rs</code>: Global initialization and system management - <code>runtime.rs</code>: Optimized Tokio runtime management - <code>errors.rs</code>: Comprehensive error handling and conversion - <code>llm/</code>: LLM provider integration with resilience patterns - <code>embeddings/</code>: Embedding provider support - <code>workflow/</code>: Workflow execution engine - <code>validation.rs</code>: Input validation utilities</p>"},{"location":"development/architecture/#3-rust-core-engine","title":"3. Rust Core Engine","text":"<p>The Rust core provides high-performance execution and reliability:</p> <pre><code>// Core workflow execution\npub struct WorkflowExecutor {\n    llm_client: Arc&lt;dyn LlmClient&gt;,\n    circuit_breaker: CircuitBreaker,\n    retry_policy: RetryPolicy,\n}\n\nimpl WorkflowExecutor {\n    pub async fn execute(&amp;self, workflow: &amp;Workflow) -&gt; Result&lt;ExecutionResult&gt; {\n        // High-performance execution logic\n    }\n}\n</code></pre> <p>Key Modules: - <code>workflow</code> - Workflow definition and validation - <code>executor</code> - Execution engine and orchestration - <code>agents</code> - Agent management and processing - <code>llm</code> - LLM provider integrations - <code>reliability</code> - Circuit breakers, retries, rate limiting</p>"},{"location":"development/architecture/#data-flow-architecture","title":"Data Flow Architecture","text":""},{"location":"development/architecture/#workflow-execution-flow","title":"Workflow Execution Flow","text":"<pre><code>Input Data\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Workflow       \u2502\n\u2502  Validation     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Execution      \u2502\n\u2502  Planning       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Node Execution \u2502\u2500\u2500\u2500\u2500\u2502  LLM Provider   \u2502\u2500\u2500\u2500\u2500\u2502  Response       \u2502\n\u2502  (Parallel/Seq) \u2502    \u2502  Integration    \u2502    \u2502  Processing     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Result         \u2502\n\u2502  Aggregation    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\nOutput Data\n</code></pre>"},{"location":"development/architecture/#node-processing-pipeline","title":"Node Processing Pipeline","text":"<pre><code>Node Input\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Input          \u2502\n\u2502  Validation     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Prompt         \u2502\n\u2502  Template       \u2502\n\u2502  Processing     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LLM Request    \u2502\n\u2502  (with Circuit  \u2502\n\u2502   Breaker)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Response       \u2502\n\u2502  Processing &amp;   \u2502\n\u2502  Validation     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\nNode Output\n</code></pre>"},{"location":"development/architecture/#component-details","title":"Component Details","text":""},{"location":"development/architecture/#workflow-management","title":"Workflow Management","text":""},{"location":"development/architecture/#workflow-builder-pattern","title":"Workflow Builder Pattern","text":"<pre><code>pub struct WorkflowBuilder {\n    name: String,\n    description: Option&lt;String&gt;,\n    nodes: HashMap&lt;NodeId, Node&gt;,\n    edges: Vec&lt;Edge&gt;,\n}\n\nimpl WorkflowBuilder {\n    pub fn add_node(&amp;mut self, node: Node) -&gt; NodeId {\n        // Add node and return unique ID\n    }\n\n    pub fn connect(&amp;mut self, from: NodeId, to: NodeId, edge: Edge) {\n        // Connect nodes with typed edges\n    }\n\n    pub fn build(self) -&gt; Result&lt;Workflow&gt; {\n        // Validate and build immutable workflow\n    }\n}\n</code></pre>"},{"location":"development/architecture/#workflow-validation","title":"Workflow Validation","text":"<p>GraphBit performs comprehensive validation:</p> <ul> <li>Structural Validation: No cycles, connected graph</li> <li>Type Validation: Compatible data types between nodes</li> <li>Dependency Validation: All required inputs available</li> <li>Resource Validation: API keys, model availability</li> </ul>"},{"location":"development/architecture/#execution-engine","title":"Execution Engine","text":""},{"location":"development/architecture/#execution-strategies","title":"Execution Strategies","text":"<p>Sequential Execution:</p> <pre><code>async fn execute_sequential(&amp;self, workflow: &amp;Workflow) -&gt; Result&lt;ExecutionResult&gt; {\n    let execution_order = self.topological_sort(workflow)?;\n\n    for node_id in execution_order {\n        let result = self.execute_node(node_id).await?;\n        self.context.set_result(node_id, result);\n    }\n\n    Ok(self.context.into_result())\n}\n</code></pre> <p>Parallel Execution:</p> <pre><code>async fn execute_parallel(&amp;self, workflow: &amp;Workflow) -&gt; Result&lt;ExecutionResult&gt; {\n    let execution_batches = self.create_execution_batches(workflow)?;\n\n    for batch in execution_batches {\n        let batch_results = join_all(\n            batch.into_iter().map(|node_id| self.execute_node(node_id))\n        ).await;\n\n        self.context.merge_results(batch_results)?;\n    }\n\n    Ok(self.context.into_result())\n}\n</code></pre>"},{"location":"development/architecture/#context-management","title":"Context Management","text":"<pre><code>pub struct ExecutionContext {\n    variables: HashMap&lt;String, Value&gt;,\n    node_results: HashMap&lt;NodeId, NodeResult&gt;,\n    metadata: ExecutionMetadata,\n}\n\nimpl ExecutionContext {\n    pub fn get_variable(&amp;self, key: &amp;str) -&gt; Option&lt;&amp;Value&gt; {\n        self.variables.get(key)\n    }\n\n    pub fn set_variable(&amp;mut self, key: String, value: Value) {\n        self.variables.insert(key, value);\n    }\n\n    pub fn substitute_template(&amp;self, template: &amp;str) -&gt; String {\n        // Template variable substitution\n    }\n}\n</code></pre>"},{"location":"development/architecture/#llm-integration-layer","title":"LLM Integration Layer","text":""},{"location":"development/architecture/#provider-abstraction","title":"Provider Abstraction","text":"<pre><code>#[async_trait]\npub trait LlmClient: Send + Sync {\n    async fn generate(&amp;self, request: GenerateRequest) -&gt; Result&lt;GenerateResponse&gt;;\n    async fn embed(&amp;self, request: EmbedRequest) -&gt; Result&lt;EmbedResponse&gt;;\n\n    fn provider_name(&amp;self) -&gt; &amp;str;\n    fn model_info(&amp;self) -&gt; ModelInfo;\n}\n</code></pre>"},{"location":"development/architecture/#provider-implementations","title":"Provider Implementations","text":"<p>OpenAI Client:</p> <pre><code>pub struct OpenAiClient {\n    client: Client,\n    api_key: String,\n    model: String,\n}\n\n#[async_trait]\nimpl LlmClient for OpenAiClient {\n    async fn generate(&amp;self, request: GenerateRequest) -&gt; Result&lt;GenerateResponse&gt; {\n        let openai_request = self.convert_request(request);\n        let response = self.client.chat().create(openai_request).await?;\n        Ok(self.convert_response(response))\n    }\n}\n</code></pre> <p>Anthropic Client:</p> <pre><code>pub struct AnthropicClient {\n    client: Client, \n    api_key: String,\n    model: String,\n}\n\n#[async_trait]\nimpl LlmClient for AnthropicClient {\n    async fn generate(&amp;self, request: GenerateRequest) -&gt; Result&lt;GenerateResponse&gt; {\n        let claude_request = self.convert_request(request);\n        let response = self.client.messages().create(claude_request).await?;\n        Ok(self.convert_response(response))\n    }\n}\n</code></pre>"},{"location":"development/architecture/#reliability-layer","title":"Reliability Layer","text":""},{"location":"development/architecture/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<pre><code>pub struct CircuitBreaker {\n    state: Arc&lt;Mutex&lt;CircuitState&gt;&gt;,\n    failure_threshold: u32,\n    timeout_duration: Duration,\n    half_open_max_calls: u32,\n}\n\nimpl CircuitBreaker {\n    pub async fn call&lt;F, T&gt;(&amp;self, operation: F) -&gt; Result&lt;T&gt;\n    where\n        F: Future&lt;Output = Result&lt;T&gt;&gt;,\n    {\n        match self.state() {\n            CircuitState::Closed =&gt; self.execute_operation(operation).await,\n            CircuitState::Open =&gt; Err(Error::CircuitBreakerOpen),\n            CircuitState::HalfOpen =&gt; self.try_half_open_operation(operation).await,\n        }\n    }\n}\n</code></pre>"},{"location":"development/architecture/#retry-policy","title":"Retry Policy","text":"<pre><code>pub struct RetryPolicy {\n    max_attempts: u32,\n    base_delay: Duration,\n    max_delay: Duration,\n    backoff_multiplier: f64,\n    jitter: f64,\n}\n\nimpl RetryPolicy {\n    pub async fn execute&lt;F, T&gt;(&amp;self, mut operation: F) -&gt; Result&lt;T&gt;\n    where\n        F: FnMut() -&gt; Pin&lt;Box&lt;dyn Future&lt;Output = Result&lt;T&gt;&gt;&gt;&gt;,\n    {\n        let mut attempt = 0;\n\n        loop {\n            match operation().await {\n                Ok(result) =&gt; return Ok(result),\n                Err(e) if attempt &gt;= self.max_attempts =&gt; return Err(e),\n                Err(_) =&gt; {\n                    attempt += 1;\n                    let delay = self.calculate_delay(attempt);\n                    tokio::time::sleep(delay).await;\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"development/architecture/#memory-management","title":"Memory Management","text":""},{"location":"development/architecture/#python-rust-boundary","title":"Python-Rust Boundary","text":"<pre><code>// Efficient data transfer\n#[pyclass]\npub struct PyExecutionResult {\n    #[pyo3(get)]\n    pub status: String,\n\n    // Internal Rust data (not exposed to Python)\n    result: Arc&lt;ExecutionResult&gt;,\n}\n\nimpl PyExecutionResult {\n    // Zero-copy access to internal data when possible\n    pub fn get_variable(&amp;self, key: &amp;str) -&gt; Option&lt;String&gt; {\n        self.result.variables.get(key).map(|v| v.to_string())\n    }\n}\n</code></pre>"},{"location":"development/architecture/#resource-management","title":"Resource Management","text":"<p>Connection Pooling:</p> <pre><code>pub struct ConnectionPool {\n    pools: HashMap&lt;String, Pool&lt;Connection&gt;&gt;,\n    max_connections: usize,\n}\n</code></pre> <p>Memory Efficient Node Storage:</p> <pre><code>// Nodes stored with minimal overhead\npub struct NodeStorage {\n    nodes: SlotMap&lt;NodeId, Node&gt;,\n    node_index: HashMap&lt;String, NodeId&gt;,\n}\n</code></pre>"},{"location":"development/architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"development/architecture/#benchmarks","title":"Benchmarks","text":"Operation Performance Notes Workflow Build ~1ms For typical 10-node workflow Node Execution ~100-500ms Depends on LLM provider Parallel Processing 2-5x speedup For independent nodes Memory Usage &lt;50MB base Scales with workflow complexity"},{"location":"development/architecture/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"development/architecture/#1-async-processing","title":"1. Async Processing","text":"<pre><code>// All I/O operations are async\npub async fn execute_workflow(&amp;self, workflow: &amp;Workflow) -&gt; Result&lt;ExecutionResult&gt; {\n    // Concurrent execution of independent nodes\n    let futures = ready_nodes.into_iter()\n        .map(|node| self.execute_node(node))\n        .collect::&lt;Vec&lt;_&gt;&gt;();\n\n    let results = join_all(futures).await;\n    // Process results...\n}\n</code></pre>"},{"location":"development/architecture/#2-connection-reuse","title":"2. Connection Reuse","text":"<pre><code>// HTTP client reuse\npub struct LlmClientManager {\n    clients: HashMap&lt;String, Arc&lt;dyn LlmClient&gt;&gt;,\n}\n</code></pre>"},{"location":"development/architecture/#3-memory-pooling","title":"3. Memory Pooling","text":"<pre><code>// Reuse allocations where possible\npub struct NodeExecutor {\n    buffer_pool: ObjectPool&lt;Vec&lt;u8&gt;&gt;,\n}\n</code></pre>"},{"location":"development/architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"development/architecture/#api-key-management","title":"API Key Management","text":"<pre><code>pub struct SecureConfig {\n    // API keys stored securely\n    encrypted_keys: HashMap&lt;String, Vec&lt;u8&gt;&gt;,\n    key_cipher: ChaCha20Poly1305,\n}\n</code></pre>"},{"location":"development/architecture/#input-validation","title":"Input Validation","text":"<pre><code>pub fn validate_input(input: &amp;str) -&gt; Result&lt;()&gt; {\n    // Comprehensive input validation\n    if input.len() &gt; MAX_INPUT_SIZE {\n        return Err(Error::InputTooLarge);\n    }\n\n    // Check for potentially dangerous content\n    validate_content_safety(input)?;\n\n    Ok(())\n}\n</code></pre>"},{"location":"development/architecture/#safe-template-processing","title":"Safe Template Processing","text":"<pre><code>pub struct SafeTemplateEngine {\n    // Prevent template injection attacks\n    allowed_functions: HashSet&lt;String&gt;,\n}\n</code></pre>"},{"location":"development/architecture/#extensibility","title":"Extensibility","text":""},{"location":"development/architecture/#plugin-architecture","title":"Plugin Architecture","text":"<pre><code>pub trait NodeProcessor: Send + Sync {\n    fn process(&amp;self, input: &amp;NodeInput) -&gt; Result&lt;NodeOutput&gt;;\n    fn node_type(&amp;self) -&gt; &amp;str;\n}\n\n// Register custom processors\npub struct ProcessorRegistry {\n    processors: HashMap&lt;String, Box&lt;dyn NodeProcessor&gt;&gt;,\n}\n</code></pre>"},{"location":"development/architecture/#custom-llm-providers","title":"Custom LLM Providers","text":"<pre><code>// Implement custom provider\npub struct CustomLlmClient {\n    // Custom implementation\n}\n\n#[async_trait]\nimpl LlmClient for CustomLlmClient {\n    async fn generate(&amp;self, request: GenerateRequest) -&gt; Result&lt;GenerateResponse&gt; {\n        // Custom LLM integration\n    }\n}\n</code></pre>"},{"location":"development/architecture/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"development/architecture/#structured-logging","title":"Structured Logging","text":"<pre><code>use tracing::{info, warn, error, instrument};\n\n#[instrument]\npub async fn execute_node(&amp;self, node: &amp;Node) -&gt; Result&lt;NodeResult&gt; {\n    info!(node_id = %node.id(), \"Starting node execution\");\n\n    let start = Instant::now();\n    let result = self.process_node(node).await;\n    let duration = start.elapsed();\n\n    match &amp;result {\n        Ok(_) =&gt; info!(\n            node_id = %node.id(),\n            duration_ms = duration.as_millis(),\n            \"Node execution completed successfully\"\n        ),\n        Err(e) =&gt; error!(\n            node_id = %node.id(),\n            error = %e,\n            \"Node execution failed\"\n        ),\n    }\n\n    result\n}\n</code></pre>"},{"location":"development/architecture/#metrics-collection","title":"Metrics Collection","text":"<pre><code>pub struct MetricsCollector {\n    execution_times: Histogram,\n    success_counter: Counter,\n    error_counter: Counter,\n}\n</code></pre> <p>This architecture provides a solid foundation for building scalable, reliable AI workflows while maintaining high performance and developer productivity. </p>"},{"location":"development/contributing/","title":"Contributing to GraphBit","text":"<p>Welcome to the GraphBit project! We're excited that you're interested in contributing to our high-performance AI agent workflow framework.</p> <p>Note: This document provides a quick overview for documentation purposes. For complete development setup and detailed contributing guidelines, please see the main CONTRIBUTING.md file in the project root.</p>"},{"location":"development/contributing/#quick-start-for-contributors","title":"Quick Start for Contributors","text":""},{"location":"development/contributing/#1-development-setup","title":"1. Development Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/InfinitiBit/graphbit.git\ncd graphbit\n\n# Set up development environment\nmake dev-setup\n\n# Install pre-commit hooks\nmake pre-commit-install\n\n# Run tests to verify setup\nmake test\n</code></pre>"},{"location":"development/contributing/#2-project-structure","title":"2. Project Structure","text":"<pre><code>graphbit/\n\u251c\u2500\u2500 core/                 # Rust core library\n\u251c\u2500\u2500 python/              # Python bindings\n\u251c\u2500\u2500 src/                 # CLI application\n\u251c\u2500\u2500 docs/                # Documentation (this folder)\n\u251c\u2500\u2500 examples/            # Example workflows\n\u251c\u2500\u2500 tests/               # Integration tests\n\u2514\u2500\u2500 benchmarks/          # Performance benchmarks\n</code></pre>"},{"location":"development/contributing/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"development/contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<ul> <li>Use GitHub issues to report bugs</li> <li>Include system information and steps to reproduce</li> <li>Provide minimal reproducible examples</li> </ul>"},{"location":"development/contributing/#feature-requests","title":"\u2728 Feature Requests","text":"<ul> <li>Discuss new features in GitHub discussions</li> <li>Consider backward compatibility</li> <li>Include use cases and rationale</li> </ul>"},{"location":"development/contributing/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Improve existing documentation</li> <li>Add examples and tutorials</li> <li>Fix typos and improve clarity</li> </ul>"},{"location":"development/contributing/#code-contributions","title":"\ud83e\uddf9 Code Contributions","text":"<ul> <li>Follow coding standards (see main CONTRIBUTING.md)</li> <li>Write tests for new features</li> <li>Ensure all quality checks pass</li> </ul>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li>Fork the repository on GitHub</li> <li>Create a feature branch from <code>main</code></li> <li>Make your changes following our coding standards</li> <li>Test your changes thoroughly</li> <li>Submit a pull request</li> </ol>"},{"location":"development/contributing/#code-quality","title":"Code Quality","text":"<p>We maintain high code quality standards:</p> <ul> <li>Rust: <code>cargo fmt</code>, <code>cargo clippy</code>, <code>cargo test</code></li> <li>Python: <code>black</code>, <code>isort</code>, <code>flake8</code>, <code>mypy</code>, <code>pytest</code></li> <li>Pre-commit: Automated quality checks on every commit</li> </ul>"},{"location":"development/contributing/#testing","title":"Testing","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test categories\ncargo test --workspace          # Rust tests\npython -m pytest tests/ -v     # Python tests\nmake integration-test           # Integration tests\n</code></pre>"},{"location":"development/contributing/#architecture-overview","title":"Architecture Overview","text":"<p>GraphBit uses a three-tier architecture:</p> <ul> <li>Python API: PyO3 bindings with async support and production-grade features</li> <li>CLI Tool: Project management and execution</li> <li>Rust Core: High-performance workflow engine, agents, LLM providers</li> </ul> <p>Key components: - Python Bindings: Production-ready PyO3 bindings with comprehensive error handling, circuit breakers, and performance monitoring - Workflow Engine: Graph execution and dependency management - Agent System: AI-powered processing components - LLM Providers: Multi-provider abstraction (OpenAI, Anthropic, Ollama, etc.) - Type System: Strong typing with comprehensive validation</p> <p>For detailed Python bindings architecture, see Python Bindings Architecture.</p>"},{"location":"development/contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"development/contributing/#rust-code","title":"Rust Code","text":"<ul> <li>Follow <code>rustfmt</code> formatting</li> <li>Use <code>clippy</code> for linting</li> <li>Write comprehensive documentation</li> <li>Include unit tests for new functionality</li> </ul>"},{"location":"development/contributing/#python-code","title":"Python Code","text":"<ul> <li>Follow PEP 8 with 200-character line length</li> <li>Use type hints for all public APIs</li> <li>Write docstrings for classes and functions</li> <li>Include type checking with <code>mypy</code></li> </ul>"},{"location":"development/contributing/#documentation_1","title":"Documentation","text":"<ul> <li>Write clear, concise documentation</li> <li>Include code examples</li> <li>Keep examples up-to-date</li> <li>Use consistent formatting</li> </ul>"},{"location":"development/contributing/#performance-considerations","title":"Performance Considerations","text":"<p>When contributing code: - Consider memory allocation patterns - Use async/await for I/O operations - Implement proper error handling - Add benchmarks for performance-critical code</p>"},{"location":"development/contributing/#security-guidelines","title":"Security Guidelines","text":"<ul> <li>Never commit API keys or secrets</li> <li>Validate all inputs</li> <li>Use secure communication (HTTPS/TLS)</li> <li>Follow secure coding practices</li> </ul>"},{"location":"development/contributing/#community-guidelines","title":"Community Guidelines","text":"<ul> <li>Be respectful and inclusive</li> <li>Help newcomers get started</li> <li>Share knowledge and expertise</li> <li>Follow our code of conduct</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Check the docs first</li> <li>GitHub Issues: Search existing issues</li> <li>Discussions: Ask questions in GitHub discussions</li> <li>Discord: Join our community Discord (if available)</li> </ul>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - CHANGELOG.md for significant contributions - GitHub contributors page - Release notes for major features</p>"},{"location":"development/contributing/#next-steps","title":"Next Steps","text":"<p>Ready to contribute? Here are some good first steps:</p> <ol> <li>Read the full CONTRIBUTING.md</li> <li>Browse the good first issue label</li> <li>Join our community discussions</li> <li>Start with documentation improvements or small bug fixes</li> </ol> <p>Thank you for contributing to GraphBit! \ud83d\ude80</p> <p>For complete development setup instructions, coding standards, and detailed guidelines, please refer to the main CONTRIBUTING.md file. </p>"},{"location":"development/debugging/","title":"Debugging Guide","text":"<p>This guide covers debugging techniques for GraphBit development, with special focus on Python bindings and common development issues.</p>"},{"location":"development/debugging/#quick-debugging-checklist","title":"Quick Debugging Checklist","text":"<p>When experiencing issues, work through this checklist:</p> <ol> <li>Environment Setup</li> <li>[ ] API keys configured (<code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>)</li> <li>[ ] <code>ARGV0</code> is unset (<code>unset ARGV0</code>)</li> <li>[ ] Python bindings installed (<code>maturin develop</code>)</li> <li> <p>[ ] Dependencies up to date</p> </li> <li> <p>Basic Health Check <code>python    import graphbit    graphbit.init(debug=True)    health = graphbit.health_check()    print(f\"System healthy: {health['overall_healthy']}\")</code></p> </li> <li> <p>Runtime Verification <code>python    info = graphbit.get_system_info()    print(f\"Runtime initialized: {info['runtime_initialized']}\")    print(f\"Worker threads: {info['runtime_worker_threads']}\")</code></p> </li> </ol>"},{"location":"development/debugging/#python-bindings-debugging","title":"Python Bindings Debugging","text":""},{"location":"development/debugging/#1-import-and-initialization-issues","title":"1. Import and Initialization Issues","text":""},{"location":"development/debugging/#problem-module-import-fails","title":"Problem: Module Import Fails","text":"<pre><code># Error: ModuleNotFoundError: No module named 'graphbit'\npython -c \"import graphbit\"\n</code></pre> <p>Solution:</p> <pre><code># Verify installation\npip list | grep graphbit\n\n# Reinstall if missing\ncd python\nmaturin develop\n\n# Check Python path\npython -c \"import sys; print(sys.path)\"\n</code></pre>"},{"location":"development/debugging/#problem-initialization-errors","title":"Problem: Initialization Errors","text":"<pre><code># Error during graphbit.init()\nimport graphbit\ntry:\n    graphbit.init(debug=True, log_level=\"debug\")\n    print(\"Initialization successful\")\nexcept Exception as e:\n    print(f\"Initialization failed: {e}\")\n\n    # Get system info to diagnose\n    try:\n        info = graphbit.get_system_info()\n        print(f\"System info: {info}\")\n    except:\n        print(\"Cannot get system info - core issue\")\n</code></pre> <p>Common Causes: - Missing environment variables - Insufficient system resources - Runtime configuration conflicts</p>"},{"location":"development/debugging/#2-llm-client-issues","title":"2. LLM Client Issues","text":""},{"location":"development/debugging/#problem-configuration-errors","title":"Problem: Configuration Errors","text":"<pre><code>import graphbit\ngraphbit.init(debug=True)\n\ntry:\n    # Test OpenAI configuration\n    config = graphbit.LlmConfig.openai(api_key=\"test-key\")\n    client = graphbit.LlmClient(config, debug=True)\n    print(\"Client created successfully\")\nexcept ValueError as e:\n    print(f\"Configuration error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"development/debugging/#problem-api-request-failures","title":"Problem: API Request Failures","text":"<pre><code>import graphbit\nimport os\n\n# Ensure debug mode is enabled\ngraphbit.init(debug=True, log_level=\"debug\")\n\n# Check API key\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    print(\"ERROR: OPENAI_API_KEY not set\")\n    exit(1)\n\nconfig = graphbit.LlmConfig.openai(api_key=api_key)\nclient = graphbit.LlmClient(config, debug=True)\n\ntry:\n    # Test basic completion\n    response = client.complete(\"Hello\", max_tokens=10)\n    print(f\"Success: {response}\")\nexcept ConnectionError as e:\n    print(f\"Network error: {e}\")\nexcept TimeoutError as e:\n    print(f\"Timeout error: {e}\")\nexcept PermissionError as e:\n    print(f\"Authentication error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n\n    # Get client statistics for diagnosis\n    stats = client.get_stats()\n    print(f\"Client stats: {stats}\")\n</code></pre>"},{"location":"development/debugging/#problem-circuit-breaker-activation","title":"Problem: Circuit Breaker Activation","text":"<pre><code># Check client statistics to see circuit breaker state\nstats = client.get_stats()\nprint(f\"Circuit breaker state: {stats['circuit_breaker_state']}\")\nprint(f\"Failed requests: {stats['failed_requests']}\")\nprint(f\"Total requests: {stats['total_requests']}\")\n\n# Reset client if needed\nclient.reset_stats()\n</code></pre>"},{"location":"development/debugging/#3-workflow-execution-issues","title":"3. Workflow Execution Issues","text":""},{"location":"development/debugging/#problem-executor-configuration","title":"Problem: Executor Configuration","text":"<pre><code>import graphbit\ngraphbit.init(debug=True)\n\nconfig = graphbit.LlmConfig.openai(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ntry:\n    # Test different executor modes\n    executor = graphbit.Executor(config, debug=True)\n    print(f\"Executor mode: {executor.get_execution_mode()}\")\n\n    # Check executor statistics\n    stats = executor.get_stats()\n    print(f\"Executor stats: {stats}\")\n\nexcept Exception as e:\n    print(f\"Executor creation failed: {e}\")\n</code></pre>"},{"location":"development/debugging/#problem-execution-timeouts","title":"Problem: Execution Timeouts","text":"<pre><code># Configure shorter timeout for testing\nexecutor = graphbit.Executor.new_low_latency(\n    config, \n    timeout_seconds=10,  # Very short timeout\n    debug=True\n)\n\ntry:\n    result = executor.execute(workflow)\n    print(\"Execution successful\")\nexcept TimeoutError as e:\n    print(f\"Execution timed out: {e}\")\n\n    # Check execution statistics\n    stats = executor.get_stats()\n    print(f\"Average duration: {stats['average_duration_ms']}ms\")\n</code></pre>"},{"location":"development/debugging/#4-runtime-and-performance-issues","title":"4. Runtime and Performance Issues","text":""},{"location":"development/debugging/#problem-memory-issues","title":"Problem: Memory Issues","text":"<pre><code>import graphbit\ngraphbit.init(debug=True)\n\n# Check memory health\nhealth = graphbit.health_check()\nprint(f\"Memory healthy: {health['memory_healthy']}\")\nprint(f\"Available memory: {health['available_memory_mb']}MB\")\n\n# Monitor memory usage during execution\nimport psutil\nimport os\n\nprocess = psutil.Process(os.getpid())\nprint(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.1f}MB\")\n</code></pre>"},{"location":"development/debugging/#problem-thread-pool-issues","title":"Problem: Thread Pool Issues","text":"<pre><code># Check runtime configuration\ninfo = graphbit.get_system_info()\nprint(f\"Worker threads: {info['runtime_worker_threads']}\")\nprint(f\"Max blocking threads: {info['runtime_max_blocking_threads']}\")\nprint(f\"CPU count: {info['cpu_count']}\")\n\n# Configure custom runtime if needed\ngraphbit.configure_runtime(\n    worker_threads=4,\n    max_blocking_threads=16,\n    thread_stack_size_mb=2\n)\n</code></pre>"},{"location":"development/debugging/#rust-core-debugging","title":"Rust Core Debugging","text":""},{"location":"development/debugging/#1-compilation-issues","title":"1. Compilation Issues","text":""},{"location":"development/debugging/#problem-build-failures","title":"Problem: Build Failures","text":"<pre><code># Clean build cache\ncargo clean\nrm -rf target/\n\n# Verbose build to see detailed errors\nRUST_BACKTRACE=full cargo build 2&gt;&amp;1 | tee build.log\n\n# Check for common issues\ngrep -i error build.log\ngrep -i warning build.log\n</code></pre>"},{"location":"development/debugging/#problem-dependency-conflicts","title":"Problem: Dependency Conflicts","text":"<pre><code># Update dependencies\ncargo update\n\n# Check for security issues\ncargo audit\n\n# Resolve version conflicts\ncargo tree --duplicates\n</code></pre>"},{"location":"development/debugging/#2-runtime-issues","title":"2. Runtime Issues","text":""},{"location":"development/debugging/#problem-panic-debugging","title":"Problem: Panic Debugging","text":"<pre><code># Enable full backtraces\nexport RUST_BACKTRACE=full\nexport RUST_LOG=debug\n\n# Run with debugging\ncargo run\n\n# Or for tests\ncargo test -- --nocapture\n</code></pre>"},{"location":"development/debugging/#problem-performance-issues","title":"Problem: Performance Issues","text":"<pre><code># Install profiling tools\ncargo install flamegraph\n\n# Profile the application\ncargo flamegraph --bin graphbit\n\n# Or profile tests\ncargo flamegraph --test integration_tests\n</code></pre>"},{"location":"development/debugging/#environment-debugging","title":"Environment Debugging","text":""},{"location":"development/debugging/#1-api-key-issues","title":"1. API Key Issues","text":"<pre><code># Check environment variables\nenv | grep -E \"(OPENAI|ANTHROPIC)_API_KEY\"\n\n# Test API key validity\ncurl -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n     https://api.openai.com/v1/models \\\n     | jq '.data[0].id'\n\n# Or for Anthropic\ncurl -H \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     -H \"anthropic-version: 2023-06-01\" \\\n     https://api.anthropic.com/v1/messages \\\n     -d '{\"model\":\"claude-3-haiku-20240307\",\"max_tokens\":10,\"messages\":[{\"role\":\"user\",\"content\":\"test\"}]}'\n</code></pre>"},{"location":"development/debugging/#2-network-issues","title":"2. Network Issues","text":"<pre><code># Test network connectivity\nimport requests\n\ntry:\n    response = requests.get(\"https://api.openai.com/v1/models\", timeout=10)\n    print(f\"OpenAI API accessible: {response.status_code}\")\nexcept Exception as e:\n    print(f\"Network issue: {e}\")\n\n# Test with GraphBit client\nimport graphbit\ngraphbit.init(debug=True)\n\nconfig = graphbit.LlmConfig.openai(api_key=os.getenv(\"OPENAI_API_KEY\"))\nclient = graphbit.LlmClient(config, debug=True)\n\ntry:\n    # Warmup to test connectivity\n    await client.warmup()\n    print(\"Warmup successful - network OK\")\nexcept Exception as e:\n    print(f\"Network/API issue: {e}\")\n</code></pre>"},{"location":"development/debugging/#logging-and-tracing","title":"Logging and Tracing","text":""},{"location":"development/debugging/#1-enable-comprehensive-logging","title":"1. Enable Comprehensive Logging","text":"<pre><code># Maximum debugging information\nimport graphbit\ngraphbit.init(\n    debug=True,\n    log_level=\"trace\",  # Most verbose\n    enable_tracing=True\n)\n\n# All operations will now have detailed logging\n</code></pre>"},{"location":"development/debugging/#2-rust-level-logging","title":"2. Rust-Level Logging","text":"<pre><code># Environment variable for Rust logging\nexport RUST_LOG=graphbit=trace,debug\nexport RUST_BACKTRACE=full\n\n# Run your application\npython your_script.py\n</code></pre>"},{"location":"development/debugging/#3-custom-logging","title":"3. Custom Logging","text":"<pre><code>import logging\nimport graphbit\n\n# Configure Python logging\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Initialize GraphBit with debug\ngraphbit.init(debug=True, log_level=\"debug\")\n\n# Your operations will now have detailed logs\n</code></pre>"},{"location":"development/debugging/#performance-debugging","title":"Performance Debugging","text":""},{"location":"development/debugging/#1-python-profiling","title":"1. Python Profiling","text":"<pre><code># Install profiling tools\npip install py-spy memory-profiler\n\n# Profile CPU usage\npy-spy record -o profile.svg -- python your_script.py\n\n# Profile memory usage\nmprof run your_script.py\nmprof plot\n</code></pre>"},{"location":"development/debugging/#2-memory-leak-detection","title":"2. Memory Leak Detection","text":"<pre><code>import tracemalloc\nimport graphbit\n\n# Start memory tracing\ntracemalloc.start()\n\n# Your GraphBit operations\ngraphbit.init()\n# ... perform operations ...\n\n# Check memory usage\ncurrent, peak = tracemalloc.get_traced_memory()\nprint(f\"Current memory usage: {current / 1024 / 1024:.1f} MB\")\nprint(f\"Peak memory usage: {peak / 1024 / 1024:.1f} MB\")\ntracemalloc.stop()\n</code></pre>"},{"location":"development/debugging/#3-performance-monitoring","title":"3. Performance Monitoring","text":"<pre><code>import time\nimport graphbit\n\ngraphbit.init(debug=True)\nconfig = graphbit.LlmConfig.openai(api_key=os.getenv(\"OPENAI_API_KEY\"))\nclient = graphbit.LlmClient(config, debug=True)\n\n# Monitor operation timing\nstart_time = time.time()\ntry:\n    response = client.complete(\"Hello\", max_tokens=10)\n    duration = time.time() - start_time\n    print(f\"Request completed in {duration:.2f}s\")\n\n    # Check client statistics\n    stats = client.get_stats()\n    print(f\"Average response time: {stats['average_response_time_ms']}ms\")\n\nexcept Exception as e:\n    duration = time.time() - start_time\n    print(f\"Request failed after {duration:.2f}s: {e}\")\n</code></pre>"},{"location":"development/debugging/#common-error-patterns","title":"Common Error Patterns","text":""},{"location":"development/debugging/#1-authentication-errors","title":"1. Authentication Errors","text":"<pre><code>PermissionError: Authentication error with OpenAI: Invalid API key\n</code></pre> <p>Solutions: - Verify API key is correct and active - Check API key has proper permissions - Ensure environment variable is set correctly</p>"},{"location":"development/debugging/#2-network-errors","title":"2. Network Errors","text":"<pre><code>ConnectionError: Network error (attempt 1): Connection timeout\n</code></pre> <p>Solutions: - Check internet connectivity - Verify API endpoints are accessible - Consider firewall/proxy issues - Increase timeout settings</p>"},{"location":"development/debugging/#3-resource-errors","title":"3. Resource Errors","text":"<pre><code>MemoryError: Resource exhausted (memory): Out of memory\n</code></pre> <p>Solutions: - Reduce concurrency settings - Use memory-optimized execution mode - Monitor memory usage - Check for memory leaks</p>"},{"location":"development/debugging/#4-timeout-errors","title":"4. Timeout Errors","text":"<pre><code>TimeoutError: Timeout in 'llm_completion' after 30000ms: Request timeout\n</code></pre> <p>Solutions: - Increase timeout settings - Use appropriate execution mode - Check network latency - Consider model/provider performance</p>"},{"location":"development/debugging/#getting-help","title":"Getting Help","text":"<p>When debugging fails:</p> <ol> <li>Collect Information:    ```bash    # System information    python -c \"import graphbit; graphbit.init(); print(graphbit.get_system_info())\"</li> </ol> <p># Health check    python -c \"import graphbit; graphbit.init(); print(graphbit.health_check())\"</p> <p># Environment    env | grep -E \"(RUST_|GRAPHBIT_|OPENAI_|ANTHROPIC_)\"    ```</p> <ol> <li>Create Minimal Reproduction:    ```python    import graphbit    import os</li> </ol> <p># Minimal failing example    graphbit.init(debug=True)    config = graphbit.LlmConfig.openai(api_key=os.getenv(\"OPENAI_API_KEY\"))    client = graphbit.LlmClient(config, debug=True)</p> <p>try:        response = client.complete(\"test\", max_tokens=5)        print(\"Success:\", response)    except Exception as e:        print(\"Error:\", e)        stats = client.get_stats()        print(\"Stats:\", stats)    ```</p> <ol> <li>Submit Issue with:</li> <li>System information output</li> <li>Full error traceback</li> <li>Minimal reproduction code</li> <li>Environment details</li> <li>Steps attempted</li> </ol> <p>This debugging guide should help identify and resolve most common issues in GraphBit development. </p>"},{"location":"development/python-bindings/","title":"Python Bindings Architecture","text":"<p>This document provides comprehensive documentation for GraphBit's Python bindings, built using PyO3 for seamless Rust-Python interoperability.</p>"},{"location":"development/python-bindings/#overview","title":"Overview","text":"<p>GraphBit's Python bindings provide a production-grade, high-performance Python API that exposes the full power of the Rust core library. The bindings are designed with:</p> <ul> <li>Type Safety: Full type checking and validation</li> <li>Performance: Zero-copy operations where possible</li> <li>Reliability: Comprehensive error handling and circuit breakers</li> <li>Async Support: Full async/await compatibility</li> <li>Resource Management: Proper cleanup and memory management</li> </ul>"},{"location":"development/python-bindings/#architecture","title":"Architecture","text":""},{"location":"development/python-bindings/#module-structure","title":"Module Structure","text":"<pre><code>python/src/\n\u251c\u2500\u2500 lib.rs              # Main Python module and initialization\n\u251c\u2500\u2500 runtime.rs          # Tokio runtime management\n\u251c\u2500\u2500 errors.rs           # Error handling and conversion\n\u251c\u2500\u2500 validation.rs       # Input validation utilities\n\u251c\u2500\u2500 llm/               # LLM provider bindings\n\u2502   \u251c\u2500\u2500 mod.rs\n\u2502   \u251c\u2500\u2500 client.rs      # LLM client with resilience patterns\n\u2502   \u2514\u2500\u2500 config.rs      # Provider configuration\n\u251c\u2500\u2500 embeddings/        # Embedding provider bindings\n\u2502   \u251c\u2500\u2500 mod.rs\n\u2502   \u251c\u2500\u2500 client.rs      # Embedding client\n\u2502   \u2514\u2500\u2500 config.rs      # Embedding configuration\n\u2514\u2500\u2500 workflow/          # Workflow execution bindings\n    \u251c\u2500\u2500 mod.rs\n    \u251c\u2500\u2500 executor.rs    # Production-grade executor\n    \u251c\u2500\u2500 workflow.rs    # Workflow definition\n    \u251c\u2500\u2500 node.rs        # Node implementation\n    \u2514\u2500\u2500 result.rs      # Execution results\n</code></pre>"},{"location":"development/python-bindings/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Production-Ready: Built for high-throughput, low-latency environments</li> <li>Resilient: Circuit breakers, retries, and timeout handling</li> <li>Observable: Comprehensive metrics and tracing</li> <li>Configurable: Flexible configuration for different use cases</li> </ol>"},{"location":"development/python-bindings/#core-components","title":"Core Components","text":""},{"location":"development/python-bindings/#1-library-initialization","title":"1. Library Initialization","text":"<p>The main library provides global initialization and system management:</p> <pre><code>import graphbit\n\n# Initialize with default settings\ngraphbit.init()\n\n# Initialize with custom configuration\ngraphbit.init(\n    log_level=\"info\",\n    enable_tracing=True,\n    debug=False\n)\n\n# System information\ninfo = graphbit.get_system_info()\nhealth = graphbit.health_check()\nversion = graphbit.version()\n</code></pre>"},{"location":"development/python-bindings/#key-functions","title":"Key Functions","text":"<ul> <li><code>init()</code>: Global library initialization with logging/tracing setup</li> <li><code>version()</code>: Get library version information</li> <li><code>get_system_info()</code>: Comprehensive system and runtime information</li> <li><code>health_check()</code>: System health validation</li> <li><code>configure_runtime()</code>: Advanced runtime configuration</li> <li><code>shutdown()</code>: Graceful shutdown for cleanup</li> </ul>"},{"location":"development/python-bindings/#2-runtime-management","title":"2. Runtime Management","text":"<p>The runtime module provides optimized Tokio runtime management:</p> <pre><code>// Runtime configuration\npub struct RuntimeConfig {\n    pub worker_threads: Option&lt;usize&gt;,\n    pub thread_stack_size: Option&lt;usize&gt;,\n    pub enable_blocking_pool: bool,\n    pub max_blocking_threads: Option&lt;usize&gt;,\n    pub thread_keep_alive: Option&lt;Duration&gt;,\n    pub thread_name_prefix: String,\n}\n</code></pre> <p>Features: - Auto-detected optimal thread configuration - Memory-efficient stack sizes - Production-grade thread management - Runtime statistics and monitoring</p>"},{"location":"development/python-bindings/#3-error-handling","title":"3. Error Handling","text":"<p>Comprehensive error handling with structured error types:</p> <pre><code>pub enum PythonBindingError {\n    Core(String),\n    Configuration { message: String, field: Option&lt;String&gt; },\n    Runtime { message: String, operation: String },\n    Network { message: String, retry_count: u32 },\n    Authentication { message: String, provider: Option&lt;String&gt; },\n    Validation { message: String, field: String, value: Option&lt;String&gt; },\n    RateLimit { message: String, retry_after: Option&lt;u64&gt; },\n    Timeout { message: String, operation: String, duration_ms: u64 },\n    ResourceExhausted { message: String, resource_type: String },\n}\n</code></pre> <p>Error Mapping: - Network errors \u2192 <code>PyConnectionError</code> - Authentication errors \u2192 <code>PyPermissionError</code> - Validation errors \u2192 <code>PyValueError</code> - Timeout errors \u2192 <code>PyTimeoutError</code> - Resource errors \u2192 <code>PyMemoryError</code></p>"},{"location":"development/python-bindings/#llm-integration","title":"LLM Integration","text":""},{"location":"development/python-bindings/#configuration","title":"Configuration","text":"<pre><code># OpenAI configuration\nconfig = graphbit.LlmConfig.openai(\n    api_key=\"your-key\",\n    model=\"gpt-4o-mini\"  # default\n)\n\n# Anthropic configuration\nconfig = graphbit.LlmConfig.anthropic(\n    api_key=\"your-key\", \n    model=\"claude-3-5-sonnet-20241022\"  # default\n)\n\n# Ollama configuration (local)\nconfig = graphbit.LlmConfig.ollama(\n    model=\"llama3.2\"  # default\n)\n</code></pre>"},{"location":"development/python-bindings/#client-usage","title":"Client Usage","text":"<pre><code># Create client with resilience features\nclient = graphbit.LlmClient(config, debug=False)\n\n# Synchronous completion\nresponse = client.complete(\n    prompt=\"Hello, world!\",\n    max_tokens=100,\n    temperature=0.7\n)\n\n# Asynchronous completion\nimport asyncio\nresponse = await client.complete_async(\n    prompt=\"Hello, world!\",\n    max_tokens=100,\n    temperature=0.7\n)\n\n# Batch processing\nresponses = await client.complete_batch(\n    prompts=[\"Hello\", \"World\"],\n    max_tokens=100,\n    temperature=0.7,\n    max_concurrency=5\n)\n\n# Streaming responses\nasync for chunk in client.complete_stream(\n    prompt=\"Tell me a story\",\n    max_tokens=500\n):\n    print(chunk, end=\"\")\n</code></pre>"},{"location":"development/python-bindings/#client-features","title":"Client Features","text":"<ul> <li>Circuit Breaker: Automatic failure detection and recovery</li> <li>Retry Logic: Exponential backoff with configurable limits</li> <li>Timeout Handling: Per-request and global timeouts</li> <li>Connection Pooling: Efficient connection reuse</li> <li>Metrics: Request/response statistics and monitoring</li> <li>Warmup: Preload models for faster first requests</li> </ul>"},{"location":"development/python-bindings/#workflow-execution","title":"Workflow Execution","text":""},{"location":"development/python-bindings/#executor-configuration","title":"Executor Configuration","text":"<pre><code># Basic executor\nexecutor = graphbit.Executor(llm_config)\n\n# High-throughput executor\nexecutor = graphbit.Executor.new_high_throughput(\n    llm_config,\n    timeout_seconds=300,\n    debug=False\n)\n\n# Low-latency executor  \nexecutor = graphbit.Executor.new_low_latency(\n    llm_config,\n    timeout_seconds=30,\n    debug=False\n)\n\n# Memory-optimized executor\nexecutor = graphbit.Executor.new_memory_optimized(\n    llm_config,\n    timeout_seconds=180,\n    debug=False\n)\n</code></pre>"},{"location":"development/python-bindings/#execution-modes","title":"Execution Modes","text":"<ol> <li>HighThroughput: Optimized for batch processing</li> <li>Higher concurrency (4x CPU cores)</li> <li>Longer timeouts</li> <li> <p>Resource-intensive operations</p> </li> <li> <p>LowLatency: Optimized for real-time applications</p> </li> <li>Shorter timeouts (30s default)</li> <li>Fewer retries</li> <li> <p>Quick response prioritization</p> </li> <li> <p>MemoryOptimized: Resource-constrained environments</p> </li> <li>Lower concurrency</li> <li>Smaller memory footprint</li> <li> <p>Efficient resource usage</p> </li> <li> <p>Balanced: General-purpose configuration</p> </li> <li>Default settings</li> <li>Good balance of performance and resources</li> </ol>"},{"location":"development/python-bindings/#workflow-execution_1","title":"Workflow Execution","text":"<pre><code># Synchronous execution\nresult = executor.execute(workflow)\n\n# Asynchronous execution\nresult = await executor.run_async(workflow)\n\n# Get execution statistics\nstats = executor.get_stats()\nprint(f\"Total executions: {stats['total_executions']}\")\nprint(f\"Success rate: {stats['successful_executions'] / stats['total_executions']}\")\nprint(f\"Average duration: {stats['average_duration_ms']}ms\")\n</code></pre>"},{"location":"development/python-bindings/#embedding-integration","title":"Embedding Integration","text":""},{"location":"development/python-bindings/#configuration_1","title":"Configuration","text":"<pre><code># OpenAI embeddings\nconfig = graphbit.EmbeddingConfig.openai(\n    api_key=\"your-key\",\n    model=\"text-embedding-3-small\"  # default\n)\n\n# HuggingFace embeddings\nconfig = graphbit.EmbeddingConfig.huggingface(\n    api_key=\"your-key\",\n    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n</code></pre>"},{"location":"development/python-bindings/#client-usage_1","title":"Client Usage","text":"<pre><code>client = graphbit.EmbeddingClient(config)\n\n# Single text embedding\nembedding = await client.embed_text(\"Hello, world!\")\n\n# Batch text embeddings\nembeddings = await client.embed_batch([\n    \"First text\",\n    \"Second text\",\n    \"Third text\"\n])\n\n# Document embedding with metadata\nembedding = await client.embed_document(\n    content=\"Document content\",\n    metadata={\"source\": \"file.txt\", \"type\": \"document\"}\n)\n</code></pre>"},{"location":"development/python-bindings/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"development/python-bindings/#memory-management","title":"Memory Management","text":"<ul> <li>Stack Size: Optimized 1MB stack per thread</li> <li>Allocator: jemalloc on Linux for better memory efficiency</li> <li>Connection Pooling: Reuse HTTP connections</li> <li>Zero-Copy: Minimize data copying between Rust and Python</li> </ul>"},{"location":"development/python-bindings/#concurrency","title":"Concurrency","text":"<ul> <li>Worker Threads: Auto-detected optimal count (2x CPU cores, capped at 32)</li> <li>Blocking Pool: Separate thread pool for I/O operations</li> <li>Circuit Breakers: Prevent cascade failures</li> <li>Rate Limiting: Respect provider limits</li> </ul>"},{"location":"development/python-bindings/#monitoring","title":"Monitoring","text":"<pre><code># System information\ninfo = graphbit.get_system_info()\nprint(f\"Worker threads: {info['runtime_worker_threads']}\")\nprint(f\"Memory allocator: {info['memory_allocator']}\")\n\n# Health check\nhealth = graphbit.health_check()\nprint(f\"Overall healthy: {health['overall_healthy']}\")\nprint(f\"Available memory: {health['available_memory_mb']}MB\")\n\n# Client statistics\nstats = client.get_stats()\nprint(f\"Total requests: {stats['total_requests']}\")\nprint(f\"Average response time: {stats['average_response_time_ms']}ms\")\n</code></pre>"},{"location":"development/python-bindings/#development-guidelines","title":"Development Guidelines","text":""},{"location":"development/python-bindings/#error-handling","title":"Error Handling","text":"<p>Always handle errors appropriately:</p> <pre><code>try:\n    result = client.complete(\"Hello, world!\")\nexcept ConnectionError as e:\n    # Network issues\n    print(f\"Connection failed: {e}\")\nexcept TimeoutError as e:\n    # Request timeout\n    print(f\"Request timed out: {e}\")\nexcept ValueError as e:\n    # Invalid input\n    print(f\"Invalid input: {e}\")\n</code></pre>"},{"location":"development/python-bindings/#resource-management","title":"Resource Management","text":"<pre><code># Initialize once per application\ngraphbit.init()\n\n# Reuse clients\nclient = graphbit.LlmClient(config)\n\n# Graceful shutdown\ngraphbit.shutdown()\n</code></pre>"},{"location":"development/python-bindings/#debugging","title":"Debugging","text":"<pre><code># Enable debug mode\ngraphbit.init(debug=True, log_level=\"debug\")\n\n# Create client with debug output\nclient = graphbit.LlmClient(config, debug=True)\n\n# Check system health\nhealth = graphbit.health_check()\nif not health['overall_healthy']:\n    print(\"System issues detected!\")\n</code></pre>"},{"location":"development/python-bindings/#best-practices","title":"Best Practices","text":""},{"location":"development/python-bindings/#initialization","title":"Initialization","text":"<ol> <li>Call <code>graphbit.init()</code> once at application startup</li> <li>Configure logging level appropriately for environment</li> <li>Use debug mode only during development</li> </ol>"},{"location":"development/python-bindings/#client-management","title":"Client Management","text":"<ol> <li>Create LLM/Embedding clients once and reuse</li> <li>Use appropriate execution modes for your use case</li> <li>Monitor client statistics for performance insights</li> </ol>"},{"location":"development/python-bindings/#error-handling_1","title":"Error Handling","text":"<ol> <li>Handle specific exception types appropriately</li> <li>Implement retry logic for transient failures</li> <li>Use circuit breaker patterns for resilience</li> </ol>"},{"location":"development/python-bindings/#performance","title":"Performance","text":"<ol> <li>Use async methods for I/O-bound operations</li> <li>Batch requests when possible</li> <li>Monitor memory usage and adjust concurrency</li> <li>Use streaming for large responses</li> </ol>"},{"location":"development/python-bindings/#migration-guide","title":"Migration Guide","text":""},{"location":"development/python-bindings/#from-v00x-to-v01x","title":"From v0.0.x to v0.1.x","text":"<p>Key changes in the Python bindings:</p> <ol> <li>Initialization: Now requires explicit <code>graphbit.init()</code></li> <li>Error Types: More specific exception types</li> <li>Async Support: Full async/await compatibility</li> <li>Configuration: Simplified configuration objects</li> <li>Metrics: Built-in statistics and monitoring</li> </ol>"},{"location":"development/python-bindings/#upgrading-code","title":"Upgrading Code","text":"<pre><code># Old way (v0.0.x)\nimport graphbit\nclient = graphbit.LlmClient(\"openai\", api_key=\"key\")\n\n# New way (v0.1.x)  \nimport graphbit\ngraphbit.init()\nconfig = graphbit.LlmConfig.openai(api_key=\"key\")\nclient = graphbit.LlmClient(config)\n</code></pre> <p>This comprehensive Python binding provides a robust, production-ready interface to GraphBit's core functionality while maintaining excellent performance and reliability characteristics. </p>"},{"location":"examples/comprehensive-pipeline/","title":"Comprehensive AI Pipeline","text":"<p>This example demonstrates a complete AI pipeline combining GraphBit's workflow system, embedding capabilities, and LLM integration for building sophisticated AI applications.</p>"},{"location":"examples/comprehensive-pipeline/#overview","title":"Overview","text":"<p>We'll build an intelligent document analysis and recommendation system that: 1. Processes documents with semantic understanding 2. Analyzes content using LLM workflows 3. Embeds documents for similarity search 4. Generates intelligent recommendations 5. Monitors system performance and health</p>"},{"location":"examples/comprehensive-pipeline/#complete-system-implementation","title":"Complete System Implementation","text":"<pre><code>import graphbit\nimport os\nimport json\nimport asyncio\nfrom typing import List, Dict, Optional, Tuple\nfrom dataclasses import dataclass\nimport time\n\n@dataclass\nclass Document:\n    \"\"\"Document data structure.\"\"\"\n    id: str\n    title: str\n    content: str\n    category: str\n    metadata: Dict[str, any]\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Analysis result data structure.\"\"\"\n    document_id: str\n    summary: str\n    key_topics: List[str]\n    sentiment: str\n    quality_score: float\n    recommendations: List[str]\n\nclass IntelligentDocumentPipeline:\n    \"\"\"Comprehensive document analysis and recommendation system.\"\"\"\n\n    def __init__(self, openai_api_key: str, anthropic_api_key: Optional[str] = None):\n        \"\"\"Initialize the intelligent document pipeline.\"\"\"\n\n        # Initialize GraphBit\n        graphbit.init(log_level=\"info\", enable_tracing=True)\n\n        # Configure multiple LLM providers\n        self.llm_configs = {\n            'openai': graphbit.LlmConfig.openai(openai_api_key, \"gpt-4o-mini\"),\n            'openai_fast': graphbit.LlmConfig.openai(openai_api_key, \"gpt-4o-mini\")\n        }\n\n        if anthropic_api_key:\n            self.llm_configs['anthropic'] = graphbit.LlmConfig.anthropic(\n                anthropic_api_key, \n                \"claude-3-5-sonnet-20241022\"\n            )\n\n        # Configure embeddings\n        self.embedding_config = graphbit.EmbeddingConfig.openai(\n            openai_api_key,\n            \"text-embedding-3-small\"\n        )\n        self.embedding_client = graphbit.EmbeddingClient(self.embedding_config)\n\n        # Create executors for different use cases\n        self.executors = {\n            'analysis': graphbit.Executor(\n                self.llm_configs['openai'],\n                timeout_seconds=180,\n                debug=True\n            ),\n            'batch': graphbit.Executor.new_high_throughput(\n                self.llm_configs['openai_fast'],\n                timeout_seconds=120,\n                debug=False\n            ),\n            'fast': graphbit.Executor.new_low_latency(\n                self.llm_configs['openai_fast'],\n                timeout_seconds=60,\n                debug=False\n            )\n        }\n\n        # Document storage\n        self.documents: List[Document] = []\n        self.embeddings: List[List[float]] = []\n        self.analysis_results: Dict[str, AnalysisResult] = {}\n\n        # Create workflows\n        self.workflows = self.create_workflows()\n\n    def create_workflows(self) -&gt; Dict[str, graphbit.Workflow]:\n        \"\"\"Create all workflow pipelines.\"\"\"\n        workflows = {}\n\n        # 1. Document Analysis Workflow\n        workflows['analysis'] = self.create_document_analysis_workflow()\n\n        # 2. Content Enhancement Workflow  \n        workflows['enhancement'] = self.create_content_enhancement_workflow()\n\n        # 3. Quality Assessment Workflow\n        workflows['quality'] = self.create_quality_assessment_workflow()\n\n        # 4. Recommendation Generation Workflow\n        workflows['recommendation'] = self.create_recommendation_workflow()\n\n        return workflows\n\n    def create_document_analysis_workflow(self) -&gt; graphbit.Workflow:\n        \"\"\"Create comprehensive document analysis workflow.\"\"\"\n\n        workflow = graphbit.Workflow(\"Document Analysis Pipeline\")\n\n        # Content Preprocessor\n        preprocessor = graphbit.Node.agent(\n            name=\"Content Preprocessor\",\n            prompt=\"\"\"Preprocess this document for analysis:\n\nTitle: {title}\nContent: {content}\nCategory: {category}\n\nTasks:\n1. Extract key information and structure\n2. Identify main topics and themes  \n3. Note any special content (data, quotes, references)\n4. Assess content complexity and readability\n5. Identify potential quality issues\n\nProvide structured preprocessing results.\n\"\"\",\n            agent_id=\"preprocessor\"\n        )\n\n        # Content Analyzer\n        analyzer = graphbit.Node.agent(\n            name=\"Content Analyzer\",\n            prompt=\"\"\"Analyze this preprocessed document content:\n\n{preprocessed_content}\n\nPerform comprehensive analysis:\n\n1. **Topic Analysis**: Identify and rank key topics (max 5)\n2. **Sentiment Analysis**: Determine overall sentiment (positive/negative/neutral)\n3. **Content Quality**: Rate quality 1-10 based on clarity, depth, accuracy\n4. **Key Insights**: Extract 3-5 main insights or findings\n5. **Content Type**: Classify the content type and purpose\n\nFormat response as JSON with clear sections for each analysis type.\n\"\"\",\n            agent_id=\"analyzer\"\n        )\n\n        # Summary Generator\n        summarizer = graphbit.Node.agent(\n            name=\"Summary Generator\",\n            prompt=\"\"\"Generate a comprehensive summary based on this analysis:\n\nContent Analysis: {analysis_results}\nOriginal Content: {preprocessed_content}\n\nCreate:\n1. **Executive Summary**: 2-3 sentence overview\n2. **Key Points**: Bullet points of main findings\n3. **Topics**: List of main topics with brief descriptions\n4. **Insights**: Notable insights or conclusions\n5. **Context**: How this content fits into its category\n\nKeep summary informative yet concise.\n\"\"\",\n            agent_id=\"summarizer\"\n        )\n\n        # Connect the pipeline\n        prep_id = workflow.add_node(preprocessor)\n        analyze_id = workflow.add_node(analyzer)\n        summary_id = workflow.add_node(summarizer)\n\n        workflow.connect(prep_id, analyze_id)\n        workflow.connect(analyze_id, summary_id)\n\n        workflow.validate()\n        return workflow\n\n    def create_content_enhancement_workflow(self) -&gt; graphbit.Workflow:\n        \"\"\"Create content enhancement and optimization workflow.\"\"\"\n\n        workflow = graphbit.Workflow(\"Content Enhancement Pipeline\")\n\n        # Content Reviewer\n        reviewer = graphbit.Node.agent(\n            name=\"Content Reviewer\",\n            prompt=\"\"\"Review this content for enhancement opportunities:\n\n{content}\n\nEvaluate:\n1. **Clarity**: Is the content clear and well-structured?\n2. **Completeness**: Are there missing elements or gaps?\n3. **Engagement**: How engaging is the content?\n4. **Accuracy**: Are there any factual concerns?\n5. **Optimization**: What improvements could be made?\n\nProvide specific, actionable recommendations.\n\"\"\",\n            agent_id=\"reviewer\"\n        )\n\n        # Enhancement Suggester\n        enhancer = graphbit.Node.agent(\n            name=\"Enhancement Suggester\",\n            prompt=\"\"\"Based on this review, suggest specific enhancements:\n\nReview Results: {review_results}\nOriginal Content: {content}\n\nSuggest improvements for:\n1. **Structure**: Better organization or formatting\n2. **Content**: Additional information or clarifications\n3. **Engagement**: Ways to make content more engaging\n4. **SEO**: Search optimization opportunities\n5. **Accessibility**: Improvements for better accessibility\n\nPrioritize suggestions by impact and feasibility.\n\"\"\",\n            agent_id=\"enhancer\"\n        )\n\n        # Connect pipeline\n        review_id = workflow.add_node(reviewer)\n        enhance_id = workflow.add_node(enhancer)\n\n        workflow.connect(review_id, enhance_id)\n\n        workflow.validate()\n        return workflow\n\n    def create_quality_assessment_workflow(self) -&gt; graphbit.Workflow:\n        \"\"\"Create quality assessment workflow.\"\"\"\n\n        workflow = graphbit.Workflow(\"Quality Assessment Pipeline\")\n\n        # Quality Assessor\n        assessor = graphbit.Node.agent(\n            name=\"Quality Assessor\",\n            prompt=\"\"\"Assess the quality of this content comprehensively:\n\n{content}\n\nRate (1-10) and explain:\n1. **Accuracy**: Factual correctness and reliability\n2. **Clarity**: How clear and understandable the content is\n3. **Depth**: Level of detail and thoroughness\n4. **Structure**: Organization and logical flow\n5. **Relevance**: How relevant to its intended purpose\n6. **Originality**: Uniqueness and fresh insights\n\nProvide overall quality score and detailed feedback.\n\"\"\",\n            agent_id=\"assessor\"\n        )\n\n        # Quality Gate\n        quality_gate = graphbit.Node.condition(\n            name=\"Quality Gate\",\n            expression=\"overall_quality &gt;= 7\"\n        )\n\n        # Improvement Recommender\n        improver = graphbit.Node.agent(\n            name=\"Improvement Recommender\",\n            prompt=\"\"\"Based on this quality assessment, recommend improvements:\n\nQuality Assessment: {quality_results}\n\nFor content that scored below 7, provide:\n1. **Priority Issues**: Most critical problems to address\n2. **Quick Wins**: Easy improvements with high impact\n3. **Long-term Improvements**: Comprehensive enhancement strategies\n4. **Resources**: Suggest tools or resources for improvement\n\nFocus on actionable, specific recommendations.\n\"\"\",\n            agent_id=\"improver\"\n        )\n\n        # Connect pipeline\n        assess_id = workflow.add_node(assessor)\n        gate_id = workflow.add_node(quality_gate)\n        improve_id = workflow.add_node(improver)\n\n        workflow.connect(assess_id, gate_id)\n        workflow.connect(gate_id, improve_id)\n\n        workflow.validate()\n        return workflow\n\n    def create_recommendation_workflow(self) -&gt; graphbit.Workflow:\n        \"\"\"Create intelligent recommendation workflow.\"\"\"\n\n        workflow = graphbit.Workflow(\"Recommendation Engine\")\n\n        # Context Analyzer\n        context_analyzer = graphbit.Node.agent(\n            name=\"Context Analyzer\",\n            prompt=\"\"\"Analyze the context for generating recommendations:\n\nCurrent Document: {current_document}\nSimilar Documents: {similar_documents}\nUser Preferences: {user_preferences}\nDocument Category: {category}\n\nAnalyze:\n1. **Content Themes**: Common themes across documents\n2. **User Patterns**: What the user seems interested in\n3. **Content Gaps**: Missing information or topics\n4. **Complementary Content**: What would complement this content\n5. **Trending Topics**: Relevant trending or popular topics\n\nProvide context analysis for recommendation generation.\n\"\"\",\n            agent_id=\"context_analyzer\"\n        )\n\n        # Recommendation Generator\n        recommender = graphbit.Node.agent(\n            name=\"Recommendation Generator\",\n            prompt=\"\"\"Generate intelligent recommendations based on this context:\n\nContext Analysis: {context_analysis}\n\nGenerate recommendations for:\n1. **Related Content**: Documents or topics to explore next\n2. **Deep Dive**: Areas for more detailed investigation\n3. **Broad Exploration**: Related but different topics\n4. **Practical Applications**: How to apply this knowledge\n5. **Learning Path**: Suggested sequence for learning more\n\nRank recommendations by relevance and provide reasoning.\n\"\"\",\n            agent_id=\"recommender\"\n        )\n\n        # Connect pipeline\n        context_id = workflow.add_node(context_analyzer)\n        rec_id = workflow.add_node(recommender)\n\n        workflow.connect(context_id, rec_id)\n\n        workflow.validate()\n        return workflow\n\n    async def add_document(self, document: Document) -&gt; str:\n        \"\"\"Add a document to the system with full analysis.\"\"\"\n\n        print(f\"\ud83d\udd04 Processing document: {document.title}\")\n\n        # Store document\n        self.documents.append(document)\n\n        # Generate embedding\n        embedding = self.embedding_client.embed(\n            f\"{document.title}\\n\\n{document.content}\"\n        )\n        self.embeddings.append(embedding)\n\n        # Run analysis workflow\n        analysis_result = await self.analyze_document(document)\n        self.analysis_results[document.id] = analysis_result\n\n        print(f\"Document processed: {document.id}\")\n        return document.id\n\n    async def analyze_document(self, document: Document) -&gt; AnalysisResult:\n        \"\"\"Analyze document using the analysis workflow.\"\"\"\n\n        # Execute analysis workflow\n        workflow = self.workflows['analysis']\n        executor = self.executors['analysis']\n\n        try:\n            result = await executor.run_async(workflow)\n\n            if result.is_success():\n                # Parse results (simplified - in practice you'd parse JSON)\n                output = result.get_output()\n\n                return AnalysisResult(\n                    document_id=document.id,\n                    summary=output[:200] + \"...\",  # Simplified\n                    key_topics=[\"topic1\", \"topic2\"],  # Would parse from output\n                    sentiment=\"neutral\",  # Would parse from output\n                    quality_score=8.0,  # Would parse from output\n                    recommendations=[\"rec1\", \"rec2\"]  # Would parse from output\n                )\n            else:\n                raise Exception(f\"Analysis failed: {result.get_error()}\")\n\n        except Exception as e:\n            print(f\"Analysis failed for {document.id}: {e}\")\n            # Return default result\n            return AnalysisResult(\n                document_id=document.id,\n                summary=\"Analysis failed\",\n                key_topics=[],\n                sentiment=\"unknown\",\n                quality_score=0.0,\n                recommendations=[]\n            )\n\n    def find_similar_documents(self, document_id: str, top_k: int = 5) -&gt; List[Dict]:\n        \"\"\"Find similar documents using embeddings.\"\"\"\n\n        # Find document index\n        doc_index = None\n        for i, doc in enumerate(self.documents):\n            if doc.id == document_id:\n                doc_index = i\n                break\n\n        if doc_index is None:\n            return []\n\n        query_embedding = self.embeddings[doc_index]\n        similarities = []\n\n        for i, (doc, embedding) in enumerate(zip(self.documents, self.embeddings)):\n            if i == doc_index:\n                continue\n\n            similarity = graphbit.EmbeddingClient.similarity(query_embedding, embedding)\n            similarities.append({\n                'document_id': doc.id,\n                'title': doc.title,\n                'similarity': similarity,\n                'category': doc.category\n            })\n\n        # Sort by similarity\n        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n        return similarities[:top_k]\n\n    async def generate_recommendations(self, document_id: str, user_preferences: Dict = None) -&gt; List[str]:\n        \"\"\"Generate intelligent recommendations for a document.\"\"\"\n\n        # Find similar documents\n        similar_docs = self.find_similar_documents(document_id, top_k=3)\n\n        # Get current document\n        current_doc = None\n        for doc in self.documents:\n            if doc.id == document_id:\n                current_doc = doc\n                break\n\n        if not current_doc:\n            return []\n\n        # Execute recommendation workflow\n        workflow = self.workflows['recommendation']\n        executor = self.executors['fast']\n\n        try:\n            result = await executor.run_async(workflow)\n\n            if result.is_success():\n                # Parse recommendations from output\n                output = result.get_output()\n                # In practice, you'd parse structured JSON output\n                return [\"Recommendation 1\", \"Recommendation 2\", \"Recommendation 3\"]\n            else:\n                return []\n\n        except Exception as e:\n            print(f\"Recommendation generation failed: {e}\")\n            return []\n\n    async def batch_process_documents(self, documents: List[Document]) -&gt; Dict[str, AnalysisResult]:\n        \"\"\"Process multiple documents in batch.\"\"\"\n\n        print(f\"Batch processing {len(documents)} documents...\")\n\n        # Add all documents first\n        for doc in documents:\n            self.documents.append(doc)\n\n            # Generate embeddings in batch\n            texts = [f\"{doc.title}\\n\\n{doc.content}\" for doc in documents]\n            embeddings = self.embedding_client.embed_many(texts)\n            self.embeddings.extend(embeddings)\n\n        # Process analysis in batches\n        batch_results = {}\n        batch_size = 5\n\n        for i in range(0, len(documents), batch_size):\n            batch = documents[i:i + batch_size]\n            print(f\"Processing batch {i//batch_size + 1}/{(len(documents) + batch_size - 1)//batch_size}\")\n\n            # Process batch concurrently\n            tasks = [self.analyze_document(doc) for doc in batch]\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n\n            for doc, result in zip(batch, results):\n                if isinstance(result, Exception):\n                    print(f\"Failed to process {doc.id}: {result}\")\n                else:\n                    batch_results[doc.id] = result\n                    self.analysis_results[doc.id] = result\n\n        print(f\"Batch processing completed: {len(batch_results)}/{len(documents)} successful\")\n        return batch_results\n\n    def get_system_statistics(self) -&gt; Dict:\n        \"\"\"Get comprehensive system statistics.\"\"\"\n\n        stats = {\n            'documents': {\n                'total': len(self.documents),\n                'categories': {},\n                'average_length': 0\n            },\n            'analysis': {\n                'completed': len(self.analysis_results),\n                'average_quality': 0,\n                'sentiment_distribution': {'positive': 0, 'negative': 0, 'neutral': 0}\n            },\n            'embeddings': {\n                'total': len(self.embeddings),\n                'dimension': len(self.embeddings[0]) if self.embeddings else 0\n            },\n            'system': graphbit.get_system_info(),\n            'health': graphbit.health_check()\n        }\n\n        # Calculate document statistics\n        if self.documents:\n            category_counts = {}\n            total_length = 0\n\n            for doc in self.documents:\n                category_counts[doc.category] = category_counts.get(doc.category, 0) + 1\n                total_length += len(doc.content)\n\n            stats['documents']['categories'] = category_counts\n            stats['documents']['average_length'] = total_length / len(self.documents)\n\n        # Calculate analysis statistics\n        if self.analysis_results:\n            quality_scores = [r.quality_score for r in self.analysis_results.values()]\n            stats['analysis']['average_quality'] = sum(quality_scores) / len(quality_scores)\n\n            sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}\n            for result in self.analysis_results.values():\n                sentiment_counts[result.sentiment] = sentiment_counts.get(result.sentiment, 0) + 1\n            stats['analysis']['sentiment_distribution'] = sentiment_counts\n\n        return stats\n\n    def export_analysis_results(self, filepath: str):\n        \"\"\"Export analysis results to JSON file.\"\"\"\n\n        export_data = {\n            'documents': [\n                {\n                    'id': doc.id,\n                    'title': doc.title,\n                    'category': doc.category,\n                    'metadata': doc.metadata\n                }\n                for doc in self.documents\n            ],\n            'analysis_results': {\n                doc_id: {\n                    'summary': result.summary,\n                    'key_topics': result.key_topics,\n                    'sentiment': result.sentiment,\n                    'quality_score': result.quality_score,\n                    'recommendations': result.recommendations\n                }\n                for doc_id, result in self.analysis_results.items()\n            },\n            'statistics': self.get_system_statistics(),\n            'export_timestamp': time.time()\n        }\n\n        with open(filepath, 'w') as f:\n            json.dump(export_data, f, indent=2)\n\n        print(f\"Analysis results exported to {filepath}\")\n\n# Example usage and demonstration\nasync def main():\n    \"\"\"Demonstrate the comprehensive AI pipeline.\"\"\"\n\n    # Setup\n    openai_key = os.getenv(\"OPENAI_API_KEY\")\n    anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n\n    if not openai_key:\n        print(\"OPENAI_API_KEY required\")\n        return\n\n    print(\"Initializing Comprehensive AI Pipeline\")\n    print(\"=\" * 60)\n\n    # Create pipeline\n    pipeline = IntelligentDocumentPipeline(openai_key, anthropic_key)\n\n    # Sample documents\n    sample_documents = [\n        Document(\n            id=\"doc1\",\n            title=\"Machine Learning Fundamentals\",\n            content=\"\"\"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It encompasses various algorithms and statistical models that allow systems to automatically learn patterns from data. The field includes supervised learning, where models learn from labeled training data, unsupervised learning, which finds hidden patterns in unlabeled data, and reinforcement learning, where agents learn through interaction with an environment. Key applications include image recognition, natural language processing, recommendation systems, and predictive analytics.\"\"\",\n            category=\"technology\",\n            metadata={\"author\": \"AI Research Team\", \"difficulty\": \"beginner\", \"tags\": [\"AI\", \"ML\", \"fundamentals\"]}\n        ),\n        Document(\n            id=\"doc2\", \n            title=\"Sustainable Energy Solutions\",\n            content=\"\"\"Renewable energy sources such as solar, wind, hydroelectric, and geothermal power are becoming increasingly crucial for addressing climate change and reducing dependence on fossil fuels. Solar technology has seen dramatic cost reductions, with photovoltaic panels becoming more efficient and affordable. Wind energy has also expanded rapidly, with offshore wind farms providing significant power generation capacity. Energy storage technologies, including advanced battery systems and pumped hydro storage, are essential for managing the intermittent nature of renewable sources. Smart grid technologies enable better integration and distribution of renewable energy across power networks.\"\"\",\n            category=\"environment\",\n            metadata={\"author\": \"Green Energy Council\", \"difficulty\": \"intermediate\", \"tags\": [\"renewable\", \"solar\", \"wind\"]}\n        ),\n        Document(\n            id=\"doc3\",\n            title=\"Digital Marketing Strategies\", \n            content=\"\"\"Modern digital marketing encompasses a wide range of strategies and channels designed to reach and engage target audiences online. Social media marketing leverages platforms like Facebook, Instagram, LinkedIn, and Twitter to build brand awareness and drive engagement. Search engine optimization (SEO) improves website visibility in search results, while pay-per-click (PPC) advertising provides immediate visibility. Content marketing focuses on creating valuable, relevant content to attract and retain customers. Email marketing remains highly effective for nurturing leads and maintaining customer relationships. Data analytics tools enable marketers to measure campaign performance and optimize strategies for better results.\"\"\",\n            category=\"business\",\n            metadata={\"author\": \"Marketing Institute\", \"difficulty\": \"intermediate\", \"tags\": [\"marketing\", \"digital\", \"SEO\"]}\n        )\n    ]\n\n    # Process documents individually\n    print(\"\\nProcessing Documents Individually\")\n    print(\"-\" * 40)\n\n    for doc in sample_documents[:2]:  # Process first 2 individually\n        doc_id = await pipeline.add_document(doc)\n        print(f\"Processed: {doc.title} (ID: {doc_id})\")\n\n    # Batch process remaining documents\n    print(\"\\nBatch Processing Remaining Documents\")\n    print(\"-\" * 40)\n\n    remaining_docs = sample_documents[2:]\n    if remaining_docs:\n        batch_results = await pipeline.batch_process_documents(remaining_docs)\n        print(f\"Batch processing completed: {len(batch_results)} documents\")\n\n    # Find similar documents\n    print(\"\\nFinding Similar Documents\")\n    print(\"-\" * 40)\n\n    similar_docs = pipeline.find_similar_documents(\"doc1\", top_k=2)\n    print(f\"Documents similar to '{pipeline.documents[0].title}':\")\n    for sim_doc in similar_docs:\n        print(f\"  - {sim_doc['title']} (similarity: {sim_doc['similarity']:.3f})\")\n\n    # Generate recommendations\n    print(\"\\nGenerating Recommendations\")\n    print(\"-\" * 40)\n\n    recommendations = await pipeline.generate_recommendations(\"doc1\")\n    print(f\"Recommendations for '{pipeline.documents[0].title}':\")\n    for i, rec in enumerate(recommendations, 1):\n        print(f\"  {i}. {rec}\")\n\n    # Display system statistics\n    print(\"\\nSystem Statistics\")\n    print(\"-\" * 40)\n\n    stats = pipeline.get_system_statistics()\n\n    print(f\"Documents: {stats['documents']['total']}\")\n    print(f\"Categories: {list(stats['documents']['categories'].keys())}\")\n    print(f\"Analysis completed: {stats['analysis']['completed']}\")\n    print(f\"Average quality score: {stats['analysis']['average_quality']:.2f}\")\n    print(f\"Embeddings generated: {stats['embeddings']['total']}\")\n    print(f\"System health: {'\u2705' if stats['health']['overall_healthy'] else '\u274c'}\")\n\n    # Export results\n    print(\"\\nExporting Results\")\n    print(\"-\" * 40)\n\n    pipeline.export_analysis_results(\"analysis_results.json\")\n\n    print(\"\\nComprehensive AI Pipeline Demo Completed!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/comprehensive-pipeline/#key-system-features","title":"Key System Features","text":""},{"location":"examples/comprehensive-pipeline/#comprehensive-integration","title":"Comprehensive Integration","text":"<ul> <li>Multi-Workflow System: Analysis, enhancement, quality assessment, recommendations</li> <li>Multiple LLM Providers: OpenAI, Anthropic support with fallbacks</li> <li>Embedding Integration: Semantic search and similarity analysis</li> <li>Performance Optimization: Different executors for different use cases</li> </ul>"},{"location":"examples/comprehensive-pipeline/#advanced-capabilities","title":"Advanced Capabilities","text":"<ul> <li>Batch Processing: Efficient handling of multiple documents</li> <li>Async Operations: Non-blocking operations for better performance</li> <li>Quality Gates: Conditional workflow execution based on quality scores</li> <li>Intelligent Recommendations: Context-aware recommendation generation</li> </ul>"},{"location":"examples/comprehensive-pipeline/#production-features","title":"Production Features","text":"<ul> <li>Error Handling: Comprehensive error management and fallbacks</li> <li>Monitoring: System health checks and performance statistics</li> <li>Export Capabilities: JSON export of analysis results</li> <li>Flexible Configuration: Multiple execution modes and provider support</li> </ul>"},{"location":"examples/comprehensive-pipeline/#real-world-applications","title":"Real-World Applications","text":"<ul> <li>Document Management: Intelligent document analysis and organization</li> <li>Content Platforms: Automated content quality assessment</li> <li>Knowledge Management: Semantic search and recommendation systems</li> <li>Research Tools: Comprehensive analysis and insight generation</li> </ul> <p>This comprehensive example demonstrates how GraphBit's various components work together to create sophisticated, production-ready AI applications that can handle complex workflows with reliability and performance. </p>"},{"location":"examples/content-generation/","title":"Content Generation Pipeline","text":"<p>This example demonstrates how to build a sophisticated content generation workflow with GraphBit, featuring research, writing, editing, and quality assurance stages.</p>"},{"location":"examples/content-generation/#overview","title":"Overview","text":"<p>We'll create a multi-agent pipeline that: 1. Researches a given topic 2. Writes initial content based on research 3. Edits for clarity and engagement 4. Reviews for quality and accuracy 5. Formats the final output</p>"},{"location":"examples/content-generation/#complete-example","title":"Complete Example","text":"<pre><code>import graphbit\nimport os\nfrom typing import Optional\n\nclass ContentGenerationPipeline:\n    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n        \"\"\"Initialize the content generation pipeline.\"\"\"\n        # Initialize GraphBit\n        graphbit.init(log_level=\"info\", enable_tracing=True)\n\n        # Create LLM configuration\n        self.llm_config = graphbit.LlmConfig.openai(api_key, model)\n\n        # Create executor with custom settings\n        self.executor = graphbit.Executor(\n            self.llm_config,\n            timeout_seconds=300,  # 5 minutes\n            debug=True\n        )\n\n    def create_workflow(self, content_type: str = \"article\") -&gt; graphbit.Workflow:\n        \"\"\"Create the content generation workflow.\"\"\"\n        # Create workflow\n        workflow = graphbit.Workflow(\"Content Generation Pipeline\")\n\n        # Stage 1: Research Agent\n        researcher = graphbit.Node.agent(\n            name=\"Research Specialist\",\n            prompt=\"\"\"Research the topic: {topic}\n\nPlease provide:\n1. Key facts and statistics\n2. Current trends and developments\n3. Expert opinions or quotes\n4. Relevant examples or case studies\n5. Important considerations or nuances\n\nFocus on accuracy and credibility. Cite sources where possible.\nFormat as structured research notes.\n\"\"\",\n            agent_id=\"researcher\"\n        )\n\n        # Stage 2: Content Writer\n        writer = graphbit.Node.agent(\n            name=\"Content Writer\",\n            prompt=\"\"\"Write a comprehensive {content_type} about: {topic}\n\nBased on this research: {research_data}\n\nRequirements:\n- Target length: {target_length} words\n- Tone: {tone}\n- Audience: {audience}\n- Include relevant examples and data from research\n- Use engaging headlines and subheadings\n- Ensure logical flow and structure\n\nCreate compelling, informative content that captures reader attention.\n\"\"\",\n            agent_id=\"writer\"\n        )\n\n        # Stage 3: Editor\n        editor = graphbit.Node.agent(\n            name=\"Content Editor\",\n            prompt=\"\"\"Edit and improve the following {content_type}:\n\n{draft_content}\n\nFocus on:\n- Clarity and readability\n- Flow and structure\n- Engaging language\n- Grammar and style\n- Consistency in tone\n- Compelling headlines\n\nMaintain the core message while making it more engaging and polished.\n\"\"\",\n            agent_id=\"editor\"\n        )\n\n        # Stage 4: Quality Reviewer\n        reviewer = graphbit.Node.agent(\n            name=\"Quality Reviewer\",\n            prompt=\"\"\"Review this {content_type} for quality and accuracy:\n\n{edited_content}\n\nProvide feedback on:\n1. Factual accuracy\n2. Completeness of coverage\n3. Logical flow\n4. Audience appropriateness\n5. Areas for improvement\n\nRate overall quality (1-10) and provide specific suggestions.\nIf quality is 7 or above, mark as APPROVED, otherwise mark as NEEDS_REVISION.\n\"\"\",\n            agent_id=\"reviewer\"\n        )\n\n        # Stage 5: Quality Gate (Condition Node)\n        quality_gate = graphbit.Node.condition(\n            name=\"Quality Gate\",\n            expression=\"quality_rating &gt;= 7\"\n        )\n\n        # Stage 6: Final Formatter\n        formatter = graphbit.Node.agent(\n            name=\"Content Formatter\",\n            prompt=\"\"\"Format this content for publication:\n\n{approved_content}\n\nApply:\n- Professional formatting\n- Proper headings hierarchy\n- Bullet points where appropriate\n- Clear paragraph breaks\n- Call-to-action if needed\n\nOutput clean, publication-ready content.\n\"\"\",\n            agent_id=\"formatter\"\n        )\n\n        # Add nodes to workflow\n        research_id = workflow.add_node(researcher)\n        writer_id = workflow.add_node(writer)\n        editor_id = workflow.add_node(editor)\n        reviewer_id = workflow.add_node(reviewer)\n        quality_id = workflow.add_node(quality_gate)\n        formatter_id = workflow.add_node(formatter)\n\n        # Connect the workflow: Research \u2192 Write \u2192 Edit \u2192 Review \u2192 Quality Check \u2192 Format\n        workflow.connect(research_id, writer_id)\n        workflow.connect(writer_id, editor_id)\n        workflow.connect(editor_id, reviewer_id)\n        workflow.connect(reviewer_id, quality_id)\n        workflow.connect(quality_id, formatter_id)\n\n        # Validate workflow\n        workflow.validate()\n\n        return workflow\n\n    def generate_content(\n        self,\n        topic: str,\n        content_type: str = \"article\",\n        target_length: int = 800,\n        tone: str = \"professional\",\n        audience: str = \"general\"\n    ) -&gt; dict:\n        \"\"\"Generate content using the pipeline.\"\"\"\n\n        print(f\"\ud83d\ude80 Starting content generation for: {topic}\")\n\n        # Create workflow\n        workflow = self.create_workflow(content_type)\n\n        # Execute workflow with input context\n        result = self.executor.execute(workflow)\n\n        if result.is_success():\n            execution_time = result.execution_time_ms()\n            print(f\"Content generation completed in {execution_time}ms\")\n\n            return {\n                \"status\": \"success\",\n                \"content\": result.get_output(),\n                \"execution_time_ms\": execution_time,\n                \"workflow_stats\": self.executor.get_stats()\n            }\n        else:\n            error_msg = result.get_error()\n            print(f\"Content generation failed: {error_msg}\")\n\n            return {\n                \"status\": \"error\",\n                \"error\": error_msg,\n                \"workflow_stats\": self.executor.get_stats()\n            }\n\n# Example usage\ndef main():\n    \"\"\"Run the content generation pipeline.\"\"\"\n\n    # Set up API key (you can also set OPENAI_API_KEY environment variable)\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"\u274c Please set OPENAI_API_KEY environment variable\")\n        return\n\n    # Create pipeline\n    pipeline = ContentGenerationPipeline(api_key, \"gpt-4o-mini\")\n\n    # Generate content\n    result = pipeline.generate_content(\n        topic=\"Sustainable Energy Solutions\",\n        content_type=\"blog post\",\n        target_length=1200,\n        tone=\"informative yet engaging\",\n        audience=\"technology enthusiasts\"\n    )\n\n    # Display results\n    if result[\"status\"] == \"success\":\n        print(\"\\nGenerated Content:\")\n        print(\"=\" * 60)\n        print(result[\"content\"])\n        print(\"\\nPerformance Stats:\")\n        print(f\"Execution time: {result['execution_time_ms']}ms\")\n    else:\n        print(f\"\\nGeneration failed: {result['error']}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/content-generation/#alternative-using-different-llm-providers","title":"Alternative: Using Different LLM Providers","text":""},{"location":"examples/content-generation/#using-anthropic-claude","title":"Using Anthropic Claude","text":"<pre><code>import graphbit\nimport os\n\ndef create_anthropic_pipeline():\n    \"\"\"Create pipeline using Anthropic Claude.\"\"\"\n    graphbit.init()\n\n    # Configure for Anthropic\n    config = graphbit.LlmConfig.anthropic(\n        api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n        model=\"claude-3-5-sonnet-20241022\"\n    )\n\n    executor = graphbit.Executor(config, debug=True)\n\n    # Create simple workflow\n    workflow = graphbit.Workflow(\"Anthropic Content Generator\")\n\n    writer = graphbit.Node.agent(\n        name=\"Claude Writer\",\n        prompt=\"\"\"Write a comprehensive article about: {topic}\n\nRequirements:\n- Length: {word_count} words\n- Tone: {tone}\n- Include practical examples\n- Structure with clear headings\n\nCreate engaging, well-researched content.\n\"\"\",\n        agent_id=\"claude_writer\"\n    )\n\n    workflow.add_node(writer)\n    workflow.validate()\n\n    return executor, workflow\n\n# Usage\nexecutor, workflow = create_anthropic_pipeline()\nresult = executor.execute(workflow)\n</code></pre>"},{"location":"examples/content-generation/#using-local-ollama-models","title":"Using Local Ollama Models","text":"<pre><code>import graphbit\n\ndef create_ollama_pipeline():\n    \"\"\"Create pipeline using local Ollama models.\"\"\"\n    graphbit.init()\n\n    # Configure for Ollama (no API key needed)\n    config = graphbit.LlmConfig.ollama(\"llama3.2\")\n\n    executor = graphbit.Executor(\n        config,\n        timeout_seconds=180,  # Longer timeout for local inference\n        debug=True\n    )\n\n    workflow = graphbit.Workflow(\"Local Content Generator\")\n\n    writer = graphbit.Node.agent(\n        name=\"Llama Writer\",\n        prompt=\"\"\"Write about: {topic}\n\nKeep it concise but informative.\nFocus on practical insights.\n\"\"\",\n        agent_id=\"llama_writer\"\n    )\n\n    workflow.add_node(writer)\n    workflow.validate()\n\n    return executor, workflow\n\n# Usage\nexecutor, workflow = create_ollama_pipeline()\nresult = executor.execute(workflow)\n</code></pre>"},{"location":"examples/content-generation/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/content-generation/#high-performance-content-generation","title":"High-Performance Content Generation","text":"<pre><code>import graphbit\nimport os\n\ndef create_high_performance_pipeline():\n    \"\"\"Create optimized pipeline for high-throughput content generation.\"\"\"\n    graphbit.init()\n\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"  # Faster model for high throughput\n    )\n\n    # Use high-throughput executor\n    executor = graphbit.Executor.new_high_throughput(\n        config,\n        timeout_seconds=60,  # Shorter timeout\n        debug=False  # Disable debug for performance\n    )\n\n    return executor\n\ndef create_low_latency_pipeline():\n    \"\"\"Create pipeline optimized for low latency.\"\"\"\n    graphbit.init()\n\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    # Use low-latency executor\n    executor = graphbit.Executor.new_low_latency(\n        config,\n        timeout_seconds=30,  # Very short timeout\n        debug=False\n    )\n\n    return executor\n\ndef create_memory_optimized_pipeline():\n    \"\"\"Create pipeline optimized for memory usage.\"\"\"\n    graphbit.init()\n\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    # Use memory-optimized executor\n    executor = graphbit.Executor.new_memory_optimized(\n        config,\n        timeout_seconds=120,\n        debug=False\n    )\n\n    return executor\n</code></pre>"},{"location":"examples/content-generation/#async-content-generation","title":"Async Content Generation","text":"<pre><code>import graphbit\nimport asyncio\nimport os\n\nasync def generate_content_async():\n    \"\"\"Generate content asynchronously.\"\"\"\n    graphbit.init()\n\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    executor = graphbit.Executor(config)\n\n    # Create workflow\n    workflow = graphbit.Workflow(\"Async Content Generator\")\n\n    writer = graphbit.Node.agent(\n        name=\"Async Writer\",\n        prompt=\"Write a brief article about: {topic}\",\n        agent_id=\"async_writer\"\n    )\n\n    workflow.add_node(writer)\n    workflow.validate()\n\n    # Execute asynchronously\n    result = await executor.run_async(workflow)\n\n    if result.is_success():\n        print(\"Async generation completed\")\n        return result.get_output()\n    else:\n        print(f\"Async generation failed: {result.get_error()}\")\n        return None\n\n# Usage\nasync def main_async():\n    content = await generate_content_async()\n    if content:\n        print(f\"Generated: {content}\")\n\n# Run async\n# asyncio.run(main_async())\n</code></pre>"},{"location":"examples/content-generation/#system-information-and-health-checks","title":"System Information and Health Checks","text":"<pre><code>import graphbit\n\ndef check_system_health():\n    \"\"\"Check GraphBit system health and capabilities.\"\"\"\n    graphbit.init()\n\n    # Get system information\n    system_info = graphbit.get_system_info()\n    print(\"System Information:\")\n    for key, value in system_info.items():\n        print(f\"  {key}: {value}\")\n\n    # Perform health check\n    health_status = graphbit.health_check()\n    print(f\"\\nHealth Status:\")\n    for key, value in health_status.items():\n        print(f\"  {key}: {value}\")\n\n    # Check version\n    version = graphbit.version()\n    print(f\"\\nGraphBit Version: {version}\")\n\n# Usage\ncheck_system_health()\n</code></pre>"},{"location":"examples/content-generation/#key-features","title":"Key Features","text":""},{"location":"examples/content-generation/#content-pipeline-components","title":"Content Pipeline Components","text":"<ul> <li>Research Agent: Gathers comprehensive information on topics</li> <li>Content Writer: Creates initial drafts based on research</li> <li>Editor: Improves clarity, flow, and engagement</li> <li>Quality Reviewer: Ensures accuracy and completeness</li> <li>Formatter: Prepares content for publication</li> </ul>"},{"location":"examples/content-generation/#reliability-features","title":"Reliability Features","text":"<ul> <li>Multiple LLM Providers: OpenAI, Anthropic, Ollama support</li> <li>Execution Modes: High-throughput, low-latency, memory-optimized</li> <li>Error Handling: Comprehensive error reporting and recovery</li> <li>Performance Monitoring: Built-in execution statistics</li> <li>Health Checks: System health and capability monitoring</li> </ul> <p>This example demonstrates GraphBit's capabilities for building production-ready content generation workflows with reliability, performance, and flexibility. </p>"},{"location":"examples/data-processing/","title":"Data Processing Workflow","text":"<p>This example demonstrates how to build a comprehensive data processing pipeline using GraphBit to analyze, transform, and generate insights from structured data.</p>"},{"location":"examples/data-processing/#overview","title":"Overview","text":"<p>We'll create a workflow that: 1. Loads and validates input data 2. Performs statistical analysis 3. Identifies patterns and anomalies 4. Generates actionable insights 5. Creates formatted reports</p>"},{"location":"examples/data-processing/#complete-example","title":"Complete Example","text":"<pre><code>import graphbit\nimport json\nimport os\n\ndef create_data_processing_pipeline():\n    \"\"\"Creates a comprehensive data processing workflow.\"\"\"\n\n    # Initialize GraphBit\n    graphbit.init(enable_tracing=True)\n\n    # Configure LLM\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create executor\n    executor = graphbit.Executor(config, timeout_seconds=180, debug=True)\n\n    # Create workflow\n    workflow = graphbit.Workflow(\"Data Processing Pipeline\")\n\n    # 1. Data Validator\n    validator = graphbit.Node.agent(\n        name=\"Data Validator\",\n        prompt=\"\"\"Validate this dataset and check for:\n- Data completeness\n- Format consistency  \n- Obvious errors or outliers\n- Missing values\n\nDataset: {input_data}\n\nProvide:\n- Validation status (VALID/INVALID)\n- Issues found\n- Recommended fixes\n- Cleaned data if possible\n\nFormat response as JSON with validation_status, issues, and cleaned_data fields.\n\"\"\",\n        agent_id=\"data_validator\"\n    )\n\n    # 2. Statistical Analyzer\n    stats_analyzer = graphbit.Node.agent(\n        name=\"Statistical Analyzer\",\n        prompt=\"\"\"Perform comprehensive statistical analysis on this dataset:\n\n{validated_data}\n\nCalculate and provide:\n- Descriptive statistics (mean, median, mode, std dev)\n- Distribution analysis\n- Correlation analysis\n- Trend identification\n- Statistical significance tests where applicable\n\nFormat as JSON with clear structure including summary_stats, correlations, and trends.\n\"\"\",\n        agent_id=\"stats_analyzer\"\n    )\n\n    # 3. Pattern Detector  \n    pattern_detector = graphbit.Node.agent(\n        name=\"Pattern Detector\",\n        prompt=\"\"\"Analyze this data for patterns and anomalies:\n\nStatistical Analysis: {stats_results}\nOriginal Data: {validated_data}\n\nIdentify:\n- Recurring patterns\n- Seasonal trends\n- Anomalies and outliers\n- Clustering or groupings\n- Predictive indicators\n\nExplain the significance of each finding.\nFormat as JSON with patterns, anomalies, and insights fields.\n\"\"\",\n        agent_id=\"pattern_detector\"\n    )\n\n    # 4. Insight Generator\n    insight_generator = graphbit.Node.agent(\n        name=\"Insight Generator\",\n        prompt=\"\"\"Generate actionable insights based on this analysis:\n\nStatistical Analysis: {stats_results}\nPattern Analysis: {pattern_results}\nOriginal Data: {validated_data}\n\nCreate:\n- Key business insights\n- Actionable recommendations\n- Risk assessments\n- Opportunities identified\n- Next steps\n\nFocus on practical, implementable insights.\n\"\"\",\n        agent_id=\"insight_generator\"\n    )\n\n    # 5. Report Generator\n    report_generator = graphbit.Node.agent(\n        name=\"Report Generator\",\n        prompt=\"\"\"Create a comprehensive data analysis report:\n\nData Validation: {validation_results}\nStatistical Analysis: {stats_results}\nPattern Analysis: {pattern_results}\nInsights: {insights}\n\nFormat as a professional report with:\n- Executive summary\n- Data quality assessment\n- Key findings\n- Statistical highlights\n- Actionable recommendations\n- Appendices with detailed analysis\n\nUse clear, business-friendly language.\n\"\"\",\n        agent_id=\"report_generator\"\n    )\n\n    # Add nodes to workflow\n    validator_id = workflow.add_node(validator)\n    stats_id = workflow.add_node(stats_analyzer)\n    pattern_id = workflow.add_node(pattern_detector)\n    insight_id = workflow.add_node(insight_generator)\n    report_id = workflow.add_node(report_generator)\n\n    # Connect processing pipeline\n    workflow.connect(validator_id, stats_id)\n    workflow.connect(stats_id, pattern_id)\n    workflow.connect(pattern_id, insight_id)\n    workflow.connect(insight_id, report_id)\n\n    # Validate workflow\n    workflow.validate()\n\n    return executor, workflow\n\ndef main():\n    \"\"\"Execute the data processing pipeline.\"\"\"\n\n    # Sample dataset\n    sample_data = {\n        \"sales_data\": [\n            {\"month\": \"Jan\", \"sales\": 15000, \"region\": \"North\", \"product\": \"A\"},\n            {\"month\": \"Feb\", \"sales\": 18000, \"region\": \"North\", \"product\": \"A\"},\n            {\"month\": \"Mar\", \"sales\": 12000, \"region\": \"South\", \"product\": \"B\"},\n            {\"month\": \"Apr\", \"sales\": 22000, \"region\": \"North\", \"product\": \"A\"},\n            {\"month\": \"May\", \"sales\": 19000, \"region\": \"South\", \"product\": \"B\"},\n            {\"month\": \"Jun\", \"sales\": 25000, \"region\": \"North\", \"product\": \"A\"},\n            {\"month\": \"Jul\", \"sales\": 16000, \"region\": \"East\", \"product\": \"C\"},\n            {\"month\": \"Aug\", \"sales\": 21000, \"region\": \"North\", \"product\": \"A\"},\n            {\"month\": \"Sep\", \"sales\": 14000, \"region\": \"South\", \"product\": \"B\"},\n            {\"month\": \"Oct\", \"sales\": 26000, \"region\": \"North\", \"product\": \"A\"},\n            {\"month\": \"Nov\", \"sales\": 20000, \"region\": \"East\", \"product\": \"C\"},\n            {\"month\": \"Dec\", \"sales\": 28000, \"region\": \"North\", \"product\": \"A\"}\n        ],\n        \"metadata\": {\n            \"source\": \"CRM System\",\n            \"period\": \"2024\",\n            \"currency\": \"USD\"\n        }\n    }\n\n    # Create workflow\n    executor, workflow = create_data_processing_pipeline()\n\n    # Execute workflow\n    print(\"\ud83d\ude80 Starting data processing pipeline...\")\n\n    result = executor.execute(workflow)\n\n    if result.is_success():\n        print(\"\u2705 Data processing completed successfully!\")\n        print(\"\ud83d\udcca Analysis Report:\")\n        print(\"=\" * 60)\n        print(result.get_output())\n\n        # Get execution statistics\n        stats = executor.get_stats()\n        print(f\"\\nExecution Stats:\")\n        print(f\"Total executions: {stats.get('total_executions', 0)}\")\n        print(f\"Success rate: {stats.get('successful_executions', 0)}/{stats.get('total_executions', 0)}\")\n        print(f\"Average duration: {stats.get('average_duration_ms', 0):.2f}ms\")\n    else:\n        print(\"\u274c Data processing failed:\")\n        print(result.get_error())\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/data-processing/#key-features","title":"Key Features","text":""},{"location":"examples/data-processing/#data-validation","title":"Data Validation","text":"<ul> <li>Comprehensive input validation</li> <li>Data quality assessment</li> <li>Error detection and correction</li> <li>Format standardization</li> </ul>"},{"location":"examples/data-processing/#statistical-analysis","title":"Statistical Analysis","text":"<ul> <li>Descriptive statistics</li> <li>Distribution analysis</li> <li>Correlation detection</li> <li>Trend identification</li> </ul>"},{"location":"examples/data-processing/#pattern-recognition","title":"Pattern Recognition","text":"<ul> <li>Anomaly detection</li> <li>Seasonal pattern identification</li> <li>Clustering analysis</li> <li>Predictive indicators</li> </ul> <p>This example shows how GraphBit can handle complex data processing tasks with reliability and scalability.</p>"},{"location":"examples/data-processing/#advanced-examples","title":"Advanced Examples","text":""},{"location":"examples/data-processing/#batch-data-processing","title":"Batch Data Processing","text":"<pre><code>import graphbit\nimport os\nimport asyncio\n\nasync def process_multiple_datasets_async():\n    \"\"\"Process multiple datasets asynchronously.\"\"\"\n\n    graphbit.init()\n\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    # Use high-throughput executor for batch processing\n    executor = graphbit.Executor.new_high_throughput(\n        config,\n        timeout_seconds=120,\n        debug=False\n    )\n\n    # Create simple analysis workflow\n    workflow = graphbit.Workflow(\"Batch Data Analyzer\")\n\n    analyzer = graphbit.Node.agent(\n        name=\"Batch Analyzer\",\n        prompt=\"\"\"Analyze this dataset quickly:\n\nData: {dataset}\n\nProvide:\n- Quick summary statistics\n- Key trends\n- Notable patterns\n- Recommendations\n\nKeep analysis concise but actionable.\n\"\"\",\n        agent_id=\"batch_analyzer\"\n    )\n\n    workflow.add_node(analyzer)\n    workflow.validate()\n\n    # Execute asynchronously\n    result = await executor.run_async(workflow)\n\n    if result.is_success():\n        return result.get_output()\n    else:\n        return f\"Error: {result.get_error()}\"\n\n# Usage\n# result = asyncio.run(process_multiple_datasets_async())\n</code></pre>"},{"location":"examples/data-processing/#time-series-analysis","title":"Time Series Analysis","text":"<pre><code>def create_time_series_pipeline():\n    \"\"\"Create specialized pipeline for time series data.\"\"\"\n\n    graphbit.init()\n\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    executor = graphbit.Executor(config, debug=True)\n    workflow = graphbit.Workflow(\"Time Series Analysis\")\n\n    # Trend Analyzer\n    trend_analyzer = graphbit.Node.agent(\n        name=\"Trend Analyzer\",\n        prompt=\"\"\"Analyze trends in this time series data:\n\n{time_series_data}\n\nIdentify:\n- Overall trend direction (up/down/stable)\n- Seasonality patterns\n- Cyclical behavior\n- Growth rates\n- Trend changes or breakpoints\n\nProvide quantitative analysis where possible.\n\"\"\",\n        agent_id=\"trend_analyzer\"\n    )\n\n    # Forecast Generator\n    forecaster = graphbit.Node.agent(\n        name=\"Forecaster\",\n        prompt=\"\"\"Based on this trend analysis, generate forecasts:\n\nTrend Analysis: {trend_analysis}\nHistorical Data: {time_series_data}\n\nCreate:\n- Short-term forecast (next 3 periods)\n- Medium-term forecast (next 6-12 periods)\n- Confidence intervals\n- Key assumptions\n- Risk factors\n\nExplain methodology and limitations.\n\"\"\",\n        agent_id=\"forecaster\"\n    )\n\n    trend_id = workflow.add_node(trend_analyzer)\n    forecast_id = workflow.add_node(forecaster)\n\n    workflow.connect(trend_id, forecast_id)\n    workflow.validate()\n\n    return executor, workflow\n\n# Usage\nexecutor, workflow = create_time_series_pipeline()\nresult = executor.execute(workflow)\n</code></pre>"},{"location":"examples/data-processing/#data-quality-assessment","title":"Data Quality Assessment","text":"<pre><code>def create_data_quality_pipeline():\n    \"\"\"Create pipeline focused on data quality assessment.\"\"\"\n\n    graphbit.init()\n\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    executor = graphbit.Executor(config)\n    workflow = graphbit.Workflow(\"Data Quality Assessment\")\n\n    # Completeness Checker\n    completeness_checker = graphbit.Node.agent(\n        name=\"Completeness Checker\",\n        prompt=\"\"\"Assess data completeness:\n\nDataset: {input_data}\n\nCheck for:\n- Missing values per field\n- Data coverage gaps\n- Incomplete records\n- Empty or null fields\n\nRate completeness (1-10) and provide recommendations.\n\"\"\",\n        agent_id=\"completeness_checker\"\n    )\n\n    # Consistency Checker\n    consistency_checker = graphbit.Node.agent(\n        name=\"Consistency Checker\",\n        prompt=\"\"\"Check data consistency:\n\nDataset: {input_data}\n\nExamine:\n- Format consistency\n- Value range consistency\n- Cross-field validation\n- Data type consistency\n- Naming convention adherence\n\nRate consistency (1-10) and identify issues.\n\"\"\",\n        agent_id=\"consistency_checker\"\n    )\n\n    # Accuracy Assessor\n    accuracy_assessor = graphbit.Node.agent(\n        name=\"Accuracy Assessor\",\n        prompt=\"\"\"Assess data accuracy:\n\nDataset: {input_data}\nCompleteness Report: {completeness_report}\nConsistency Report: {consistency_report}\n\nEvaluate:\n- Logical value ranges\n- Cross-validation checks\n- Outlier detection\n- Business rule compliance\n\nProvide accuracy score and recommendations.\n\"\"\",\n        agent_id=\"accuracy_assessor\"\n    )\n\n    # Quality Score Calculator\n    quality_calculator = graphbit.Node.agent(\n        name=\"Quality Calculator\",\n        prompt=\"\"\"Calculate overall data quality score:\n\nCompleteness: {completeness_report}\nConsistency: {consistency_report}\nAccuracy: {accuracy_report}\n\nProvide:\n- Overall quality score (1-10)\n- Quality grade (A-F)\n- Priority improvement areas\n- Action plan for quality improvement\n\nCreate executive summary of data quality.\n\"\"\",\n        agent_id=\"quality_calculator\"\n    )\n\n    complete_id = workflow.add_node(completeness_checker)\n    consistent_id = workflow.add_node(consistency_checker)\n    accurate_id = workflow.add_node(accuracy_assessor)\n    quality_id = workflow.add_node(quality_calculator)\n\n    # Run completeness and consistency in parallel, then accuracy, then quality\n    workflow.connect(complete_id, accurate_id)\n    workflow.connect(consistent_id, accurate_id)\n    workflow.connect(accurate_id, quality_id)\n\n    workflow.validate()\n\n    return executor, workflow\n\n# Usage\nexecutor, workflow = create_data_quality_pipeline()\nresult = executor.execute(workflow)\n</code></pre>"},{"location":"examples/data-processing/#using-alternative-llm-providers","title":"Using Alternative LLM Providers","text":""},{"location":"examples/data-processing/#anthropic-claude-for-data-analysis","title":"Anthropic Claude for Data Analysis","text":"<pre><code>def create_anthropic_data_pipeline():\n    \"\"\"Create data pipeline using Anthropic Claude.\"\"\"\n\n    graphbit.init()\n\n    config = graphbit.LlmConfig.anthropic(\n        api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n        model=\"claude-3-5-sonnet-20241022\"\n    )\n\n    executor = graphbit.Executor(config, debug=True)\n    workflow = graphbit.Workflow(\"Claude Data Analyzer\")\n\n    analyzer = graphbit.Node.agent(\n        name=\"Claude Analyzer\",\n        prompt=\"\"\"Analyze this dataset with Claude's analytical capabilities:\n\n{dataset}\n\nProvide comprehensive analysis including:\n- Statistical summary\n- Pattern recognition\n- Anomaly detection\n- Insights and recommendations\n\nUse Claude's strong reasoning for deep insights.\n\"\"\",\n        agent_id=\"claude_analyzer\"\n    )\n\n    workflow.add_node(analyzer)\n    workflow.validate()\n\n    return executor, workflow\n\n# Usage\nexecutor, workflow = create_anthropic_data_pipeline()\nresult = executor.execute(workflow)\n</code></pre>"},{"location":"examples/data-processing/#local-ollama-for-private-data","title":"Local Ollama for Private Data","text":"<pre><code>def create_ollama_data_pipeline():\n    \"\"\"Create data pipeline using local Ollama for sensitive data.\"\"\"\n\n    graphbit.init()\n\n    # No API key needed for local Ollama\n    config = graphbit.LlmConfig.ollama(\"llama3.2\")\n\n    executor = graphbit.Executor(\n        config,\n        timeout_seconds=240,  # Longer timeout for local processing\n        debug=True\n    )\n\n    workflow = graphbit.Workflow(\"Private Data Analyzer\")\n\n    analyzer = graphbit.Node.agent(\n        name=\"Local Analyzer\",\n        prompt=\"\"\"Analyze this sensitive dataset locally:\n\n{dataset}\n\nProvide:\n- Basic statistics\n- Key patterns\n- Security considerations\n- Privacy-preserving insights\n\nKeep analysis secure and local.\n\"\"\",\n        agent_id=\"local_analyzer\"\n    )\n\n    workflow.add_node(analyzer)\n    workflow.validate()\n\n    return executor, workflow\n\n# Usage\nexecutor, workflow = create_ollama_data_pipeline()\nresult = executor.execute(workflow)\n</code></pre>"},{"location":"examples/data-processing/#embeddings-for-similarity-analysis","title":"Embeddings for Similarity Analysis","text":"<pre><code>def create_embedding_analysis_pipeline():\n    \"\"\"Create pipeline using embeddings for similarity analysis.\"\"\"\n\n    graphbit.init()\n\n    # Configure embeddings\n    embedding_config = graphbit.EmbeddingConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"text-embedding-3-small\"\n    )\n\n    embedding_client = graphbit.EmbeddingClient(embedding_config)\n\n    # Sample text data\n    texts = [\n        \"Revenue increased by 15% this quarter\",\n        \"Sales performance exceeded expectations\",\n        \"Customer satisfaction scores improved\",\n        \"Market share declined in key segments\",\n        \"Operating expenses rose significantly\"\n    ]\n\n    # Generate embeddings\n    embeddings = embedding_client.embed_many(texts)\n\n    # Calculate similarities\n    similarities = []\n    for i in range(len(embeddings)):\n        for j in range(i + 1, len(embeddings)):\n            similarity = graphbit.EmbeddingClient.similarity(\n                embeddings[i], \n                embeddings[j]\n            )\n            similarities.append({\n                \"text1\": texts[i],\n                \"text2\": texts[j],\n                \"similarity\": similarity\n            })\n\n    # Find most similar pairs\n    similarities.sort(key=lambda x: x[\"similarity\"], reverse=True)\n\n    print(\"\ud83d\udd0d Most Similar Text Pairs:\")\n    for sim in similarities[:3]:\n        print(f\"Similarity: {sim['similarity']:.3f}\")\n        print(f\"Text 1: {sim['text1']}\")\n        print(f\"Text 2: {sim['text2']}\")\n        print(\"-\" * 40)\n\n# Usage\ncreate_embedding_analysis_pipeline()\n</code></pre>"},{"location":"examples/data-processing/#system-health-and-monitoring","title":"System Health and Monitoring","text":"<pre><code>def monitor_data_processing_health():\n    \"\"\"Monitor GraphBit health during data processing.\"\"\"\n\n    graphbit.init()\n\n    # Check system health\n    health = graphbit.health_check()\n    print(\"Health Check:\")\n    for key, value in health.items():\n        print(f\"  {key}: {value}\")\n\n    # Get system info\n    info = graphbit.get_system_info()\n    print(\"\\nSystem Information:\")\n    for key, value in info.items():\n        print(f\"  {key}: {value}\")\n\n    # Check version\n    version = graphbit.version()\n    print(f\"\\nVersion: {version}\")\n\n    return health[\"overall_healthy\"]\n\n# Usage\nif monitor_data_processing_health():\n    print(\"System ready for data processing\")\nelse:\n    print(\"System issues detected\")\n</code></pre>"},{"location":"examples/data-processing/#key-benefits","title":"Key Benefits","text":""},{"location":"examples/data-processing/#flexibility","title":"Flexibility","text":"<ul> <li>Multiple Analysis Types: Statistical, pattern, quality, time series</li> <li>Multiple LLM Providers: OpenAI, Anthropic, Ollama support</li> <li>Execution Modes: Sync, async, batch processing</li> </ul>"},{"location":"examples/data-processing/#reliability","title":"Reliability","text":"<ul> <li>Error Handling: Comprehensive error reporting</li> <li>Health Monitoring: System health checks</li> <li>Performance Tracking: Built-in execution statistics</li> </ul>"},{"location":"examples/data-processing/#security","title":"Security","text":"<ul> <li>Local Processing: Ollama for sensitive data</li> <li>Privacy-Preserving: Keep data processing local when needed</li> <li>Embedding Analysis: Semantic similarity without exposing content</li> </ul> <p>This example demonstrates GraphBit's capabilities for building robust, flexible data processing workflows that can handle various analysis tasks with reliability and performance optimization. </p>"},{"location":"examples/llm-integration/","title":"LLM Integration and Advanced Usage","text":"<p>This example demonstrates comprehensive LLM integration with GraphBit, showcasing various providers, execution modes, and advanced features.</p>"},{"location":"examples/llm-integration/#overview","title":"Overview","text":"<p>We'll explore: 1. Multiple LLM Providers: OpenAI, Anthropic, HuggingFace, Ollama 2. Execution Modes: Sync, async, batch, streaming 3. Performance Optimization: High-throughput, low-latency, memory-optimized 4. Error Handling: Resilience patterns and fallbacks 5. Monitoring: Performance metrics and health checks</p>"},{"location":"examples/llm-integration/#complete-llm-client-example","title":"Complete LLM Client Example","text":"<pre><code>import graphbit\nimport os\nimport asyncio\nimport time\nfrom typing import List, Dict, Optional\n\nclass AdvancedLLMSystem:\n    def __init__(self):\n        \"\"\"Initialize the advanced LLM system.\"\"\"\n        # Initialize GraphBit\n        graphbit.init(enable_tracing=True)\n\n        # Store multiple provider clients\n        self.clients = {}\n        self.initialize_providers()\n\n    def initialize_providers(self):\n        \"\"\"Initialize all available LLM providers.\"\"\"\n        print(\"Initializing LLM providers...\")\n\n        # OpenAI client\n        if os.getenv(\"OPENAI_API_KEY\"):\n            openai_config = graphbit.LlmConfig.openai(\n                api_key=os.getenv(\"OPENAI_API_KEY\"),\n                model=\"gpt-4o-mini\"\n            )\n            self.clients['openai'] = graphbit.LlmClient(openai_config, debug=True)\n            print(\"OpenAI client initialized\")\n\n        # Anthropic client\n        if os.getenv(\"ANTHROPIC_API_KEY\"):\n            anthropic_config = graphbit.LlmConfig.anthropic(\n                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n                model=\"claude-3-5-sonnet-20241022\"\n            )\n            self.clients['anthropic'] = graphbit.LlmClient(anthropic_config, debug=True)\n            print(\"Anthropic client initialized\")\n\n        # DeepSeek client\n        if os.getenv(\"DEEPSEEK_API_KEY\"):\n            deepseek_config = graphbit.LlmConfig.deepseek(\n                api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n                model=\"deepseek-chat\"\n            )\n            self.clients['deepseek'] = graphbit.LlmClient(deepseek_config, debug=True)\n            print(\"DeepSeek client initialized\")\n\n        # HuggingFace client\n        if os.getenv(\"HUGGINGFACE_API_KEY\"):\n            huggingface_config = graphbit.LlmConfig.huggingface(\n                api_key=os.getenv(\"HUGGINGFACE_API_KEY\"),\n                model=\"microsoft/DialoGPT-medium\"\n            )\n            self.clients['huggingface'] = graphbit.LlmClient(huggingface_config, debug=True)\n            print(\"HuggingFace client initialized\")\n\n        # Ollama client (no API key required)\n        try:\n            ollama_config = graphbit.LlmConfig.ollama(\"llama3.2\")\n            self.clients['ollama'] = graphbit.LlmClient(ollama_config, debug=True)\n            print(\"Ollama client initialized\")\n        except Exception as e:\n            print(f\"Ollama client failed: {e}\")\n\n        if not self.clients:\n            raise Exception(\"No LLM providers available. Please set API keys or install Ollama.\")\n\n    def test_basic_completion(self, provider: str = 'openai'):\n        \"\"\"Test basic text completion.\"\"\"\n        if provider not in self.clients:\n            print(f\"Provider '{provider}' not available\")\n            return None\n\n        client = self.clients[provider]\n        prompt = \"Explain quantum computing in simple terms.\"\n\n        print(f\"\\nTesting basic completion with {provider}...\")\n        print(f\"Prompt: {prompt}\")\n\n        try:\n            start_time = time.time()\n            response = client.complete(\n                prompt=prompt,\n                max_tokens=200,\n                temperature=0.7\n            )\n            duration = (time.time() - start_time) * 1000\n\n            print(f\"Completed in {duration:.2f}ms\")\n            print(f\"Response: {response[:200]}...\")\n\n            return response\n        except Exception as e:\n            print(f\"Completion failed: {e}\")\n            return None\n\n    async def test_async_completion(self, provider: str = 'openai'):\n        \"\"\"Test asynchronous completion.\"\"\"\n        if provider not in self.clients:\n            print(f\"Provider '{provider}' not available\")\n            return None\n\n        client = self.clients[provider]\n        prompt = \"Write a haiku about artificial intelligence.\"\n\n        print(f\"\\nTesting async completion with {provider}...\")\n        print(f\"Prompt: {prompt}\")\n\n        try:\n            start_time = time.time()\n            response = await client.complete_async(\n                prompt=prompt,\n                max_tokens=100,\n                temperature=0.8\n            )\n            duration = (time.time() - start_time) * 1000\n\n            print(f\"Async completed in {duration:.2f}ms\")\n            print(f\"Response: {response}\")\n\n            return response\n        except Exception as e:\n            print(f\"Async completion failed: {e}\")\n            return None\n\n    async def test_batch_completion(self, provider: str = 'openai'):\n        \"\"\"Test batch completion for multiple prompts.\"\"\"\n        if provider not in self.clients:\n            print(f\"Provider '{provider}' not available\")\n            return None\n\n        client = self.clients[provider]\n        prompts = [\n            \"What is machine learning?\",\n            \"Explain neural networks briefly.\",\n            \"What are the benefits of cloud computing?\",\n            \"How does blockchain work?\",\n            \"What is the future of AI?\"\n        ]\n\n        print(f\"\\nTesting batch completion with {provider}...\")\n        print(f\"Processing {len(prompts)} prompts...\")\n\n        try:\n            start_time = time.time()\n            responses = await client.complete_batch(\n                prompts=prompts,\n                max_tokens=100,\n                temperature=0.6,\n                max_concurrency=3\n            )\n            duration = (time.time() - start_time) * 1000\n\n            print(f\"Batch completed in {duration:.2f}ms\")\n            print(f\"Average per prompt: {duration/len(prompts):.2f}ms\")\n\n            for i, (prompt, response) in enumerate(zip(prompts, responses)):\n                print(f\"\\n{i+1}. {prompt}\")\n                print(f\"   \u2192 {response[:100]}...\")\n\n            return responses\n        except Exception as e:\n            print(f\"Batch completion failed: {e}\")\n            return None\n\n    async def test_chat_optimized(self, provider: str = 'openai'):\n        \"\"\"Test optimized chat completion.\"\"\"\n        if provider not in self.clients:\n            print(f\"Provider '{provider}' not available\")\n            return None\n\n        client = self.clients[provider]\n        messages = [\n            (\"system\", \"You are a helpful AI assistant specialized in technology.\"),\n            (\"user\", \"What's the difference between AI and ML?\"),\n            (\"assistant\", \"AI is the broader concept of machines being able to carry out tasks in a smart way, while ML is a specific subset of AI that involves training algorithms on data.\"),\n            (\"user\", \"Can you give me a practical example?\")\n        ]\n\n        print(f\"\\nTesting chat-optimized completion with {provider}...\")\n\n        try:\n            start_time = time.time()\n            response = await client.chat_optimized(\n                messages=messages,\n                max_tokens=150,\n                temperature=0.7\n            )\n            duration = (time.time() - start_time) * 1000\n\n            print(f\"Chat completed in {duration:.2f}ms\")\n            print(f\"Response: {response}\")\n\n            return response\n        except Exception as e:\n            print(f\"Chat completion failed: {e}\")\n            return None\n\n    async def test_streaming_completion(self, provider: str = 'openai'):\n        \"\"\"Test streaming completion.\"\"\"\n        if provider not in self.clients:\n            print(f\"Provider '{provider}' not available\")\n            return None\n\n        client = self.clients[provider]\n        prompt = \"Write a detailed explanation of how machine learning works, covering the key concepts step by step.\"\n\n        print(f\"\\nTesting streaming completion with {provider}...\")\n        print(f\"Prompt: {prompt}\")\n        print(\"Streaming response:\")\n\n        try:\n            start_time = time.time()\n\n            # Note: Streaming returns an async iterator\n            stream = await client.complete_stream(\n                prompt=prompt,\n                max_tokens=300,\n                temperature=0.7\n            )\n\n            full_response = \"\"\n            async for chunk in stream:\n                print(chunk, end='', flush=True)\n                full_response += chunk\n\n            duration = (time.time() - start_time) * 1000\n            print(f\"\\n\\nStreaming completed in {duration:.2f}ms\")\n            print(f\"Total tokens: ~{len(full_response.split())}\")\n\n            return full_response\n        except Exception as e:\n            print(f\"Streaming completion failed: {e}\")\n            return None\n\n    def test_client_warmup(self, provider: str = 'openai'):\n        \"\"\"Test client warmup for improved performance.\"\"\"\n        if provider not in self.clients:\n            print(f\"Provider '{provider}' not available\")\n            return\n\n        client = self.clients[provider]\n\n        print(f\"\\nTesting client warmup with {provider}...\")\n\n        try:\n            # Warmup the client\n            start_time = time.time()\n            asyncio.run(client.warmup())\n            warmup_duration = (time.time() - start_time) * 1000\n\n            print(f\"Warmup completed in {warmup_duration:.2f}ms\")\n\n            # Test performance after warmup\n            start_time = time.time()\n            response = client.complete(\"Quick test after warmup\", max_tokens=50)\n            completion_duration = (time.time() - start_time) * 1000\n\n            print(f\"Post-warmup completion: {completion_duration:.2f}ms\")\n\n        except Exception as e:\n            print(f\"Warmup failed: {e}\")\n\n    def get_client_statistics(self, provider: str = 'openai'):\n        \"\"\"Get detailed client statistics.\"\"\"\n        if provider not in self.clients:\n            print(f\"Provider '{provider}' not available\")\n            return None\n\n        client = self.clients[provider]\n\n        print(f\"\\nGetting statistics for {provider}...\")\n\n        try:\n            stats = client.get_stats()\n\n            print(f\"Client Statistics for {provider}:\")\n            for key, value in stats.items():\n                if isinstance(value, float):\n                    print(f\"  {key}: {value:.3f}\")\n                else:\n                    print(f\"  {key}: {value}\")\n\n            return stats\n        except Exception as e:\n            print(f\"Failed to get statistics: {e}\")\n            return None\n\n    def reset_client_statistics(self, provider: str = 'openai'):\n        \"\"\"Reset client statistics.\"\"\"\n        if provider not in self.clients:\n            print(f\"Provider '{provider}' not available\")\n            return\n\n        client = self.clients[provider]\n\n        try:\n            client.reset_stats()\n            print(f\"Statistics reset for {provider}\")\n        except Exception as e:\n            print(f\"Failed to reset statistics: {e}\")\n\n    def compare_providers(self, prompt: str = \"Explain the concept of recursion in programming.\"):\n        \"\"\"Compare responses from all available providers.\"\"\"\n        print(f\"\\nComparing providers...\")\n        print(f\"Prompt: {prompt}\")\n\n        results = {}\n\n        for provider_name, client in self.clients.items():\n            print(f\"\\n--- Testing {provider_name} ---\")\n            try:\n                start_time = time.time()\n                response = client.complete(\n                    prompt=prompt,\n                    max_tokens=150,\n                    temperature=0.7\n                )\n                duration = (time.time() - start_time) * 1000\n\n                results[provider_name] = {\n                    'response': response,\n                    'duration_ms': duration,\n                    'success': True\n                }\n\n                print(f\"{provider_name}: {duration:.2f}ms\")\n                print(f\"Response: {response[:100]}...\")\n\n            except Exception as e:\n                results[provider_name] = {\n                    'error': str(e),\n                    'success': False\n                }\n                print(f\"{provider_name}: {e}\")\n\n        return results\n\n# Performance-optimized clients\ndef create_performance_optimized_clients():\n    \"\"\"Create clients optimized for different performance characteristics.\"\"\"\n\n    graphbit.init()\n\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"OpenAI API key required for performance tests\")\n        return None\n\n    # High-throughput client\n    high_throughput_config = graphbit.LlmConfig.openai(api_key, \"gpt-4o-mini\")\n    high_throughput_client = graphbit.LlmClient(high_throughput_config, debug=False)\n\n    # Low-latency client  \n    low_latency_config = graphbit.LlmConfig.openai(api_key, \"gpt-4o-mini\")\n    low_latency_client = graphbit.LlmClient(low_latency_config, debug=False)\n\n    return {\n        'high_throughput': high_throughput_client,\n        'low_latency': low_latency_client\n    }\n\nasync def performance_benchmark():\n    \"\"\"Benchmark different client configurations.\"\"\"\n\n    clients = create_performance_optimized_clients()\n    if not clients:\n        return\n\n    test_prompt = \"Summarize the benefits of renewable energy in one paragraph.\"\n\n    print(\"\\n\ud83c\udfc3 Performance Benchmark\")\n    print(\"=\" * 50)\n\n    for config_name, client in clients.items():\n        print(f\"\\nTesting {config_name} configuration...\")\n\n        # Single completion test\n        start_time = time.time()\n        try:\n            response = client.complete(test_prompt, max_tokens=100)\n            single_duration = (time.time() - start_time) * 1000\n            print(f\"Single completion: {single_duration:.2f}ms\")\n        except Exception as e:\n            print(f\"Single completion failed: {e}\")\n            continue\n\n        # Batch test\n        batch_prompts = [test_prompt] * 5\n        start_time = time.time()\n        try:\n            batch_responses = await client.complete_batch(\n                batch_prompts,\n                max_tokens=100,\n                max_concurrency=3\n            )\n            batch_duration = (time.time() - start_time) * 1000\n            avg_per_prompt = batch_duration / len(batch_prompts)\n            print(f\"Batch completion (5 prompts): {batch_duration:.2f}ms total, {avg_per_prompt:.2f}ms avg\")\n        except Exception as e:\n            print(f\"Batch completion failed: {e}\")\n\n        # Get statistics\n        try:\n            stats = client.get_stats()\n            print(f\"Stats: {stats.get('total_requests', 0)} requests, \"\n                  f\"{stats.get('average_response_time_ms', 0):.2f}ms avg response time\")\n        except:\n            pass\n\n# Error handling and resilience examples\nasync def test_error_handling():\n    \"\"\"Test error handling and resilience features.\"\"\"\n\n    graphbit.init()\n\n    # Test with invalid API key\n    print(\"\\nTesting Error Handling\")\n    print(\"=\" * 40)\n\n    try:\n        invalid_config = graphbit.LlmConfig.openai(\"invalid-key\", \"gpt-4o-mini\")\n        invalid_client = graphbit.LlmClient(invalid_config)\n\n        print(\"Testing with invalid API key...\")\n        response = invalid_client.complete(\"Test prompt\", max_tokens=50)\n        print(\"Expected error but got response\")\n    except Exception as e:\n        print(f\"Correctly handled invalid API key: {type(e).__name__}\")\n\n    # Test timeout handling\n    if os.getenv(\"OPENAI_API_KEY\"):\n        try:\n            config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n            client = graphbit.LlmClient(config)\n\n            print(\"\\nTesting very long prompt (potential timeout)...\")\n            very_long_prompt = \"Write a comprehensive essay about \" + \"technology \" * 1000\n\n            response = client.complete(very_long_prompt, max_tokens=2000)\n            print(\"Long prompt handled successfully\")\n        except Exception as e:\n            print(f\"Timeout/limit handled: {type(e).__name__}\")\n\n# System health monitoring\ndef monitor_llm_system_health():\n    \"\"\"Monitor LLM system health and performance.\"\"\"\n\n    graphbit.init()\n\n    print(\"\\nSystem Health Check\")\n    print(\"=\" * 40)\n\n    # Check GraphBit health\n    health = graphbit.health_check()\n    print(\"GraphBit Health:\")\n    for key, value in health.items():\n        status = \"Ok!\" if value else \"Not Ok!\"\n        print(f\"  {status} {key}: {value}\")\n\n    # Get system information\n    info = graphbit.get_system_info()\n    print(f\"\\nSystem Information:\")\n    print(f\"  Version: {info.get('version', 'unknown')}\")\n    print(f\"  Runtime threads: {info.get('runtime_worker_threads', 'unknown')}\")\n    print(f\"  Memory allocator: {info.get('memory_allocator', 'unknown')}\")\n\n    # Test provider connectivity\n    print(f\"\\nProvider Connectivity:\")\n\n    providers_to_test = [\n        ('OpenAI', lambda: graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\", \"test\"), \"gpt-4o-mini\")),\n        ('Anthropic', lambda: graphbit.LlmConfig.anthropic(os.getenv(\"ANTHROPIC_API_KEY\", \"test\"), \"claude-3-5-sonnet-20241022\")),\n        ('Ollama', lambda: graphbit.LlmConfig.ollama(\"llama3.2\"))\n    ]\n\n    for provider_name, config_func in providers_to_test:\n        try:\n            config = config_func()\n            client = graphbit.LlmClient(config)\n            print(f\"  {provider_name}: Configuration valid\")\n        except Exception as e:\n            print(f\"  {provider_name}: {str(e)[:50]}...\")\n\n# Example usage\nasync def main():\n    \"\"\"Run comprehensive LLM system demonstration.\"\"\"\n\n    print(\"GraphBit LLM Integration Demo\")\n    print(\"=\" * 60)\n\n    try:\n        # Initialize system\n        llm_system = AdvancedLLMSystem()\n\n        # Test basic functionality\n        for provider in llm_system.clients.keys():\n            llm_system.test_basic_completion(provider)\n            await llm_system.test_async_completion(provider)\n            break  # Just test first available provider for demo\n\n        # Test advanced features with primary provider\n        primary_provider = list(llm_system.clients.keys())[0]\n\n        await llm_system.test_batch_completion(primary_provider)\n        await llm_system.test_chat_optimized(primary_provider)\n\n        # Test performance features\n        llm_system.test_client_warmup(primary_provider)\n        llm_system.get_client_statistics(primary_provider)\n\n        # Compare providers if multiple available\n        if len(llm_system.clients) &gt; 1:\n            llm_system.compare_providers()\n\n        # Performance benchmark\n        await performance_benchmark()\n\n        # Error handling tests\n        await test_error_handling()\n\n        # System health check\n        monitor_llm_system_health()\n\n        print(\"\\nDemo completed successfully!\")\n\n    except Exception as e:\n        print(f\"\\nDemo failed: {e}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/llm-integration/#simplified-examples","title":"Simplified Examples","text":""},{"location":"examples/llm-integration/#quick-openai-integration","title":"Quick OpenAI Integration","text":"<pre><code>import graphbit\nimport os\n\ndef quick_openai_example():\n    \"\"\"Simple OpenAI integration example.\"\"\"\n\n    # Initialize\n    graphbit.init()\n\n    # Configure and create client\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n    client = graphbit.LlmClient(config)\n\n    # Simple completion\n    response = client.complete(\n        \"Explain quantum computing in 3 sentences.\",\n        max_tokens=100,\n        temperature=0.7\n    )\n\n    print(f\"Response: {response}\")\n\n    # Get statistics\n    stats = client.get_stats()\n    print(f\"Total requests: {stats.get('total_requests', 0)}\")\n\n# Usage\nquick_openai_example()\n</code></pre>"},{"location":"examples/llm-integration/#local-ollama-integration","title":"Local Ollama Integration","text":"<pre><code>import graphbit\n\ndef quick_ollama_example():\n    \"\"\"Simple Ollama integration example.\"\"\"\n\n    # Initialize\n    graphbit.init()\n\n    # Configure Ollama (no API key needed)\n    config = graphbit.LlmConfig.ollama(\"llama3.2\")\n    client = graphbit.LlmClient(config, debug=True)\n\n    # Test completion\n    try:\n        response = client.complete(\n            \"What are the benefits of local AI models?\",\n            max_tokens=150,\n            temperature=0.8\n        )\n        print(f\"Ollama response: {response}\")\n    except Exception as e:\n        print(f\"Ollama error (make sure Ollama is running): {e}\")\n\n# Usage\nquick_ollama_example()\n</code></pre>"},{"location":"examples/llm-integration/#anthropic-claude-integration","title":"Anthropic Claude Integration","text":"<pre><code>import graphbit\nimport os\n\ndef quick_anthropic_example():\n    \"\"\"Simple Anthropic Claude integration example.\"\"\"\n\n    # Initialize\n    graphbit.init()\n\n    # Configure Anthropic\n    config = graphbit.LlmConfig.anthropic(\n        api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n        model=\"claude-3-5-sonnet-20241022\"\n    )\n    client = graphbit.LlmClient(config)\n\n    # Complex reasoning task\n    response = client.complete(\n        \"\"\"Analyze the pros and cons of remote work from both \n        employee and employer perspectives. Be balanced and thorough.\"\"\",\n        max_tokens=300,\n        temperature=0.6\n    )\n\n    print(f\"Claude's analysis: {response}\")\n\n# Usage (requires ANTHROPIC_API_KEY)\n# quick_anthropic_example()\n</code></pre>"},{"location":"examples/llm-integration/#best-practices","title":"Best Practices","text":""},{"location":"examples/llm-integration/#configuration-management","title":"Configuration Management","text":"<pre><code>def setup_production_llm_config():\n    \"\"\"Set up production-ready LLM configuration.\"\"\"\n\n    graphbit.init(log_level=\"warn\", enable_tracing=False)\n\n    # Primary provider with fallback\n    providers = []\n\n    if os.getenv(\"OPENAI_API_KEY\"):\n        providers.append(('openai', graphbit.LlmConfig.openai(\n            os.getenv(\"OPENAI_API_KEY\"),\n            \"gpt-4o-mini\"\n        )))\n\n    if os.getenv(\"ANTHROPIC_API_KEY\"):\n        providers.append(('anthropic', graphbit.LlmConfig.anthropic(\n            os.getenv(\"ANTHROPIC_API_KEY\"),\n            \"claude-3-5-sonnet-20241022\"\n        )))\n\n    # Add local fallback\n    try:\n        providers.append(('ollama', graphbit.LlmConfig.ollama(\"llama3.2\")))\n    except:\n        pass\n\n    if not providers:\n        raise Exception(\"No LLM providers configured\")\n\n    return providers\n\ndef robust_completion(prompt: str, max_retries: int = 3):\n    \"\"\"Completion with provider fallback.\"\"\"\n\n    providers = setup_production_llm_config()\n\n    for provider_name, config in providers:\n        for attempt in range(max_retries):\n            try:\n                client = graphbit.LlmClient(config, debug=False)\n                return client.complete(prompt, max_tokens=200)\n            except Exception as e:\n                print(f\"Attempt {attempt + 1} with {provider_name} failed: {e}\")\n                if attempt == max_retries - 1:\n                    continue  # Try next provider\n                time.sleep(2 ** attempt)  # Exponential backoff\n\n    raise Exception(\"All providers failed\")\n</code></pre>"},{"location":"examples/llm-integration/#key-features","title":"Key Features","text":""},{"location":"examples/llm-integration/#provider-flexibility","title":"Provider Flexibility","text":"<ul> <li>Multiple Providers: OpenAI, Anthropic, Ollama support</li> <li>Easy Switching: Consistent API across providers</li> <li>Fallback Support: Automatic provider failover</li> </ul>"},{"location":"examples/llm-integration/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Async Operations: Non-blocking completions</li> <li>Batch Processing: Efficient multiple prompt handling</li> <li>Streaming: Real-time response streaming</li> <li>Client Warmup: Improved initial response times</li> </ul>"},{"location":"examples/llm-integration/#monitoring-and-reliability","title":"Monitoring and Reliability","text":"<ul> <li>Statistics Tracking: Detailed performance metrics</li> <li>Health Checks: System health monitoring</li> <li>Error Handling: Comprehensive error management</li> <li>Resilience Patterns: Circuit breakers and retry logic</li> </ul> <p>This example demonstrates GraphBit's comprehensive LLM integration capabilities for building production-ready AI applications. </p>"},{"location":"examples/semantic-search/","title":"Semantic Search and Analysis","text":"<p>This example demonstrates how to build a semantic search and analysis system using GraphBit's embedding capabilities and LLM integration.</p>"},{"location":"examples/semantic-search/#overview","title":"Overview","text":"<p>We'll create a system that: 1. Embeds text documents for semantic search 2. Searches for semantically similar content 3. Analyzes results with LLM insights 4. Compares multiple documents for similarity 5. Generates intelligent summaries</p>"},{"location":"examples/semantic-search/#complete-example","title":"Complete Example","text":"<pre><code>import graphbit\nimport os\nimport json\nfrom typing import List, Dict, Tuple\n\nclass SemanticSearchSystem:\n    def __init__(self, openai_api_key: str):\n        \"\"\"Initialize the semantic search system.\"\"\"\n        # Initialize GraphBit\n        graphbit.init(enable_tracing=True)\n\n        # Configure embeddings\n        self.embedding_config = graphbit.EmbeddingConfig.openai(\n            api_key=openai_api_key,\n            model=\"text-embedding-3-small\"\n        )\n        self.embedding_client = graphbit.EmbeddingClient(self.embedding_config)\n\n        # Configure LLM for analysis\n        self.llm_config = graphbit.LlmConfig.openai(\n            api_key=openai_api_key,\n            model=\"gpt-4o-mini\"\n        )\n        self.llm_client = graphbit.LlmClient(self.llm_config)\n\n        # Document storage\n        self.documents = []\n        self.embeddings = []\n        self.document_index = {}\n\n    def add_documents(self, documents: List[Dict[str, str]]):\n        \"\"\"Add documents to the search index.\"\"\"\n        print(f\"Adding {len(documents)} documents to index...\")\n\n        # Extract text for embedding\n        texts = [doc['content'] for doc in documents]\n\n        # Generate embeddings in batch\n        embeddings = self.embedding_client.embed_many(texts)\n\n        # Store documents and embeddings\n        start_idx = len(self.documents)\n        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n            doc_id = start_idx + i\n            self.documents.append(doc)\n            self.embeddings.append(embedding)\n            self.document_index[doc_id] = {\n                'title': doc.get('title', f'Document {doc_id}'),\n                'content_preview': doc['content'][:200] + '...',\n                'metadata': doc.get('metadata', {})\n            }\n\n        print(f\"Added {len(documents)} documents. Total: {len(self.documents)}\")\n\n    def search(self, query: str, top_k: int = 5) -&gt; List[Dict]:\n        \"\"\"Search for semantically similar documents.\"\"\"\n        if not self.documents:\n            print(\"No documents in index\")\n            return []\n\n        print(f\"\ud83d\udd0d Searching for: '{query}'\")\n\n        # Embed the query\n        query_embedding = self.embedding_client.embed(query)\n\n        # Calculate similarities\n        similarities = []\n        for i, doc_embedding in enumerate(self.embeddings):\n            similarity = graphbit.EmbeddingClient.similarity(\n                query_embedding, \n                doc_embedding\n            )\n            similarities.append({\n                'doc_id': i,\n                'similarity': similarity,\n                'title': self.document_index[i]['title'],\n                'content_preview': self.document_index[i]['content_preview'],\n                'metadata': self.document_index[i]['metadata']\n            })\n\n        # Sort by similarity and return top-k\n        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n        return similarities[:top_k]\n\n    def analyze_search_results(self, query: str, results: List[Dict]) -&gt; str:\n        \"\"\"Analyze search results with LLM insights.\"\"\"\n        if not results:\n            return \"No results to analyze.\"\n\n        print(\"\ud83e\udd16 Analyzing search results with LLM...\")\n\n        # Prepare context for LLM\n        results_text = \"\\n\\n\".join([\n            f\"Document {i+1}: {result['title']}\\n\"\n            f\"Similarity: {result['similarity']:.3f}\\n\"\n            f\"Preview: {result['content_preview']}\"\n            for i, result in enumerate(results)\n        ])\n\n        prompt = f\"\"\"Analyze these search results for the query: \"{query}\"\n\nSearch Results:\n{results_text}\n\nProvide:\n1. Summary of what the results reveal about the query\n2. Key themes and patterns across the documents\n3. Quality assessment of the search matches\n4. Recommendations for refining the search or exploring related topics\n5. Most relevant documents and why\n\nBe insightful and practical in your analysis.\n\"\"\"\n\n        try:\n            analysis = self.llm_client.complete(prompt)\n            return analysis\n        except Exception as e:\n            return f\"Analysis failed: {str(e)}\"\n\n    def compare_documents(self, doc_ids: List[int]) -&gt; Dict:\n        \"\"\"Compare multiple documents for similarity.\"\"\"\n        if len(doc_ids) &lt; 2:\n            return {\"error\": \"Need at least 2 documents to compare\"}\n\n        print(f\"Comparing {len(doc_ids)} documents...\")\n\n        # Get embeddings for specified documents\n        selected_embeddings = [self.embeddings[doc_id] for doc_id in doc_ids]\n        selected_docs = [self.documents[doc_id] for doc_id in doc_ids]\n\n        # Calculate pairwise similarities\n        comparisons = []\n        for i in range(len(doc_ids)):\n            for j in range(i + 1, len(doc_ids)):\n                similarity = graphbit.EmbeddingClient.similarity(\n                    selected_embeddings[i],\n                    selected_embeddings[j]\n                )\n                comparisons.append({\n                    'doc1_id': doc_ids[i],\n                    'doc2_id': doc_ids[j],\n                    'doc1_title': self.document_index[doc_ids[i]]['title'],\n                    'doc2_title': self.document_index[doc_ids[j]]['title'],\n                    'similarity': similarity\n                })\n\n        # Sort by similarity\n        comparisons.sort(key=lambda x: x['similarity'], reverse=True)\n\n        return {\n            'comparisons': comparisons,\n            'most_similar': comparisons[0] if comparisons else None,\n            'least_similar': comparisons[-1] if comparisons else None,\n            'average_similarity': sum(c['similarity'] for c in comparisons) / len(comparisons) if comparisons else 0\n        }\n\n    def generate_document_summary(self, doc_id: int) -&gt; str:\n        \"\"\"Generate an intelligent summary of a document.\"\"\"\n        if doc_id &gt;= len(self.documents):\n            return \"Document not found\"\n\n        document = self.documents[doc_id]\n        print(f\"\ud83d\udcdd Generating summary for: {self.document_index[doc_id]['title']}\")\n\n        prompt = f\"\"\"Summarize this document concisely:\n\nTitle: {document.get('title', 'Untitled')}\nContent: {document['content']}\n\nProvide:\n1. Main topics and themes\n2. Key insights or findings\n3. Important details or data points\n4. Practical implications or takeaways\n\nKeep the summary informative but concise (2-3 paragraphs).\n\"\"\"\n\n        try:\n            summary = self.llm_client.complete(prompt, max_tokens=500)\n            return summary\n        except Exception as e:\n            return f\"Summary generation failed: {str(e)}\"\n\n    def get_statistics(self) -&gt; Dict:\n        \"\"\"Get system statistics.\"\"\"\n        if not self.documents:\n            return {\"documents\": 0, \"embeddings\": 0}\n\n        # Calculate average similarity across all documents\n        all_similarities = []\n        for i in range(len(self.embeddings)):\n            for j in range(i + 1, len(self.embeddings)):\n                similarity = graphbit.EmbeddingClient.similarity(\n                    self.embeddings[i],\n                    self.embeddings[j]\n                )\n                all_similarities.append(similarity)\n\n        return {\n            \"total_documents\": len(self.documents),\n            \"total_embeddings\": len(self.embeddings),\n            \"average_document_similarity\": sum(all_similarities) / len(all_similarities) if all_similarities else 0,\n            \"max_similarity\": max(all_similarities) if all_similarities else 0,\n            \"min_similarity\": min(all_similarities) if all_similarities else 0\n        }\n\n# Example usage\ndef main():\n    \"\"\"Demonstrate the semantic search system.\"\"\"\n\n    # Set up API key\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"Please set OPENAI_API_KEY environment variable\")\n        return\n\n    # Create search system\n    search_system = SemanticSearchSystem(api_key)\n\n    # Sample documents\n    sample_documents = [\n        {\n            \"title\": \"Introduction to Machine Learning\",\n            \"content\": \"\"\"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can automatically learn patterns from data and make predictions or decisions. The field encompasses supervised learning, unsupervised learning, and reinforcement learning approaches.\"\"\",\n            \"metadata\": {\"category\": \"technology\", \"difficulty\": \"beginner\"}\n        },\n        {\n            \"title\": \"Deep Learning Neural Networks\",\n            \"content\": \"\"\"Deep learning uses artificial neural networks with multiple layers to model and understand complex patterns in data. These networks can automatically learn hierarchical representations of data, making them particularly effective for tasks like image recognition, natural language processing, and speech recognition. Popular architectures include convolutional neural networks and recurrent neural networks.\"\"\",\n            \"metadata\": {\"category\": \"technology\", \"difficulty\": \"advanced\"}\n        },\n        {\n            \"title\": \"Sustainable Energy Solutions\",\n            \"content\": \"\"\"Renewable energy sources like solar, wind, and hydroelectric power are becoming increasingly important for environmental sustainability. These technologies offer clean alternatives to fossil fuels and can help reduce carbon emissions. Energy storage systems and smart grid technologies are crucial for integrating renewable energy into existing power infrastructure.\"\"\",\n            \"metadata\": {\"category\": \"environment\", \"difficulty\": \"intermediate\"}\n        },\n        {\n            \"title\": \"Climate Change Impacts\",\n            \"content\": \"\"\"Climate change is causing significant environmental disruptions including rising sea levels, extreme weather events, and ecosystem changes. The scientific consensus indicates human activities, particularly greenhouse gas emissions, are the primary drivers. Mitigation strategies include transitioning to renewable energy, improving energy efficiency, and implementing carbon capture technologies.\"\"\",\n            \"metadata\": {\"category\": \"environment\", \"difficulty\": \"intermediate\"}\n        },\n        {\n            \"title\": \"Digital Marketing Strategies\",\n            \"content\": \"\"\"Modern digital marketing encompasses social media marketing, search engine optimization, content marketing, and data analytics. Successful campaigns require understanding customer behavior, creating engaging content, and leveraging multiple digital channels. Personalization and automation tools are increasingly important for reaching target audiences effectively.\"\"\",\n            \"metadata\": {\"category\": \"business\", \"difficulty\": \"beginner\"}\n        }\n    ]\n\n    # Add documents to index\n    search_system.add_documents(sample_documents)\n\n    # Perform searches\n    queries = [\n        \"artificial intelligence and neural networks\",\n        \"renewable energy and sustainability\",\n        \"online marketing and social media\"\n    ]\n\n    for query in queries:\n        print(f\"\\n{'='*60}\")\n        print(f\"Query: {query}\")\n        print('='*60)\n\n        # Search for similar documents\n        results = search_system.search(query, top_k=3)\n\n        # Display results\n        print(\"\\n\ud83d\udd0d Search Results:\")\n        for i, result in enumerate(results, 1):\n            print(f\"{i}. {result['title']} (Similarity: {result['similarity']:.3f})\")\n            print(f\"   {result['content_preview']}\")\n            print()\n\n        # Analyze results with LLM\n        analysis = search_system.analyze_search_results(query, results)\n        print(\"LLM Analysis:\")\n        print(analysis)\n        print()\n\n    # Compare documents\n    print(f\"\\n{'='*60}\")\n    print(\"Document Comparison\")\n    print('='*60)\n    comparison = search_system.compare_documents([0, 1, 2])\n\n    print(\"Document Similarities:\")\n    for comp in comparison['comparisons'][:3]:\n        print(f\"{comp['doc1_title']} &lt;-&gt; {comp['doc2_title']}: {comp['similarity']:.3f}\")\n\n    # Generate summaries\n    print(f\"\\n{'='*60}\")\n    print(\"Document Summaries\")\n    print('='*60)\n\n    for i in range(min(2, len(sample_documents))):\n        summary = search_system.generate_document_summary(i)\n        print(f\"\\n\ud83d\udcdd Summary of '{search_system.document_index[i]['title']}':\")\n        print(summary)\n\n    # Show statistics\n    print(f\"\\n{'='*60}\")\n    print(\"System Statistics\")\n    print('='*60)\n    stats = search_system.get_statistics()\n    for key, value in stats.items():\n        if isinstance(value, float):\n            print(f\"{key}: {value:.3f}\")\n        else:\n            print(f\"{key}: {value}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/semantic-search/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/semantic-search/#batch-processing-with-async-operations","title":"Batch Processing with Async Operations","text":"<pre><code>import asyncio\nimport graphbit\nimport os\n\nasync def process_large_document_collection():\n    \"\"\"Process large document collections asynchronously.\"\"\"\n\n    graphbit.init()\n\n    # Configure for high-throughput processing\n    llm_config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    llm_client = graphbit.LlmClient(llm_config, debug=False)\n\n    # Large collection of documents (simulated)\n    documents = [f\"Document {i} content about various topics...\" for i in range(100)]\n\n    # Process in batches\n    batch_size = 10\n    results = []\n\n    for i in range(0, len(documents), batch_size):\n        batch = documents[i:i + batch_size]\n        print(f\"Processing batch {i//batch_size + 1}...\")\n\n        # Use batch completion for efficiency\n        prompts = [f\"Summarize this document: {doc}\" for doc in batch]\n\n        try:\n            batch_results = await llm_client.complete_batch(\n                prompts,\n                max_tokens=200,\n                temperature=0.3,\n                max_concurrency=5\n            )\n            results.extend(batch_results)\n            print(f\"Completed batch {i//batch_size + 1}\")\n        except Exception as e:\n            print(f\"Batch {i//batch_size + 1} failed: {e}\")\n\n    return results\n\n# Usage\n# results = asyncio.run(process_large_document_collection())\n</code></pre>"},{"location":"examples/semantic-search/#multi-provider-search-system","title":"Multi-Provider Search System","text":"<pre><code>def create_multi_provider_system():\n    \"\"\"Create search system with multiple LLM providers.\"\"\"\n\n    graphbit.init()\n\n    # OpenAI for embeddings\n    embedding_config = graphbit.EmbeddingConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"text-embedding-3-small\"\n    )\n    embedding_client = graphbit.EmbeddingClient(embedding_config)\n\n    # Multiple LLM providers for analysis\n    providers = {\n        'openai': graphbit.LlmClient(\n            graphbit.LlmConfig.openai(\n                api_key=os.getenv(\"OPENAI_API_KEY\"),\n                model=\"gpt-4o-mini\"\n            )\n        ),\n        'anthropic': graphbit.LlmClient(\n            graphbit.LlmConfig.anthropic(\n                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n                model=\"claude-3-5-sonnet-20241022\"\n            )\n        ) if os.getenv(\"ANTHROPIC_API_KEY\") else None,\n        'ollama': graphbit.LlmClient(\n            graphbit.LlmConfig.ollama(\"llama3.2\")\n        )\n    }\n\n    # Filter available providers\n    available_providers = {k: v for k, v in providers.items() if v is not None}\n\n    def analyze_with_multiple_providers(query: str, results: List[Dict]) -&gt; Dict[str, str]:\n        \"\"\"Get analysis from multiple LLM providers.\"\"\"\n        analyses = {}\n\n        prompt = f\"Analyze these search results for '{query}': {results}\"\n\n        for provider_name, client in available_providers.items():\n            try:\n                print(f\"\ud83e\udd16 Getting analysis from {provider_name}...\")\n                analysis = client.complete(prompt, max_tokens=300)\n                analyses[provider_name] = analysis\n            except Exception as e:\n                analyses[provider_name] = f\"Error: {str(e)}\"\n\n        return analyses\n\n    return embedding_client, analyze_with_multiple_providers\n\n# Usage\nembedding_client, analyzer = create_multi_provider_system()\n</code></pre>"},{"location":"examples/semantic-search/#workflow-based-semantic-analysis","title":"Workflow-Based Semantic Analysis","text":"<pre><code>def create_semantic_analysis_workflow():\n    \"\"\"Create comprehensive semantic analysis workflow.\"\"\"\n\n    graphbit.init()\n\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    executor = graphbit.Executor(config, debug=True)\n    workflow = graphbit.Workflow(\"Semantic Analysis Pipeline\")\n\n    # Document Preprocessor\n    preprocessor = graphbit.Node.agent(\n        name=\"Document Preprocessor\",\n        prompt=\"\"\"Preprocess this document for semantic analysis:\n\n{document}\n\nTasks:\n- Extract key topics and themes\n- Identify important entities (people, places, concepts)\n- Determine document type and structure\n- Note any special formatting or data\n\nProvide structured output for further analysis.\n\"\"\",\n        agent_id=\"preprocessor\"\n    )\n\n    # Semantic Analyzer\n    analyzer = graphbit.Node.agent(\n        name=\"Semantic Analyzer\",\n        prompt=\"\"\"Perform semantic analysis on this preprocessed document:\n\n{preprocessed_document}\n\nAnalyze:\n- Semantic relationships between concepts\n- Document sentiment and tone\n- Key insights and findings\n- Conceptual density and complexity\n- Domain-specific terminology\n\nProvide detailed semantic breakdown.\n\"\"\",\n        agent_id=\"semantic_analyzer\"\n    )\n\n    # Insight Generator\n    insight_generator = graphbit.Node.agent(\n        name=\"Insight Generator\",\n        prompt=\"\"\"Generate actionable insights from this semantic analysis:\n\n{semantic_analysis}\n\nCreate:\n- Summary of key findings\n- Practical implications\n- Related topics for exploration\n- Recommendations for further analysis\n- Quality assessment of the content\n\nFocus on useful, actionable insights.\n\"\"\",\n        agent_id=\"insight_generator\"\n    )\n\n    # Add nodes and connect\n    prep_id = workflow.add_node(preprocessor)\n    analyze_id = workflow.add_node(analyzer)\n    insight_id = workflow.add_node(insight_generator)\n\n    workflow.connect(prep_id, analyze_id)\n    workflow.connect(analyze_id, insight_id)\n\n    workflow.validate()\n\n    return executor, workflow\n\n# Usage\nexecutor, workflow = create_semantic_analysis_workflow()\nresult = executor.execute(workflow)\n</code></pre>"},{"location":"examples/semantic-search/#huggingface-integration","title":"HuggingFace Integration","text":"<pre><code>def create_huggingface_search_system():\n    \"\"\"Create search system using HuggingFace embeddings.\"\"\"\n\n    graphbit.init()\n\n    # Configure HuggingFace embeddings\n    embedding_config = graphbit.EmbeddingConfig.huggingface(\n        api_key=os.getenv(\"HUGGINGFACE_API_KEY\"),\n        model=\"sentence-transformers/all-MiniLM-L6-v2\"\n    )\n\n    embedding_client = graphbit.EmbeddingClient(embedding_config)\n\n    # Test embedding generation\n    test_texts = [\n        \"Machine learning algorithms\",\n        \"Natural language processing\",\n        \"Computer vision applications\"\n    ]\n\n    try:\n        embeddings = embedding_client.embed_many(test_texts)\n        print(f\"Generated {len(embeddings)} embeddings with HuggingFace\")\n\n        # Calculate similarities\n        for i in range(len(embeddings)):\n            for j in range(i + 1, len(embeddings)):\n                similarity = graphbit.EmbeddingClient.similarity(\n                    embeddings[i], \n                    embeddings[j]\n                )\n                print(f\"'{test_texts[i]}' &lt;-&gt; '{test_texts[j]}': {similarity:.3f}\")\n\n    except Exception as e:\n        print(f\"HuggingFace embedding failed: {e}\")\n\n    return embedding_client\n\n# Usage (requires HUGGINGFACE_API_KEY)\n# client = create_huggingface_search_system()\n</code></pre>"},{"location":"examples/semantic-search/#system-monitoring-and-health","title":"System Monitoring and Health","text":"<pre><code>def monitor_semantic_search_system():\n    \"\"\"Monitor system health and performance.\"\"\"\n\n    graphbit.init()\n\n    # Check system health\n    health = graphbit.health_check()\n    print(\"System Health:\")\n    for key, value in health.items():\n        status = \"Ok!\" if value else \"Not Ok!\"\n        print(f\"  {status} {key}: {value}\")\n\n    # Get system information\n    info = graphbit.get_system_info()\n    print(\"\\nSystem Information:\")\n    for key, value in info.items():\n        print(f\"  {key}: {value}\")\n\n    # Test embedding client performance\n    embedding_config = graphbit.EmbeddingConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"text-embedding-3-small\"\n    )\n\n    try:\n        client = graphbit.EmbeddingClient(embedding_config)\n\n        # Performance test\n        import time\n        start_time = time.time()\n\n        test_embedding = client.embed(\"Performance test text\")\n\n        end_time = time.time()\n        duration = (end_time - start_time) * 1000\n\n        print(f\"\\n\u26a1 Performance Test:\")\n        print(f\"  Embedding generation: {duration:.2f}ms\")\n        print(f\"  Embedding dimension: {len(test_embedding)}\")\n\n        return True\n\n    except Exception as e:\n        print(f\"\\nPerformance test failed: {e}\")\n        return False\n\n# Usage\nsystem_healthy = monitor_semantic_search_system()\n</code></pre>"},{"location":"examples/semantic-search/#key-benefits","title":"Key Benefits","text":""},{"location":"examples/semantic-search/#semantic-understanding","title":"Semantic Understanding","text":"<ul> <li>Deep Search: Beyond keyword matching to semantic similarity</li> <li>Context Awareness: Understanding document relationships and themes</li> <li>Intelligent Analysis: LLM-powered insights and recommendations</li> </ul>"},{"location":"examples/semantic-search/#scalability","title":"Scalability","text":"<ul> <li>Batch Processing: Efficient handling of large document collections</li> <li>Async Operations: Non-blocking processing for better performance</li> <li>Multiple Providers: Flexibility to use different LLM providers</li> </ul>"},{"location":"examples/semantic-search/#flexibility","title":"Flexibility","text":"<ul> <li>Multi-Provider Support: OpenAI, Anthropic, Ollama, HuggingFace</li> <li>Workflow Integration: Combine with GraphBit's workflow system</li> <li>Custom Analysis: Tailored semantic analysis pipelines</li> </ul> <p>This example demonstrates how GraphBit's embedding capabilities can be combined with LLM analysis to create powerful semantic search and analysis systems. </p>"},{"location":"getting-started/examples/","title":"Basic Examples","text":"<p>This guide provides simple, practical examples to help you get started with GraphBit quickly.</p>"},{"location":"getting-started/examples/#example-1-simple-text-analysis","title":"Example 1: Simple Text Analysis","text":"<p>Analyze text content and extract key insights:</p> <pre><code>import graphbit\nimport os\n\n# Initialize\ngraphbit.init()\nconfig = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n\n# Create workflow\nworkflow = graphbit.Workflow(\"Text Analysis\")\nanalyzer = graphbit.Node.agent(\n    name=\"Text Analyzer\",\n    prompt=\"Analyze this text and provide 3 key insights: {input}\",\n    agent_id=\"analyzer\"\n)\n\nworkflow.add_node(analyzer)\n\n# Execute\nexecutor = graphbit.Executor(config)\nresult = executor.execute(workflow)\n\nprint(f\"Analysis: {result.get_variable('output')}\")\n</code></pre>"},{"location":"getting-started/examples/#example-2-sequential-pipeline","title":"Example 2: Sequential Pipeline","text":"<p>Create a multi-step content processing pipeline:</p> <pre><code>import graphbit\nimport os\n\ndef create_content_pipeline():\n    graphbit.init()\n    config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n\n    workflow = graphbit.Workflow(\"Content Pipeline\")\n\n    # Step 1: Research\n    researcher = graphbit.Node.agent(\n        name=\"Researcher\",\n        prompt=\"Research 3 key facts about: {topic}\",\n        agent_id=\"researcher\"\n    )\n\n    # Step 2: Writer\n    writer = graphbit.Node.agent(\n        name=\"Writer\",\n        prompt=\"Write a paragraph about {topic} using these facts: {research_output}\",\n        agent_id=\"writer\"\n    )\n\n    # Connect nodes\n    research_id = workflow.add_node(researcher)\n    writer_id = workflow.add_node(writer)\n    workflow.connect(research_id, writer_id)\n\n    executor = graphbit.Executor(config)\n\n    return executor.execute(workflow)\n\n# Usage\nresult = create_content_pipeline()\nprint(f\"Final content: {result.get_variable('output')}\")\n</code></pre>"},{"location":"getting-started/examples/#example-3-conditional-workflow","title":"Example 3: Conditional Workflow","text":"<p>Use condition nodes for branching logic:</p> <pre><code>import graphbit\nimport os\n\ndef create_quality_check_workflow():\n    graphbit.init()\n    config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n\n    workflow = graphbit.Workflow(\"Quality Check\")\n\n    # Analyzer\n    analyzer = graphbit.Node.agent(\n        name=\"Quality Checker\",\n        prompt=\"Rate this content quality from 1-10: {input}\",\n        agent_id=\"checker\"\n    )\n\n    # Condition\n    quality_gate = graphbit.Node.condition(\n        name=\"Quality Gate\",\n        expression=\"quality_score &gt; 7\"\n    )\n\n    # Approver\n    approver = graphbit.Node.agent(\n        name=\"Content Approver\",\n        prompt=\"Approve this high-quality content: {input}\",\n        agent_id=\"approver\"\n    )\n\n    # Connect with conditional flow\n    check_id = workflow.add_node(analyzer)\n    gate_id = workflow.add_node(quality_gate)\n    approve_id = workflow.add_node(approver)\n\n    workflow.connect(check_id, gate_id)\n    workflow.connect(gate_id, approve_id)\n\n    executor = graphbit.Executor(config)\n\n    return executor.execute(workflow)\n</code></pre>"},{"location":"getting-started/examples/#example-4-data-transformation","title":"Example 4: Data Transformation","text":"<p>Transform data between workflow steps:</p> <pre><code>import graphbit\nimport os\n\ndef create_transform_workflow():\n    graphbit.init()\n    config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n\n    workflow = graphbit.Workflow(\"Data Transform\")\n\n    # Generator\n    generator = graphbit.Node.agent(\n        name=\"Data Generator\",\n        prompt=\"Generate a JSON object with name, age, and city for a person\",\n        agent_id=\"generator\"\n    )\n\n    # Transformer\n    transformer = graphbit.Node.transform(\n        name=\"JSON Extractor\",\n        transformation=\"json_extract\"\n    )\n\n    # Processor\n    processor = graphbit.Node.agent(\n        name=\"Data Processor\",\n        prompt=\"Summarize this person's data: {extracted_data}\",\n        agent_id=\"processor\"\n    )\n\n    # Connect\n    gen_id = workflow.add_node(generator)\n    trans_id = workflow.add_node(transformer)\n    proc_id = workflow.add_node(processor)\n\n    workflow.connect(gen_id, trans_id)\n    workflow.connect(trans_id, proc_id)\n\n    executor = graphbit.Executor(config)\n\n    return executor.execute(workflow)\n</code></pre>"},{"location":"getting-started/examples/#example-5-multiple-llm-providers","title":"Example 5: Multiple LLM Providers","text":"<p>Use different LLM providers in the same workflow:</p> <pre><code>import graphbit\nimport os\n\ndef multi_provider_example():\n    graphbit.init()\n\n    # OpenAI for creative tasks\n    openai_config = graphbit.LlmConfig.openai(\n        os.getenv(\"OPENAI_API_KEY\"), \n        \"gpt-4o-mini\"\n    )\n\n    # Anthropic for analytical tasks\n    anthropic_config = graphbit.LlmConfig.anthropic(\n        os.getenv(\"ANTHROPIC_API_KEY\"),\n        \"claude-3-5-sonnet-20241022\"\n    )\n\n    # Ollama for local execution\n    ollama_config = graphbit.LlmConfig.ollama(\"llama3.2\")\n\n    # Create separate executors for different providers\n    creative_executor = graphbit.Executor(openai_config)\n    analytical_executor = graphbit.Executor(anthropic_config)\n    local_executor = graphbit.Executor(ollama_config)\n\n    # Creative workflow\n    creative_workflow = graphbit.Workflow(\"Creative Writing\")\n    writer = graphbit.Node.agent(\n        name=\"Creative Writer\",\n        prompt=\"Write a creative story about: {topic}\",\n        agent_id=\"creative_writer\"\n    )\n\n    creative_workflow.add_node(writer)\n\n    # Analytical workflow\n    analytical_workflow = graphbit.Workflow(\"Analysis\")\n    analyzer = graphbit.Node.agent(\n        name=\"Data Analyzer\",\n        prompt=\"Analyze this data and provide insights: {data}\",\n        agent_id=\"analyzer\"\n    )\n\n    analytical_workflow.add_node(analyzer)\n\n    # Execute with different providers\n    creative_result = creative_executor.execute(creative_workflow)\n    analytical_result = analytical_executor.execute(analytical_workflow)\n\n    print(f\"Creative (OpenAI): {creative_result.get_variable('output')}\")\n    print(f\"Analytical (Anthropic): {analytical_result.get_variable('output')}\")\n\n    return creative_result, analytical_result\n</code></pre>"},{"location":"getting-started/examples/#example-6-error-handling-and-performance-optimization","title":"Example 6: Error Handling and Performance Optimization","text":"<p>Build robust workflows with error handling and optimized performance:</p> <pre><code>import graphbit\nimport os\n\ndef robust_workflow_example():\n    graphbit.init()\n    config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n\n    # Create high-throughput executor with timeout\n    executor = graphbit.Executor.new_high_throughput(config, timeout_seconds=120)\n\n    # Alternative: Create low-latency executor for real-time\n    # executor = graphbit.Executor.new_low_latency(config, timeout_seconds=30)\n\n    # Alternative: Create memory-optimized executor\n    # executor = graphbit.Executor.new_memory_optimized(config)\n\n    workflow = graphbit.Workflow(\"Robust Workflow\")\n\n    agent = graphbit.Node.agent(\n        name=\"Reliable Agent\",\n        prompt=\"Process this data reliably: {input}\",\n        agent_id=\"reliable\"\n    )\n\n    workflow.add_node(agent)\n\n    try:\n        result = executor.execute(workflow)\n        if result.is_success():\n            print(f\"Success: {result.get_variable('output')}\")\n            print(f\"Execution time: {result.execution_time_ms()}ms\")\n            print(f\"Execution mode: {executor.get_execution_mode()}\")\n        else:\n            print(f\"Workflow failed: {result.state()}\")\n\n    except Exception as e:\n        print(f\"Exception: {e}\")\n\n# Run the example\nrobust_workflow_example()\n</code></pre>"},{"location":"getting-started/examples/#example-7-embeddings-and-similarity","title":"Example 7: Embeddings and Similarity","text":"<p>Use embeddings for semantic search and similarity:</p> <pre><code>import graphbit\nimport os\n\ndef embeddings_example():\n    graphbit.init()\n\n    # Create embedding service\n    embedding_config = graphbit.EmbeddingConfig.openai(\n        os.getenv(\"OPENAI_API_KEY\"), \n        \"text-embedding-3-small\"\n    )\n\n    service = graphbit.EmbeddingClient(embedding_config)\n\n    # Process multiple texts\n    texts = [\n        \"Machine learning is revolutionizing technology\",\n        \"AI is transforming how we work and live\",\n        \"The weather is nice today\"\n    ]\n\n    print(\"Generating embeddings...\")\n\n    # Note: Actual embedding implementation depends on the binding\n    # This is a conceptual example\n    for i, text in enumerate(texts):\n        print(f\"Text {i+1}: {text}\")\n\n    print(\"Embeddings generated successfully\")\n\n# Run async example\nembeddings_example()\n</code></pre>"},{"location":"getting-started/examples/#example-8-system-monitoring-and-diagnostics","title":"Example 8: System Monitoring and Diagnostics","text":"<p>Monitor GraphBit performance and health:</p> <pre><code>import graphbit\nimport os\n\ndef system_monitoring_example():\n    graphbit.init()\n\n    # Get system information\n    system_info = graphbit.get_system_info()\n    print(\"\ud83d\udda5\ufe0f System Information:\")\n    print(f\"   GraphBit version: {system_info['version']}\")\n    print(f\"   Python binding version: {system_info['python_binding_version']}\")\n    print(f\"   CPU count: {system_info['cpu_count']}\")\n    print(f\"   Memory allocator: {system_info['memory_allocator']}\")\n    print(f\"   Runtime initialized: {system_info['runtime_initialized']}\")\n\n    # Perform health check\n    health = graphbit.health_check()\n    print(\"\\nHealth Check:\")\n    print(f\"   Overall healthy: {health['overall_healthy']}\")\n    print(f\"   Runtime healthy: {health['runtime_healthy']}\")\n    print(f\"   Memory healthy: {health['memory_healthy']}\")\n    print(f\"   Available memory: {health.get('available_memory_mb', 'N/A')} MB\")\n\n    # Create and monitor an executor\n    config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n    executor = graphbit.Executor(config, debug=True)  # Enable debugging\n\n    # Get executor statistics\n    stats = executor.get_stats()\n    print(f\"\\nExecutor Statistics:\")\n    print(f\"   Execution mode: {executor.get_execution_mode()}\")\n    print(f\"   Lightweight mode: {executor.is_lightweight_mode()}\")\n\n    # Execute a simple workflow to generate stats\n    workflow = graphbit.Workflow(\"Monitoring Test\")\n    agent = graphbit.Node.agent(\n        name=\"Monitor Agent\",\n        prompt=\"Say hello: {input}\",\n        agent_id=\"monitor\"\n    )\n    workflow.add_node(agent)\n\n    result = executor.execute(workflow)\n\n    # Get updated statistics\n    updated_stats = executor.get_stats()\n    print(f\"\\nUpdated Statistics:\")\n    for key, value in updated_stats.items():\n        print(f\"   {key}: {value}\")\n\n# Run monitoring example\nif __name__ == \"__main__\":\n    system_monitoring_example()\n</code></pre>"},{"location":"getting-started/examples/#example-9-workflow-validation","title":"Example 9: Workflow Validation","text":"<p>Validate workflow structure before execution:</p> <pre><code>import graphbit\nimport os\n\ndef workflow_validation_example():\n    graphbit.init()\n    config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n\n    # Create a workflow\n    workflow = graphbit.Workflow(\"Validation Test\")\n\n    # Add nodes\n    node1 = graphbit.Node.agent(\n        name=\"First Agent\",\n        prompt=\"Process input: {input}\",\n        agent_id=\"agent1\"\n    )\n\n    node2 = graphbit.Node.agent(\n        name=\"Second Agent\", \n        prompt=\"Continue processing: {previous_output}\",\n        agent_id=\"agent2\"\n    )\n\n    id1 = workflow.add_node(node1)\n    id2 = workflow.add_node(node2)\n    workflow.connect(id1, id2)\n\n    # Validate workflow structure\n    try:\n        workflow.validate()\n        print(\"Workflow validation passed\")\n\n        # Execute the validated workflow\n        executor = graphbit.Executor(config)\n        result = executor.execute(workflow)\n        print(f\"Execution completed: {result.get_variable('output')}\")\n\n    except Exception as e:\n        print(f\"Workflow validation failed: {e}\")\n\n# Run validation example\nworkflow_validation_example()\n</code></pre>"},{"location":"getting-started/examples/#tips-for-getting-started","title":"Tips for Getting Started","text":"<ol> <li>Start Simple: Begin with single-node workflows</li> <li>Test Incrementally: Add complexity gradually</li> <li>Handle Errors: Always include error handling with try-catch blocks</li> <li>Monitor Performance: Use <code>result.execution_time_ms()</code> to track execution times</li> <li>Validate Workflows: Call <code>workflow.validate()</code> before execution</li> <li>Use Templates: Create reusable workflow patterns</li> <li>Choose Appropriate Executors: Use <code>new_high_throughput()</code>, <code>new_low_latency()</code>, or <code>new_memory_optimized()</code> based on your needs</li> <li>Check System Health: Use <code>graphbit.health_check()</code> for diagnostics</li> </ol>"},{"location":"getting-started/examples/#next-steps","title":"Next Steps","text":"<p>Once you're comfortable with these examples:</p> <ul> <li>Explore Core Concepts</li> <li>Learn about Advanced Features</li> <li>Check out Complete Examples</li> <li>Read the API Reference </li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will help you install GraphBit on your system and set up your development environment.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.10 or higher (&lt; 3.13)</li> <li>Operating System: Linux, macOS, or Windows</li> <li>Memory: 4GB RAM minimum, 8GB recommended for high-throughput workloads</li> <li>Storage: 1GB free space</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-install-from-pypi-recommended","title":"Method 1: Install from PyPI (Recommended)","text":"<p>The easiest way to install GraphBit is using pip:</p> <pre><code>pip install graphbit\n</code></pre>"},{"location":"getting-started/installation/#method-2-install-from-source","title":"Method 2: Install from Source","text":"<p>For development or the latest features:</p> <pre><code># Clone the repository\ngit clone https://github.com/InfinitiBit/graphbit.git\ncd graphbit\n\n# Install Rust (if not already installed)\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource ~/.cargo/env\n\n# Set environment variable for compilation\nunset ARGV0\n\n# Build and install Python bindings\ncd python\nmaturin develop\n</code></pre>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/installation/#1-api-keys-configuration","title":"1. API Keys Configuration","text":"<p>GraphBit supports multiple LLM providers. Set up API keys for your preferred providers:</p> <pre><code># OpenAI (required for most examples)\nexport OPENAI_API_KEY=\"sk-your-openai-api-key-here\"\n\n# Anthropic (optional)\nexport ANTHROPIC_API_KEY=\"sk-your-anthropic-api-key-here\"\n\n# HuggingFace (optional)\nexport HUGGINGFACE_API_KEY=\"hf-your-huggingface-token-here\"\n</code></pre> <p>\u26a0\ufe0f Security Note: Never commit API keys to version control. Use environment variables or secure secret management.</p>"},{"location":"getting-started/installation/#2-environment-file-recommended","title":"2. Environment File (Recommended)","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># Copy the example environment file\ncp .env.example .env\n\n# Edit with your API keys\nnano .env\n</code></pre> <p>Example <code>.env</code> file:</p> <pre><code>OPENAI_API_KEY=sk-your-openai-api-key-here\nANTHROPIC_API_KEY=sk-your-anthropic-api-key-here\nHUGGINGFACE_API_KEY=hf-your-huggingface-token-here\n</code></pre>"},{"location":"getting-started/installation/#3-local-llm-setup-optional","title":"3. Local LLM Setup (Optional)","text":"<p>To use local models with Ollama:</p> <pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Start Ollama server\nollama serve\n\n# Pull a model (in another terminal)\nollama pull llama3.2\nollama pull phi3\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Test your installation with this simple script:</p> <pre><code>import graphbit\nimport os\n\n# Initialize GraphBit\ngraphbit.init()\n\n# Test basic functionality\nprint(f\"GraphBit version: {graphbit.version()}\")\n\n# Get system information\nsystem_info = graphbit.get_system_info()\nprint(f\"Python binding version: {system_info['python_binding_version']}\")\nprint(f\"Runtime initialized: {system_info['runtime_initialized']}\")\n\n# Perform health check\nhealth = graphbit.health_check()\nprint(f\"System healthy: {health['overall_healthy']}\")\n\n# Test LLM configuration (requires API key)\nif os.getenv(\"OPENAI_API_KEY\"):\n    config = graphbit.LlmConfig.openai(\n        os.getenv(\"OPENAI_API_KEY\"), \n        \"gpt-4o-mini\"\n    )\n    print(f\"LLM Provider: {config.provider()}\")\n    print(f\"Model: {config.model()}\")\n    print(\"Installation successful!\")\nelse:\n    print(\"No OPENAI_API_KEY found - set up API keys to use LLM features\")\n</code></pre> <p>Save this as <code>test_installation.py</code> and run:</p> <pre><code>python test_installation.py\n</code></pre> <p>Expected output:</p> <pre><code>GraphBit version: [version]\nPython binding version: [version]\nRuntime initialized: True\nSystem healthy: True\nLLM Provider: openai\nModel: gpt-4o-mini\nInstallation successful!\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributors and advanced users:</p> <pre><code># Clone and setup development environment\ngit clone https://github.com/InfinitiBit/graphbit.git\ncd graphbit\n\n# Install development dependencies\nmake dev-setup\n\n# Install pre-commit hooks\nmake pre-commit-install\n\n# Build Python bindings in development mode\ncd python\nmaturin develop\n\n# Run tests to verify setup\ncd ..\nmake test\n</code></pre>"},{"location":"getting-started/installation/#docker-installation-alternative","title":"Docker Installation (Alternative)","text":"<p>Run GraphBit in a containerized environment:</p> <pre><code># Pull the official image\ndocker pull graphbit/graphbit:latest\n\n# Run with environment variables\ndocker run -e OPENAI_API_KEY=$OPENAI_API_KEY \\\n           -v $(pwd):/workspace \\\n           graphbit/graphbit:latest\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#1-import-error","title":"1. Import Error","text":"<pre><code>ImportError: No module named 'graphbit'\n</code></pre> <p>Solution: Ensure you're using the correct Python environment and GraphBit is installed:</p> <pre><code>pip list | grep graphbit\npip install --upgrade graphbit\n</code></pre>"},{"location":"getting-started/installation/#2-rust-compilation-errors","title":"2. Rust Compilation Errors","text":"<pre><code>error: Microsoft Visual C++ 14.0 is required (Windows)\n</code></pre> <p>Solution: Install Microsoft C++ Build Tools or Visual Studio with C++ support.</p> <p>For Linux/macOS compilation issues:</p> <pre><code># Ensure ARGV0 is unset for proper compilation\nunset ARGV0\n\n# Make sure Rust is properly installed\nrustc --version\n</code></pre>"},{"location":"getting-started/installation/#3-api-key-issues","title":"3. API Key Issues","text":"<pre><code>Authentication failed\n</code></pre> <p>Solution: Verify your API keys are correctly set:</p> <pre><code>echo $OPENAI_API_KEY\n</code></pre>"},{"location":"getting-started/installation/#4-runtime-initialization-errors","title":"4. Runtime Initialization Errors","text":"<pre><code>Failed to initialize GraphBit runtime\n</code></pre> <p>Solution: Check system health and reinitialize:</p> <pre><code>import graphbit\ngraphbit.init(debug=True)  # Enable debug logging\nhealth = graphbit.health_check()\nprint(health)\n</code></pre>"},{"location":"getting-started/installation/#5-permission-errors-linuxmacos","title":"5. Permission Errors (Linux/macOS)","text":"<pre><code># If you get permission errors, try:\npip install --user graphbit\n\n# Or use virtual environment (recommended)\npython -m venv graphbit-env\nsource graphbit-env/bin/activate  # Linux/macOS\n# graphbit-env\\Scripts\\activate   # Windows\npip install graphbit\n</code></pre>"},{"location":"getting-started/installation/#6-memory-issues","title":"6. Memory Issues","text":"<p>If you encounter memory-related errors, use memory-optimized executor:</p> <pre><code>import graphbit\nconfig = graphbit.LlmConfig.openai(api_key, \"gpt-4o-mini\")\nexecutor = graphbit.Executor.new_memory_optimized(config)\n</code></pre>"},{"location":"getting-started/installation/#performance-optimization","title":"Performance Optimization","text":"<p>For optimal performance:</p> <ol> <li>Choose the right executor:</li> <li><code>Executor.new_high_throughput()</code> for batch processing</li> <li><code>Executor.new_low_latency()</code> for real-time applications</li> <li> <p><code>Executor.new_memory_optimized()</code> for resource-constrained environments</p> </li> <li> <p>Monitor system health:    <code>python    health = graphbit.health_check()    if not health['overall_healthy']:        print(\"System issues detected\")</code></p> </li> <li> <p>Configure runtime for your workload:    <code>python    # Before calling graphbit.init()    graphbit.configure_runtime(        worker_threads=8,        max_blocking_threads=16    )    graphbit.init()</code></p> </li> </ol>"},{"location":"getting-started/installation/#get-help","title":"Get Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the FAQ</li> <li>Search GitHub Issues</li> <li>Create a new issue with:</li> <li>Your operating system and Python version</li> <li>Complete error message</li> <li>Steps to reproduce</li> <li>Output of <code>graphbit.get_system_info()</code> and <code>graphbit.health_check()</code></li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installed, proceed to the Quick Start Tutorial to build your first AI workflow!</p>"},{"location":"getting-started/installation/#update-graphbit","title":"Update GraphBit","text":"<p>Keep GraphBit updated for the latest features and bug fixes:</p> <pre><code># Update from PyPI\npip install --upgrade graphbit\n\n# Update from source\ncd graphbit\ngit pull origin main\ncd python\nmaturin develop\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start Tutorial","text":"<p>Welcome to GraphBit! This tutorial will guide you through creating your first AI agent workflow in just 5 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have: - GraphBit installed (Installation Guide) - An OpenAI API key set in your environment - Python 3.10+ available</p>"},{"location":"getting-started/quickstart/#your-first-workflow","title":"Your First Workflow","text":"<p>Let's create a simple content analysis workflow that analyzes text and provides insights.</p>"},{"location":"getting-started/quickstart/#step-1-basic-setup","title":"Step 1: Basic Setup","text":"<p>Create a new Python file <code>my_first_workflow.py</code>:</p> <pre><code>import graphbit\nimport os\n\n# Initialize GraphBit\ngraphbit.init()\n\n# Configure LLM (using OpenAI GPT-4)\n# GraphBit supports multiple providers: openai, anthropic, huggingface, ollama\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n\nconfig = graphbit.LlmConfig.openai(api_key, \"gpt-4o-mini\")\n\n# Alternative configurations:\n# config = graphbit.LlmConfig.anthropic(os.getenv(\"ANTHROPIC_API_KEY\"), \"claude-3-5-sonnet-20241022\")\n# config = graphbit.LlmConfig.huggingface(os.getenv(\"HUGGINGFACE_API_KEY\"), \"microsoft/DialoGPT-medium\")\n# config = graphbit.LlmConfig.ollama(\"llama3.2\")  # Local model, no API key needed\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-create-your-first-agent-node","title":"Step 2: Create Your First Agent Node","text":"<pre><code># Create a workflow\nworkflow = graphbit.Workflow(\"Content Analysis Pipeline\")\n\n# Create an analyzer agent\nanalyzer = graphbit.Node.agent(\n    name=\"Content Analyzer\",\n    prompt=\"Analyze the following content and provide key insights: {input}\",\n    agent_id=\"analyzer\"\n)\n\n# Add the node to the workflow\nanalyzer_id = workflow.add_node(analyzer)\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-build-and-execute-the-workflow","title":"Step 3: Build and Execute the Workflow","text":"<pre><code># Create executor with basic configuration\nexecutor = graphbit.Executor(config)\n\n# Execute the workflow\nprint(\"\ud83d\ude80 Executing workflow...\")\nresult = executor.execute(workflow)\n\n# Display results\nprint(f\"\u2705 Workflow completed in {result.execution_time_ms()}ms\")\nprint(f\"\ud83d\udcca Result: {result.get_variable('output')}\")\n</code></pre>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<p>Here's the complete working example:</p> <pre><code>import graphbit\nimport os\n\ndef main():\n    # Initialize GraphBit\n    graphbit.init()\n\n    # Configure LLM\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        print(\"Please set OPENAI_API_KEY environment variable\")\n        return\n\n    config = graphbit.LlmConfig.openai(api_key, \"gpt-4o-mini\")\n\n    # Build workflow\n    workflow = graphbit.Workflow(\"Content Analysis Pipeline\")\n\n    analyzer = graphbit.Node.agent(\n        name=\"Content Analyzer\",\n        prompt=\"Analyze this content and provide 3 key insights: {input}\",\n        agent_id=\"analyzer\"\n    )\n\n    analyzer_id = workflow.add_node(analyzer)\n\n    # Execute workflow\n    executor = graphbit.Executor(config)\n\n    print(\"Analyzing content...\")\n    result = executor.execute(workflow)\n\n    print(f\"Analysis completed in {result.execution_time_ms()}ms\")\n    print(f\"Insights:\\n{result.get_variable('output')}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"getting-started/quickstart/#run-your-workflow","title":"Run Your Workflow","text":"<pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\npython my_first_workflow.py\n</code></pre> <p>Expected output:</p> <pre><code>Analyzing content...\nAnalysis completed in 2347ms\nInsights:\nBased on the analysis, here are 3 key insights:\n1. [AI-generated insight about the content]\n2. [Another relevant insight]\n3. [Additional analysis point]\n</code></pre>"},{"location":"getting-started/quickstart/#multi-step-workflows","title":"Multi-Step Workflows","text":"<p>Let's create a more complex workflow with multiple connected nodes:</p> <pre><code>import graphbit\nimport os\n\ndef create_content_pipeline():\n    graphbit.init()\n    config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n\n    # Build multi-step workflow\n    workflow = graphbit.Workflow(\"Content Creation Pipeline\")\n\n    # Step 1: Research Agent\n    researcher = graphbit.Node.agent(\n        name=\"Researcher\",\n        prompt=\"Research key points about: {topic}. Provide 5 important facts.\",\n        agent_id=\"researcher\"\n    )\n\n    # Step 2: Writer Agent  \n    writer = graphbit.Node.agent(\n        name=\"Content Writer\", \n        prompt=\"Write a 200-word article about {topic} using this research: {research_data}\",\n        agent_id=\"writer\"\n    )\n\n    # Step 3: Editor Agent\n    editor = graphbit.Node.agent(\n        name=\"Editor\",\n        prompt=\"Edit and improve this article for clarity and engagement: {draft_content}\",\n        agent_id=\"editor\"\n    )\n\n    # Add nodes and create connections\n    research_id = workflow.add_node(researcher)\n    writer_id = workflow.add_node(writer)\n    editor_id = workflow.add_node(editor)\n\n    # Connect the workflow: Research \u2192 Write \u2192 Edit\n    workflow.connect(research_id, writer_id)\n    workflow.connect(writer_id, editor_id)\n\n    # Build and execute\n    executor = graphbit.Executor(config)\n\n    print(\"Executing content creation pipeline...\")\n    result = executor.execute(workflow)\n\n    print(f\"Pipeline completed in {result.execution_time_ms()}ms\")\n    print(f\"Final article:\\n{result.get_variable('output')}\")\n\nif __name__ == \"__main__\":\n    create_content_pipeline()\n</code></pre>"},{"location":"getting-started/quickstart/#adding-reliability-features","title":"Adding Reliability Features","text":"<p>Enhance your workflow with error handling and retries:</p> <pre><code>import graphbit\nimport os\n\ndef reliable_workflow():\n    graphbit.init()\n    config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n\n    # Create executor with reliability features\n    executor = graphbit.Executor(config, timeout_seconds=60)\n\n    # Build workflow\n    workflow = graphbit.Workflow(\"Reliable Analysis\")\n\n    analyzer = graphbit.Node.agent(\n        name=\"Robust Analyzer\",\n        prompt=\"Provide detailed analysis of: {input}\",\n        agent_id=\"robust_analyzer\"\n    )\n\n    analyzer_id = workflow.add_node(analyzer)\n\n    # Execute with error handling\n    try:\n        print(\"\ud83d\udee1\ufe0f Executing reliable workflow...\")\n        result = executor.execute(workflow)\n        print(f\"Success! Completed in {result.execution_time_ms()}ms\")\n        print(f\"Result: {result.get_variable('output')}\")\n\n    except Exception as e:\n        print(f\"Workflow failed: {e}\")\n\nif __name__ == \"__main__\":\n    reliable_workflow()\n</code></pre>"},{"location":"getting-started/quickstart/#working-with-different-llm-providers","title":"Working with Different LLM Providers","text":"<p>GraphBit supports multiple LLM providers:</p> <pre><code>import graphbit\nimport os\n\n# OpenAI Configuration\nopenai_config = graphbit.LlmConfig.openai(\n    os.getenv(\"OPENAI_API_KEY\"), \n    \"gpt-4o-mini\"\n)\n\n# Anthropic Configuration\nanthropic_config = graphbit.LlmConfig.anthropic(\n    os.getenv(\"ANTHROPIC_API_KEY\"),\n    \"claude-3-5-sonnet-20241022\"\n)\n\n# Ollama Configuration (local)\nollama_config = graphbit.LlmConfig.ollama(\"llama3.2\")\n\n# Use any configuration with the same workflow\ndef run_with_provider(config):\n    workflow = graphbit.Workflow(\"Multi-Provider Test\")\n\n    agent = graphbit.Node.agent(\n        name=\"Test Agent\",\n        prompt=\"Say hello and identify yourself: {input}\",\n        agent_id=\"test_agent\"\n    )\n\n    workflow.add_node(agent)\n\n    executor = graphbit.Executor(config)\n    result = executor.execute(workflow)\n\n    print(f\"Provider: {config.provider()}\")\n    print(f\"Model: {config.model()}\")\n    print(f\"Response: {result.get_variable('output')}\")\n    print(\"---\")\n\n# Test different providers\nif __name__ == \"__main__\":\n    graphbit.init()\n\n    if os.getenv(\"OPENAI_API_KEY\"):\n        run_with_provider(openai_config)\n\n    if os.getenv(\"ANTHROPIC_API_KEY\"):\n        run_with_provider(anthropic_config)\n\n    # Ollama works without API key\n    run_with_provider(ollama_config)\n</code></pre>"},{"location":"getting-started/quickstart/#performance-optimization","title":"Performance Optimization","text":"<p>Create optimized executors for different use cases:</p> <pre><code>import graphbit\nimport os\n\ndef performance_examples():\n    graphbit.init()\n    config = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"), \"gpt-4o-mini\")\n\n    # High-throughput executor for batch processing\n    high_throughput = graphbit.Executor.new_high_throughput(config, timeout_seconds=120)\n\n    # Low-latency executor for real-time applications\n    low_latency = graphbit.Executor.new_low_latency(config, timeout_seconds=30)\n\n    # Memory-optimized executor for resource-constrained environments\n    memory_optimized = graphbit.Executor.new_memory_optimized(config)\n\n    # Use appropriate executor based on your needs\n    workflow = graphbit.Workflow(\"Performance Test\")\n\n    agent = graphbit.Node.agent(\n        name=\"Performance Agent\",\n        prompt=\"Process this efficiently: {input}\",\n        agent_id=\"perf_agent\"\n    )\n\n    workflow.add_node(agent)\n\n    # Execute with chosen executor\n    result = low_latency.execute(workflow)\n    print(f\"Execution mode: {low_latency.get_execution_mode()}\")\n    print(f\"Completed in: {result.execution_time_ms()}ms\")\n\nif __name__ == \"__main__\":\n    performance_examples()\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've created your first GraphBit workflows. Here's what to explore next:</p>"},{"location":"getting-started/quickstart/#learn-more","title":"Learn More","text":"<ul> <li>Core Concepts - Understand workflows, agents, and nodes</li> <li>Node Types - Explore all available node types</li> <li>LLM Providers - Configure different AI providers</li> </ul>"},{"location":"getting-started/quickstart/#advanced-features","title":"Advanced Features","text":"<ul> <li>Dynamic Graph Generation - Auto-generate workflow nodes</li> <li>Embeddings &amp; Vector Search - Add semantic search</li> <li>Performance Optimization - Tune for production</li> </ul>"},{"location":"getting-started/quickstart/#examples","title":"Examples","text":"<ul> <li>Content Generation Pipeline - Complete content creation workflow</li> <li>Data Processing Workflow - ETL with AI agents</li> <li>Code Review Automation - Automated code analysis</li> </ul>"},{"location":"getting-started/quickstart/#production-ready","title":"Production Ready","text":"<ul> <li>Error Handling &amp; Reliability - Production-grade error handling</li> <li>Monitoring &amp; Observability - Track workflow performance</li> <li>Configuration Options - Fine-tune your setup</li> </ul>"},{"location":"getting-started/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quickstart/#conditional-workflows","title":"Conditional Workflows","text":"<pre><code># Add condition nodes for branching logic\ncondition = graphbit.Node.condition(\n    name=\"Quality Check\",\n    expression=\"quality_score &gt; 0.8\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#transform-data","title":"Transform Data","text":"<pre><code># Transform node for data processing\ntransformer = graphbit.Node.transform(\n    name=\"JSON Extractor\",\n    transformation=\"json_extract\"\n)\n</code></pre>"},{"location":"getting-started/quickstart/#system-information-and-health-check","title":"System Information and Health Check","text":"<pre><code># Check GraphBit system status\nimport graphbit\n\ngraphbit.init()\n\n# Get system information\nsystem_info = graphbit.get_system_info()\nprint(f\"GraphBit version: {system_info['version']}\")\nprint(f\"Runtime healthy: {system_info['runtime_initialized']}\")\n\n# Perform health check\nhealth = graphbit.health_check()\nprint(f\"Overall healthy: {health['overall_healthy']}\")\n</code></pre> <p>Ready to build more complex workflows? Continue with our User Guide! </p>"},{"location":"user-guide/agents/","title":"Agents","text":"<p>Agents are AI-powered components that execute tasks within GraphBit workflows. This guide covers how to create, configure, and optimize agents for different use cases.</p>"},{"location":"user-guide/agents/#overview","title":"Overview","text":"<p>In GraphBit, agents are implemented as specialized workflow nodes that: - Execute AI tasks using configured LLM providers - Process inputs through prompt templates with variable substitution - Generate outputs that flow to connected nodes - Support different execution contexts and requirements</p>"},{"location":"user-guide/agents/#creating-agents","title":"Creating Agents","text":""},{"location":"user-guide/agents/#basic-agent-creation","title":"Basic Agent Creation","text":"<pre><code>import graphbit\n\n# Initialize GraphBit\ngraphbit.init()\n\n# Create a basic agent node\nanalyzer = graphbit.Node.agent(\n    name=\"Data Analyzer\",\n    prompt=\"Analyze the following data and identify key patterns: {input}\",\n    agent_id=\"analyzer\"  # Optional - auto-generated if not provided\n)\n\n# Access agent properties\nprint(f\"Agent ID: {analyzer.id()}\")\nprint(f\"Agent Name: {analyzer.name()}\")\n</code></pre>"},{"location":"user-guide/agents/#agent-with-explicit-configuration","title":"Agent with Explicit Configuration","text":"<pre><code># Agent with explicit ID for referencing\ncontent_creator = graphbit.Node.agent(\n    name=\"Content Creator\",\n    prompt=\"Create engaging content about: {topic}\",\n    agent_id=\"content_creator_v1\"\n)\n\n# Agent for specific domain\ntechnical_writer = graphbit.Node.agent(\n    name=\"Technical Documentation Writer\",\n    prompt=\"\"\"\n    Write comprehensive technical documentation for: {feature}\n\n    Include:\n    - Overview and purpose\n    - Implementation details\n    - Usage examples\n    - Best practices\n\n    Feature details: {input}\n    \"\"\",\n    agent_id=\"tech_doc_writer\"\n)\n</code></pre>"},{"location":"user-guide/agents/#agent-configuration-in-workflows","title":"Agent Configuration in Workflows","text":""},{"location":"user-guide/agents/#single-agent-workflow","title":"Single Agent Workflow","text":"<pre><code># Create workflow with single agent\nworkflow = graphbit.Workflow(\"Content Analysis\")\n\n# Create and add agent\nanalyzer = graphbit.Node.agent(\n    name=\"Content Analyzer\",\n    prompt=\"Analyze this content for sentiment, key themes, and quality: {input}\",\n    agent_id=\"content_analyzer\"\n)\n\nanalyzer_id = workflow.add_node(analyzer)\nworkflow.validate()\n\n# Execute with LLM configuration\nllm_config = graphbit.LlmConfig.openai(\n    api_key=\"your-openai-key\",\n    model=\"gpt-4o-mini\"\n)\n\nexecutor = graphbit.Executor(llm_config, timeout_seconds=60)\nresult = executor.execute(workflow)\n</code></pre>"},{"location":"user-guide/agents/#multi-agent-workflow","title":"Multi-Agent Workflow","text":"<pre><code># Create workflow with multiple specialized agents\nworkflow = graphbit.Workflow(\"Multi-Agent Analysis Pipeline\")\n\n# Create specialized agents\nsentiment_agent = graphbit.Node.agent(\n    name=\"Sentiment Analyzer\",\n    prompt=\"Analyze the sentiment of this text (positive/negative/neutral): {input}\",\n    agent_id=\"sentiment_analyzer\"\n)\n\ntopic_agent = graphbit.Node.agent(\n    name=\"Topic Extractor\", \n    prompt=\"Extract the main topics and themes from: {input}\",\n    agent_id=\"topic_extractor\"\n)\n\nsummary_agent = graphbit.Node.agent(\n    name=\"Content Summarizer\",\n    prompt=\"Create a concise summary of: {input}\",\n    agent_id=\"summarizer\"\n)\n\n# Aggregation agent\naggregator = graphbit.Node.agent(\n    name=\"Analysis Aggregator\",\n    prompt=\"\"\"\n    Combine the following analysis results into a comprehensive report:\n\n    Sentiment Analysis: {sentiment_output}\n    Topic Analysis: {topic_output}  \n    Summary: {summary_output}\n\n    Provide an integrated analysis with key insights.\n    \"\"\",\n    agent_id=\"aggregator\"\n)\n\n# Build workflow\nsentiment_id = workflow.add_node(sentiment_agent)\ntopic_id = workflow.add_node(topic_agent)\nsummary_id = workflow.add_node(summary_agent)\nagg_id = workflow.add_node(aggregator)\n\n# Connect nodes for parallel processing then aggregation\nworkflow.connect(sentiment_id, agg_id)\nworkflow.connect(topic_id, agg_id)\nworkflow.connect(summary_id, agg_id)\n\nworkflow.validate()\n</code></pre>"},{"location":"user-guide/agents/#prompt-engineering","title":"Prompt Engineering","text":""},{"location":"user-guide/agents/#basic-prompt-structure","title":"Basic Prompt Structure","text":"<p>Design effective prompts for your agents:</p> <pre><code># Simple, direct prompt\nsimple_agent = graphbit.Node.agent(\n    name=\"Simple Translator\",\n    prompt=\"Translate this text to French: {input}\",\n    agent_id=\"translator\"\n)\n\n# Structured prompt with clear instructions\nstructured_agent = graphbit.Node.agent(\n    name=\"Structured Analyzer\",\n    prompt=\"\"\"\n    Task: Analyze the provided text for business insights\n\n    Text to analyze: {input}\n\n    Please provide:\n    1. Key business themes identified\n    2. Market opportunities mentioned\n    3. Risk factors highlighted  \n    4. Recommended actions\n\n    Format your response as a structured analysis.\n    \"\"\",\n    agent_id=\"business_analyzer\"\n)\n</code></pre>"},{"location":"user-guide/agents/#variable-substitution","title":"Variable Substitution","text":"<p>Use variables in prompts for dynamic content:</p> <pre><code># Multi-variable prompt\nflexible_prompt = \"\"\"\nContext: You are a {role} expert analyzing {content_type} content.\n\nTask: {task_description}\n\nContent to analyze: {input}\n\nAnalysis requirements:\n- Focus on {focus_area}\n- Provide {detail_level} analysis\n- Use {tone} tone\n- Consider {constraints}\n\nPlease provide your analysis following these requirements.\n\"\"\"\n\nflexible_agent = graphbit.Node.agent(\n    name=\"Flexible Content Analyzer\",\n    prompt=flexible_prompt,\n    agent_id=\"flexible_analyzer\"\n)\n</code></pre>"},{"location":"user-guide/agents/#domain-specific-prompts","title":"Domain-Specific Prompts","text":"<p>Create agents for specific domains:</p> <pre><code># Financial analysis agent\nfinancial_agent = graphbit.Node.agent(\n    name=\"Financial Analyst\",\n    prompt=\"\"\"\n    As a financial expert, analyze the following financial data:\n\n    {input}\n\n    Provide analysis covering:\n    - Revenue trends and patterns\n    - Cost structure analysis\n    - Profitability insights\n    - Risk assessment\n    - Strategic recommendations\n\n    Use standard financial analysis frameworks in your assessment.\n    \"\"\",\n    agent_id=\"financial_analyst\"\n)\n\n# Marketing content agent\nmarketing_agent = graphbit.Node.agent(\n    name=\"Marketing Content Creator\",\n    prompt=\"\"\"\n    Create compelling marketing content for: {product}\n\n    Target audience: {audience}\n    Key features: {features}\n    Brand tone: {brand_tone}\n\n    Create:\n    1. Attention-grabbing headline\n    2. Benefit-focused description\n    3. Clear call-to-action\n    4. Key selling points\n\n    Content: {input}\n    \"\"\",\n    agent_id=\"marketing_creator\"\n)\n\n# Technical documentation agent\ntechnical_agent = graphbit.Node.agent(\n    name=\"Technical Documentation Writer\",\n    prompt=\"\"\"\n    Write clear, comprehensive technical documentation for developers.\n\n    Topic: {input}\n\n    Include:\n    - Clear overview and purpose\n    - Step-by-step implementation guide\n    - Code examples with explanations\n    - Common pitfalls and solutions\n    - Best practices and recommendations\n\n    Use clear, professional technical writing style.\n    \"\"\",\n    agent_id=\"tech_writer\"\n)\n</code></pre>"},{"location":"user-guide/agents/#agent-specialization-patterns","title":"Agent Specialization Patterns","text":""},{"location":"user-guide/agents/#sequential-processing-agents","title":"Sequential Processing Agents","text":"<p>Create agents that build on each other's work:</p> <pre><code>workflow = graphbit.Workflow(\"Sequential Content Processing\")\n\n# Stage 1: Content preparation\nprep_agent = graphbit.Node.agent(\n    name=\"Content Preparation Agent\",\n    prompt=\"Clean and structure this raw content for further processing: {input}\",\n    agent_id=\"content_prep\"\n)\n\n# Stage 2: Content analysis\nanalysis_agent = graphbit.Node.agent(\n    name=\"Content Analysis Agent\", \n    prompt=\"Analyze the prepared content for key insights: {prepared_content}\",\n    agent_id=\"content_analysis\"\n)\n\n# Stage 3: Content enhancement\nenhancement_agent = graphbit.Node.agent(\n    name=\"Content Enhancement Agent\",\n    prompt=\"Enhance the analyzed content with additional details: {analyzed_content}\",\n    agent_id=\"content_enhancement\"\n)\n\n# Connect sequentially\nprep_id = workflow.add_node(prep_agent)\nanalysis_id = workflow.add_node(analysis_agent)\nenhance_id = workflow.add_node(enhancement_agent)\n\nworkflow.connect(prep_id, analysis_id)\nworkflow.connect(analysis_id, enhance_id)\n</code></pre>"},{"location":"user-guide/agents/#parallel-specialist-agents","title":"Parallel Specialist Agents","text":"<p>Create specialized agents that work in parallel:</p> <pre><code>workflow = graphbit.Workflow(\"Parallel Content Analysis\")\n\n# Input preparation\ninput_agent = graphbit.Node.agent(\n    name=\"Input Processor\",\n    prompt=\"Prepare content for specialized analysis: {input}\",\n    agent_id=\"input_processor\"\n)\n\n# Parallel specialists\nseo_agent = graphbit.Node.agent(\n    name=\"SEO Specialist\",\n    prompt=\"Analyze SEO aspects of: {processed_content}\",\n    agent_id=\"seo_specialist\"\n)\n\nreadability_agent = graphbit.Node.agent(\n    name=\"Readability Specialist\",\n    prompt=\"Analyze readability and clarity of: {processed_content}\",\n    agent_id=\"readability_specialist\"\n)\n\ncompliance_agent = graphbit.Node.agent(\n    name=\"Compliance Specialist\", \n    prompt=\"Check compliance and accuracy of: {processed_content}\",\n    agent_id=\"compliance_specialist\"\n)\n\n# Results integrator\nintegrator = graphbit.Node.agent(\n    name=\"Results Integrator\",\n    prompt=\"\"\"\n    Integrate specialized analysis results:\n\n    SEO Analysis: {seo_output}\n    Readability Analysis: {readability_output}\n    Compliance Analysis: {compliance_output}\n\n    Provide comprehensive recommendations.\n    \"\"\",\n    agent_id=\"results_integrator\"\n)\n\n# Build parallel structure\ninput_id = workflow.add_node(input_agent)\nseo_id = workflow.add_node(seo_agent)\nread_id = workflow.add_node(readability_agent)\ncomp_id = workflow.add_node(compliance_agent)\nint_id = workflow.add_node(integrator)\n\n# Connect input to all specialists\nworkflow.connect(input_id, seo_id)\nworkflow.connect(input_id, read_id)\nworkflow.connect(input_id, comp_id)\n\n# Connect specialists to integrator\nworkflow.connect(seo_id, int_id)\nworkflow.connect(read_id, int_id)\nworkflow.connect(comp_id, int_id)\n</code></pre>"},{"location":"user-guide/agents/#agent-configuration-with-different-llm-providers","title":"Agent Configuration with Different LLM Providers","text":""},{"location":"user-guide/agents/#provider-optimized-agents","title":"Provider-Optimized Agents","text":"<p>Configure agents for different LLM providers:</p> <pre><code>def create_provider_optimized_agents():\n    \"\"\"Create agents optimized for different providers\"\"\"\n\n    # OpenAI-optimized agent (structured prompts work well)\n    openai_agent = graphbit.Node.agent(\n        name=\"OpenAI Structured Analyzer\",\n        prompt=\"\"\"\n        Task: Comprehensive content analysis\n\n        Content: {input}\n\n        Analysis Framework:\n        1. Content Structure Analysis\n        2. Quality Assessment\n        3. Improvement Recommendations\n        4. Risk Evaluation\n\n        Provide detailed analysis for each framework component.\n        \"\"\",\n        agent_id=\"openai_analyzer\"\n    )\n\n    # Anthropic-optimized agent (conversational style)\n    anthropic_agent = graphbit.Node.agent(\n        name=\"Claude Conversational Analyzer\",\n        prompt=\"\"\"\n        I'd like you to analyze this content from multiple perspectives.\n\n        Content: {input}\n\n        Please help me understand:\n        - What are the main themes and messages?\n        - How effective is the communication style?\n        - What improvements would you suggest?\n        - Are there any potential issues or concerns?\n\n        Please be thorough in your analysis and explain your reasoning.\n        \"\"\",\n        agent_id=\"claude_analyzer\"\n    )\n\n    # Ollama-optimized agent (concise prompts for local models)\n    ollama_agent = graphbit.Node.agent(\n        name=\"Local Model Analyzer\",\n        prompt=\"Analyze this content briefly: {input}\",\n        agent_id=\"local_analyzer\"\n    )\n\n    return {\n        \"openai\": openai_agent,\n        \"anthropic\": anthropic_agent,\n        \"ollama\": ollama_agent\n    }\n</code></pre>"},{"location":"user-guide/agents/#execution-with-different-providers","title":"Execution with Different Providers","text":"<pre><code>def execute_with_different_providers(agents, workflow_factory):\n    \"\"\"Execute same workflow with different providers\"\"\"\n\n    # OpenAI execution\n    openai_config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n    openai_executor = graphbit.Executor(openai_config, timeout_seconds=60)\n\n    # Anthropic execution\n    anthropic_config = graphbit.LlmConfig.anthropic(\n        api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n        model=\"claude-3-5-sonnet-20241022\"\n    )\n    anthropic_executor = graphbit.Executor(anthropic_config, timeout_seconds=120)\n\n    # Ollama execution\n    ollama_config = graphbit.LlmConfig.ollama(model=\"llama3.2\")\n    ollama_executor = graphbit.Executor(ollama_config, timeout_seconds=180)\n\n    return {\n        \"openai\": openai_executor,\n        \"anthropic\": anthropic_executor,\n        \"ollama\": ollama_executor\n    }\n</code></pre>"},{"location":"user-guide/agents/#error-handling-and-resilience","title":"Error Handling and Resilience","text":""},{"location":"user-guide/agents/#robust-agent-design","title":"Robust Agent Design","text":"<p>Design agents that handle edge cases:</p> <pre><code># Agent with error handling instructions\nrobust_agent = graphbit.Node.agent(\n    name=\"Robust Content Processor\",\n    prompt=\"\"\"\n    Process the following content. If the content is unclear, incomplete, \n    or problematic, please:\n\n    1. Identify specific issues\n    2. Provide what analysis is possible\n    3. Suggest what additional information would be helpful\n    4. Indicate confidence level in your analysis\n\n    Content: {input}\n\n    If you cannot process the content, explain why and suggest alternatives.\n    \"\"\",\n    agent_id=\"robust_processor\"\n)\n\n# Agent with fallback behavior\nfallback_agent = graphbit.Node.agent(\n    name=\"Fallback Handler\",\n    prompt=\"\"\"\n    This content may have been processed unsuccessfully by a previous agent.\n\n    Previous result: {previous_output}\n    Original content: {original_input}\n\n    Please provide a basic analysis of the original content, noting any\n    issues or limitations in your analysis.\n    \"\"\",\n    agent_id=\"fallback_handler\"\n)\n</code></pre>"},{"location":"user-guide/agents/#advanced-agent-patterns","title":"Advanced Agent Patterns","text":""},{"location":"user-guide/agents/#agent-with-context-memory","title":"Agent with Context Memory","text":"<p>Create agents that maintain context across processing steps:</p> <pre><code># Context-aware agent\ncontext_agent = graphbit.Node.agent(\n    name=\"Context Aware Processor\",\n    prompt=\"\"\"\n    Previous context: {context_history}\n    Current input: {input}\n    Processing step: {step_number}\n\n    Process the current input while maintaining awareness of the previous context.\n    Update the context for the next processing step.\n\n    Provide:\n    1. Analysis of current input\n    2. Relationship to previous context\n    3. Updated context summary for next steps\n    \"\"\",\n    agent_id=\"context_processor\"\n)\n</code></pre>"},{"location":"user-guide/agents/#quality-control-agents","title":"Quality Control Agents","text":"<p>Create agents that validate and improve outputs:</p> <pre><code># Quality validator agent\nvalidator_agent = graphbit.Node.agent(\n    name=\"Quality Validator\",\n    prompt=\"\"\"\n    Review the following content for quality:\n\n    Content: {input}\n\n    Evaluate:\n    - Accuracy and factual correctness\n    - Clarity and readability\n    - Completeness of information\n    - Logical flow and structure\n\n    Provide quality score (1-10) and specific improvement suggestions.\n    \"\"\",\n    agent_id=\"quality_validator\"\n)\n\n# Content improver agent\nimprover_agent = graphbit.Node.agent(\n    name=\"Content Improver\",\n    prompt=\"\"\"\n    Improve the following content based on quality feedback:\n\n    Original content: {original_content}\n    Quality feedback: {quality_feedback}\n\n    Provide improved version addressing the specific feedback points.\n    Maintain the core message while enhancing quality.\n    \"\"\",\n    agent_id=\"content_improver\"\n)\n</code></pre>"},{"location":"user-guide/agents/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/agents/#1-agent-naming-and-organization","title":"1. Agent Naming and Organization","text":"<pre><code># Good: Descriptive, clear names\nemail_spam_detector = graphbit.Node.agent(\n    name=\"Email Spam Detection Agent\",\n    prompt=\"Analyze this email for spam indicators: {email_content}\",\n    agent_id=\"email_spam_detector_v1\"\n)\n\n# Good: Consistent naming convention\nfinancial_risk_analyzer = graphbit.Node.agent(\n    name=\"Financial Risk Analysis Agent\",\n    prompt=\"Assess financial risks in: {financial_data}\",\n    agent_id=\"financial_risk_analyzer_v1\"\n)\n\n# Avoid: Vague names\nagent1 = graphbit.Node.agent(\n    name=\"Agent 1\",\n    prompt=\"Do something with: {input}\",\n    agent_id=\"a1\"\n)\n</code></pre>"},{"location":"user-guide/agents/#2-prompt-design-guidelines","title":"2. Prompt Design Guidelines","text":"<pre><code># Good: Clear, specific prompts\ncontent_analyzer = graphbit.Node.agent(\n    name=\"Marketing Content Analyzer\",\n    prompt=\"\"\"\n    Analyze this marketing content for effectiveness:\n\n    Content: {input}\n\n    Evaluate:\n    1. Target audience alignment\n    2. Message clarity and impact\n    3. Call-to-action effectiveness\n    4. Brand consistency\n    5. Competitive differentiation\n\n    Provide specific recommendations for improvement.\n    \"\"\",\n    agent_id=\"marketing_content_analyzer\"\n)\n\n# Avoid: Vague, unclear prompts\nvague_agent = graphbit.Node.agent(\n    name=\"Content Thing\",\n    prompt=\"Look at this: {input}\",\n    agent_id=\"vague\"\n)\n</code></pre>"},{"location":"user-guide/agents/#3-agent-composition","title":"3. Agent Composition","text":"<pre><code>def create_modular_agents():\n    \"\"\"Create modular, reusable agents\"\"\"\n\n    agents = {}\n\n    # Base analysis agent\n    agents['base_analyzer'] = graphbit.Node.agent(\n        name=\"Base Content Analyzer\",\n        prompt=\"Provide basic analysis of: {input}\",\n        agent_id=\"base_analyzer\"\n    )\n\n    # Specialized enhancement agents\n    agents['seo_enhancer'] = graphbit.Node.agent(\n        name=\"SEO Enhancement Agent\",\n        prompt=\"Enhance SEO aspects of: {analyzed_content}\",\n        agent_id=\"seo_enhancer\"\n    )\n\n    agents['readability_enhancer'] = graphbit.Node.agent(\n        name=\"Readability Enhancement Agent\",\n        prompt=\"Improve readability of: {analyzed_content}\",\n        agent_id=\"readability_enhancer\"\n    )\n\n    return agents\n\n# Usage: Compose agents into workflows as needed\ndef create_seo_workflow(agents):\n    workflow = graphbit.Workflow(\"SEO Content Pipeline\")\n\n    base_id = workflow.add_node(agents['base_analyzer'])\n    seo_id = workflow.add_node(agents['seo_enhancer'])\n\n    workflow.connect(base_id, seo_id)\n    return workflow\n</code></pre>"},{"location":"user-guide/agents/#4-testing-and-validation","title":"4. Testing and Validation","text":"<pre><code>def test_agent_configuration():\n    \"\"\"Test agent configuration before production use\"\"\"\n\n    # Create test agent\n    test_agent = graphbit.Node.agent(\n        name=\"Test Agent\",\n        prompt=\"Test prompt with {input}\",\n        agent_id=\"test_agent\"\n    )\n\n    # Validate agent properties\n    assert test_agent.name() == \"Test Agent\"\n    assert test_agent.id() is not None\n\n    # Test workflow integration\n    workflow = graphbit.Workflow(\"Test Workflow\")\n    node_id = workflow.add_node(test_agent)\n\n    try:\n        workflow.validate()\n        print(\"\u2705 Agent configuration is valid\")\n        return True\n    except Exception as e:\n        print(f\"\u274c Agent configuration failed: {e}\")\n        return False\n</code></pre>"},{"location":"user-guide/agents/#whats-next","title":"What's Next","text":"<ul> <li>Learn about Workflow Builder for complex agent orchestration</li> <li>Explore LLM Providers for provider-specific optimizations</li> <li>Check Performance for agent execution optimization</li> <li>See Validation for agent output validation strategies</li> </ul>"},{"location":"user-guide/async-vs-sync/","title":"Async vs Sync Usage in GraphBit Python API","text":"<p>GraphBit's Python bindings provide both synchronous (blocking) and asynchronous (non-blocking) interfaces for key operations, allowing you to choose the best fit for your application\u2014whether it's a quick script, a web server, or a data pipeline.</p>"},{"location":"user-guide/async-vs-sync/#overview","title":"Overview","text":"<ul> <li>Synchronous (Sync) functions block the current thread until the operation completes. They are simple to use in scripts and REPLs.</li> <li>Asynchronous (Async) functions return immediately and run in the background, allowing your program to do other work or handle many tasks concurrently. Async is ideal for web servers, pipelines, and high-throughput applications.</li> </ul>"},{"location":"user-guide/async-vs-sync/#supported-sync-and-async-functions","title":"Supported Sync and Async Functions","text":""},{"location":"user-guide/async-vs-sync/#module","title":"Module","text":"Function Type <code>init</code> Sync <code>version</code> Sync <code>get_system_info</code> Sync <code>health_check</code> Sync <code>configure_runtime</code> Sync <code>shutdown</code> Sync"},{"location":"user-guide/async-vs-sync/#llm-client","title":"LLM Client","text":"Function Type <code>complete</code> Sync <code>get_stats</code> Sync <code>complete_async</code> Async <code>complete_stream</code> Async <code>complete_batch</code> Async <code>warmup</code> Async"},{"location":"user-guide/async-vs-sync/#embedding-client","title":"Embedding Client","text":"Function Type <code>embed</code> Sync <code>embed_many</code> Sync <code>similarity</code> Sync"},{"location":"user-guide/async-vs-sync/#workflow","title":"Workflow","text":"Function Type <code>add_node</code> Sync <code>connect</code> Sync <code>validate</code> Sync"},{"location":"user-guide/async-vs-sync/#workflow-executor","title":"Workflow Executor","text":"Function Type <code>new_high_thoroughput</code> Sync <code>new_low_latency</code> Sync <code>new_memory_optimized</code> Sync <code>configure</code> Sync <code>get_stats</code> Sync <code>reset_stats</code> Sync <code>get_execution_mode</code> Sync <code>execute</code> Async <code>run_async</code> Async"},{"location":"user-guide/async-vs-sync/#workflow-node","title":"Workflow Node","text":"Function Type <code>agent</code> Sync <code>condition</code> Sync <code>transform</code> Sync"},{"location":"user-guide/async-vs-sync/#workflow-result","title":"Workflow Result","text":"Function Type <code>is_success</code> Sync <code>is_failed</code> Sync <code>state</code> Sync <code>execution_time_ms</code> Sync <code>get_variable</code> Sync <code>get_all_variables</code> Sync <code>variables</code> Sync"},{"location":"user-guide/async-vs-sync/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/async-vs-sync/#synchronous-usage","title":"Synchronous Usage","text":"<pre><code>import os\nimport graphbit\n\ngraphbit.init()\n\nconfig = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"))\n\nclient = graphbit.LlmClient(config)\nresult = client.complete(\"Hello, world!\")\nprint(result)\n</code></pre>"},{"location":"user-guide/async-vs-sync/#asynchronous-usage","title":"Asynchronous Usage","text":"<pre><code>import os\nimport graphbit\nimport asyncio\n\nconfig = graphbit.LlmConfig.openai(os.getenv(\"OPENAI_API_KEY\"))\n\nclient = graphbit.LlmClient(config)\n\nasync def main():\n    result = await client.complete_async(\"Hello, async world!\")\n    print(result)\n\n    # Streaming completion\n    stream_result = await client.complete_stream(\"Stream this!\")\n    print(stream_result)\n\nasyncio.run(main())\n</code></pre> <p>This documentation demonstrates sync and async functions supported by GraphBit\u2019s Python API, enabling you to select the best approach for your use case, whether you need straightforward synchronous calls or high-performance asynchronous workflows.</p>"},{"location":"user-guide/concepts/","title":"Core Concepts","text":"<p>Understanding GraphBit's fundamental concepts will help you build powerful AI agent workflows. This guide covers the key components and how they work together.</p>"},{"location":"user-guide/concepts/#overview","title":"Overview","text":"<p>GraphBit is built around these core concepts:</p> <ol> <li>Library Initialization - Setting up the GraphBit environment</li> <li>LLM Providers - Configuring language model clients</li> <li>Workflows - Directed graphs that define the execution flow</li> <li>Nodes - Individual processing units (agents, conditions, transforms)</li> <li>Executors - Engines that run workflows with different performance characteristics</li> <li>Embeddings - Vector embeddings for semantic operations</li> </ol>"},{"location":"user-guide/concepts/#library-initialization","title":"Library Initialization","text":"<p>Before using GraphBit, you must initialize the library:</p> <pre><code>import graphbit\n\n# Basic initialization\ngraphbit.init()\n\n# With custom configuration\ngraphbit.init(\n    log_level=\"info\",           # trace, debug, info, warn, error\n    enable_tracing=True,        # Enable detailed logging\n    debug=False                 # Debug mode\n)\n\n# Check system status\nprint(f\"GraphBit version: {graphbit.version()}\")\nprint(\"System info:\", graphbit.get_system_info())\nprint(\"Health check:\", graphbit.health_check())\n</code></pre>"},{"location":"user-guide/concepts/#runtime-configuration","title":"Runtime Configuration","text":"<p>For advanced use cases, configure the async runtime:</p> <pre><code># Configure before init() if needed\ngraphbit.configure_runtime(\n    worker_threads=8,           # Number of worker threads\n    max_blocking_threads=16,    # Max blocking thread pool size\n    thread_stack_size_mb=2      # Stack size per thread in MB\n)\n\ngraphbit.init()\n</code></pre>"},{"location":"user-guide/concepts/#llm-providers","title":"LLM Providers","text":"<p>GraphBit supports multiple LLM providers with consistent APIs.</p>"},{"location":"user-guide/concepts/#provider-configuration","title":"Provider Configuration","text":"<pre><code># OpenAI\nopenai_config = graphbit.LlmConfig.openai(\n    api_key=\"your-openai-key\",\n    model=\"gpt-4o-mini\"  # Optional, defaults to gpt-4o-mini\n)\n\n# Anthropic\nanthropic_config = graphbit.LlmConfig.anthropic(\n    api_key=\"your-anthropic-key\", \n    model=\"claude-3-5-sonnet-20241022\"  # Optional, defaults to claude-3-5-sonnet\n)\n\n# DeepSeek\ndeepseek_config = graphbit.LlmConfig.deepseek(\n    api_key=\"your-deepseek-key\",\n    model=\"deepseek-chat\"  # Optional, defaults to deepseek-chat\n)\n\n# Ollama (local models)\nollama_config = graphbit.LlmConfig.ollama(\n    model=\"llama3.2\"  # Optional, defaults to llama3.2\n)\n</code></pre>"},{"location":"user-guide/concepts/#llm-client-usage","title":"LLM Client Usage","text":"<pre><code># Create client\nclient = graphbit.LlmClient(openai_config, debug=False)\n\n# Basic completion\nresponse = client.complete(\n    prompt=\"Explain quantum computing\",\n    max_tokens=500,\n    temperature=0.7\n)\n\n# Async completion\nimport asyncio\nasync_response = await client.complete_async(\n    prompt=\"Write a poem about AI\",\n    max_tokens=200,\n    temperature=0.9\n)\n\n# Batch processing\nresponses = await client.complete_batch(\n    prompts=[\"Question 1\", \"Question 2\", \"Question 3\"],\n    max_tokens=100,\n    temperature=0.5,\n    max_concurrency=3\n)\n\n# Chat-style interaction\nchat_response = await client.chat_optimized(\n    messages=[\n        (\"user\", \"Hello, how are you?\"),\n        (\"assistant\", \"I'm doing well, thank you!\"),\n        (\"user\", \"Can you help me with Python?\")\n    ],\n    max_tokens=300\n)\n\n# Streaming responses\nasync for chunk in client.complete_stream(\n    prompt=\"Tell me a story\",\n    max_tokens=1000\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"user-guide/concepts/#client-management","title":"Client Management","text":"<pre><code># Get client statistics\nstats = client.get_stats()\nprint(f\"Total requests: {stats['total_requests']}\")\nprint(f\"Success rate: {stats['successful_requests'] / stats['total_requests']}\")\n\n# Warmup client (pre-initialize connections)\nawait client.warmup()\n\n# Reset statistics\nclient.reset_stats()\n</code></pre>"},{"location":"user-guide/concepts/#workflows","title":"Workflows","text":"<p>A Workflow defines the structure and flow of your AI pipeline.</p>"},{"location":"user-guide/concepts/#creating-workflows","title":"Creating Workflows","text":"<pre><code># Create a workflow\nworkflow = graphbit.Workflow(\"My AI Pipeline\")\n\n# Add nodes to workflow\nagent_node = graphbit.Node.agent(\n    name=\"Analyzer\",\n    prompt=\"Analyze this data: {input}\",\n    agent_id=\"analyzer_001\"  # Optional\n)\n\ntransform_node = graphbit.Node.transform(\n    name=\"Formatter\", \n    transformation=\"uppercase\"\n)\n\ncondition_node = graphbit.Node.condition(\n    name=\"Quality Check\",\n    expression=\"quality_score &gt; 0.8\"\n)\n\n# Add nodes and get their IDs\nagent_id = workflow.add_node(agent_node)\ntransform_id = workflow.add_node(transform_node)\ncondition_id = workflow.add_node(condition_node)\n\n# Connect nodes\nworkflow.connect(agent_id, transform_id)\nworkflow.connect(transform_id, condition_id)\n\n# Validate workflow structure\nworkflow.validate()\n</code></pre>"},{"location":"user-guide/concepts/#nodes","title":"Nodes","text":"<p>Nodes are the building blocks of workflows. Each node performs a specific function.</p>"},{"location":"user-guide/concepts/#node-types","title":"Node Types","text":""},{"location":"user-guide/concepts/#1-agent-nodes","title":"1. Agent Nodes","text":"<p>Execute AI tasks using LLM providers:</p> <pre><code># Basic agent node\nanalyzer = graphbit.Node.agent(\n    name=\"Data Analyzer\",\n    prompt=\"Analyze the following data and identify key patterns: {input}\",\n    agent_id=\"analyzer\"  # Optional - auto-generated if not provided\n)\n\n# Access node properties\nprint(f\"Node ID: {analyzer.id()}\")\nprint(f\"Node Name: {analyzer.name()}\")\n</code></pre>"},{"location":"user-guide/concepts/#2-transform-nodes","title":"2. Transform Nodes","text":"<p>Process and modify data:</p> <pre><code># Text transformation\nformatter = graphbit.Node.transform(\n    name=\"Text Formatter\",\n    transformation=\"uppercase\"  # Available: uppercase, lowercase, etc.\n)\n</code></pre>"},{"location":"user-guide/concepts/#3-condition-nodes","title":"3. Condition Nodes","text":"<p>Make decisions based on data evaluation:</p> <pre><code># Conditional logic\ngate = graphbit.Node.condition(\n    name=\"Quality Gate\",\n    expression=\"score &gt; 75 and confidence &gt; 0.8\"\n)\n</code></pre>"},{"location":"user-guide/concepts/#workflow-execution","title":"Workflow Execution","text":"<p>Executors run workflows with different performance characteristics.</p>"},{"location":"user-guide/concepts/#executor-types","title":"Executor Types","text":"<pre><code># Standard executor\nexecutor = graphbit.Executor(\n    config=llm_config,\n    lightweight_mode=False,  # For backward compatibility\n    timeout_seconds=300,     # 5 minutes\n    debug=False\n)\n\n# High-throughput executor (for batch processing)\nhigh_throughput = graphbit.Executor.new_high_throughput(\n    llm_config=llm_config,\n    timeout_seconds=600,\n    debug=False\n)\n\n# Low-latency executor (for real-time applications)\nlow_latency = graphbit.Executor.new_low_latency(\n    llm_config=llm_config,\n    timeout_seconds=30,\n    debug=False\n)\n\n# Memory-optimized executor (for resource-constrained environments)\nmemory_optimized = graphbit.Executor.new_memory_optimized(\n    llm_config=llm_config,\n    timeout_seconds=300,\n    debug=False\n)\n</code></pre>"},{"location":"user-guide/concepts/#execution-modes","title":"Execution Modes","text":"<pre><code># Synchronous execution\nresult = executor.execute(workflow)\n\n# Check execution results\nif result.is_completed():\n    print(\"Success:\", result.output())\nelif result.is_failed():\n    print(\"Failed:\", result.error())\n\n# Asynchronous execution\nasync_result = await executor.run_async(workflow)\n</code></pre>"},{"location":"user-guide/concepts/#executor-configuration","title":"Executor Configuration","text":"<pre><code># Runtime configuration\nexecutor.configure(\n    timeout_seconds=600,\n    max_retries=5,\n    enable_metrics=True,\n    debug=False\n)\n\n# Get execution statistics\nstats = executor.get_stats()\nprint(f\"Total executions: {stats['total_executions']}\")\nprint(f\"Success rate: {stats['successful_executions'] / stats['total_executions']}\")\nprint(f\"Average duration: {stats['average_duration_ms']}ms\")\n\n# Reset statistics\nexecutor.reset_stats()\n\n# Check execution mode\nprint(f\"Current mode: {executor.get_execution_mode()}\")\n\n# Legacy mode support\nexecutor.set_lightweight_mode(True)  # Enable lightweight mode\nprint(f\"Lightweight mode: {executor.is_lightweight_mode()}\")\n</code></pre>"},{"location":"user-guide/concepts/#embeddings","title":"Embeddings","text":"<p>Embeddings provide semantic vector representations for text.</p>"},{"location":"user-guide/concepts/#embedding-configuration","title":"Embedding Configuration","text":"<pre><code># OpenAI embeddings\nembedding_config = graphbit.EmbeddingConfig.openai(\n    api_key=\"your-openai-key\",\n    model=\"text-embedding-3-small\"  # Optional\n)\n\n# HuggingFace embeddings  \nhf_embedding_config = graphbit.EmbeddingConfig.huggingface(\n    api_key=\"your-hf-key\",\n    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n</code></pre>"},{"location":"user-guide/concepts/#using-embeddings","title":"Using Embeddings","text":"<pre><code># Create embedding client\nembedding_client = graphbit.EmbeddingClient(embedding_config)\n\n# Single text embedding\nvector = embedding_client.embed(\"This is a sample text\")\nprint(f\"Embedding dimension: {len(vector)}\")\n\n# Batch text embeddings\ntexts = [\"Text 1\", \"Text 2\", \"Text 3\"]\nvectors = embedding_client.embed_many(texts)\nprint(f\"Generated {len(vectors)} embeddings\")\n\n# Calculate similarity\nsimilarity = graphbit.EmbeddingClient.similarity(vector1, vector2)\nprint(f\"Cosine similarity: {similarity}\")\n</code></pre>"},{"location":"user-guide/concepts/#error-handling","title":"Error Handling","text":"<p>GraphBit provides comprehensive error handling:</p> <pre><code>try:\n    graphbit.init()\n\n    # Your workflow code here\n    result = executor.execute(workflow)\n\n    if result.is_failed():\n        print(f\"Workflow failed: {result.error()}\")\n\nexcept Exception as e:\n    print(f\"GraphBit error: {e}\")\n</code></pre>"},{"location":"user-guide/concepts/#best-practices","title":"Best Practices","text":"<ol> <li>Always initialize GraphBit before using any functionality</li> <li>Use appropriate executor types for your performance requirements</li> <li>Handle errors gracefully and check execution results</li> <li>Monitor execution statistics for performance optimization</li> <li>Configure timeouts appropriate for your use case</li> <li>Use warmup for production clients to reduce cold start latency</li> </ol>"},{"location":"user-guide/concepts/#whats-next","title":"What's Next","text":"<ul> <li>Learn about Workflow Builder for advanced workflow construction</li> <li>Explore LLM Providers for detailed provider configuration</li> <li>Check Embeddings for semantic operations</li> <li>See Performance for optimization techniques</li> </ul>"},{"location":"user-guide/dynamics-graph/","title":"Dynamic Workflow Creation","text":"<p>GraphBit supports dynamic workflow creation, allowing you to build and modify workflows at runtime based on data, conditions, and business logic. This powerful feature enables adaptive workflows that can respond to changing requirements.</p>"},{"location":"user-guide/dynamics-graph/#overview","title":"Overview","text":"<p>Dynamic workflow creation allows you to: - Create workflows that adapt to input data - Generate nodes and connections programmatically - Modify workflow structure based on runtime conditions - Build self-organizing processing pipelines - Implement conditional workflow branches</p>"},{"location":"user-guide/dynamics-graph/#basic-dynamic-workflow-creation","title":"Basic Dynamic Workflow Creation","text":""},{"location":"user-guide/dynamics-graph/#simple-dynamic-workflow","title":"Simple Dynamic Workflow","text":"<pre><code>import graphbit\n\n# Initialize GraphBit\ngraphbit.init()\n\ndef create_dynamic_workflow(input_data):\n    \"\"\"Creates a workflow dynamically based on input data.\"\"\"\n\n    # Analyze input to determine workflow structure\n    data_type = detect_data_type(input_data)\n\n    if data_type == \"text\":\n        return create_text_processing_workflow()\n    elif data_type == \"numerical\":\n        return create_numerical_analysis_workflow()\n    elif data_type == \"mixed\":\n        return create_mixed_data_workflow()\n    else:\n        return create_generic_workflow()\n\ndef detect_data_type(data):\n    \"\"\"Detect the type of input data.\"\"\"\n    if isinstance(data, str):\n        return \"text\"\n    elif isinstance(data, (int, float)):\n        return \"numerical\"\n    elif isinstance(data, dict) or isinstance(data, list):\n        return \"mixed\"\n    else:\n        return \"unknown\"\n\ndef create_text_processing_workflow():\n    \"\"\"Create workflow optimized for text processing.\"\"\"\n\n    workflow = graphbit.Workflow(\"Text Processing Workflow\")\n\n    # Text analyzer\n    analyzer = graphbit.Node.agent(\n        name=\"Text Analyzer\",\n        prompt=\"Analyze this text: {input}\",\n        agent_id=\"text_analyzer\"\n    )\n\n    # Sentiment detector\n    sentiment = graphbit.Node.agent(\n        name=\"Sentiment Detector\",\n        prompt=\"Determine sentiment of: {analyzed_text}\",\n        agent_id=\"sentiment_detector\"\n    )\n\n    # Build text processing chain\n    analyzer_id = workflow.add_node(analyzer)\n    sentiment_id = workflow.add_node(sentiment)\n\n    workflow.connect(analyzer_id, sentiment_id)\n\n    return workflow\n\ndef create_numerical_analysis_workflow():\n    \"\"\"Create workflow optimized for numerical analysis.\"\"\"\n\n    workflow = graphbit.Workflow(\"Numerical Analysis Workflow\")\n\n    # Statistical analyzer\n    stats = graphbit.Node.agent(\n        name=\"Statistical Analyzer\",\n        prompt=\"Perform statistical analysis on: {input}\",\n        agent_id=\"stats_analyzer\"\n    )\n\n    # Trend detector\n    trends = graphbit.Node.agent(\n        name=\"Trend Detector\",\n        prompt=\"Identify trends in: {stats_results}\",\n        agent_id=\"trend_detector\"\n    )\n\n    # Build numerical analysis chain\n    stats_id = workflow.add_node(stats)\n    trends_id = workflow.add_node(trends)\n\n    workflow.connect(stats_id, trends_id)\n\n    return workflow\n\ndef create_mixed_data_workflow():\n    \"\"\"Create workflow for mixed data types.\"\"\"\n\n    workflow = graphbit.Workflow(\"Mixed Data Workflow\")\n\n    # Data classifier\n    classifier = graphbit.Node.agent(\n        name=\"Data Classifier\",\n        prompt=\"Classify this mixed data: {input}\",\n        agent_id=\"classifier\"\n    )\n\n    # Multi-modal processor\n    processor = graphbit.Node.agent(\n        name=\"Multi-Modal Processor\",\n        prompt=\"Process classified data: {classified_data}\",\n        agent_id=\"multimodal_processor\"\n    )\n\n    # Build mixed data chain\n    classifier_id = workflow.add_node(classifier)\n    processor_id = workflow.add_node(processor)\n\n    workflow.connect(classifier_id, processor_id)\n\n    return workflow\n\ndef create_generic_workflow():\n    \"\"\"Create generic workflow for unknown data types.\"\"\"\n\n    workflow = graphbit.Workflow(\"Generic Workflow\")\n\n    # Generic processor\n    processor = graphbit.Node.agent(\n        name=\"Generic Processor\",\n        prompt=\"Process this input: {input}\",\n        agent_id=\"generic_processor\"\n    )\n\n    workflow.add_node(processor)\n\n    return workflow\n</code></pre>"},{"location":"user-guide/dynamics-graph/#advanced-dynamic-generation","title":"Advanced Dynamic Generation","text":""},{"location":"user-guide/dynamics-graph/#data-driven-node-creation","title":"Data-Driven Node Creation","text":"<pre><code>def create_data_driven_workflow(schema):\n    \"\"\"Create workflow based on data schema.\"\"\"\n\n    workflow = graphbit.Workflow(\"Data-Driven Workflow\")\n\n    node_ids = []\n\n    # Create nodes based on schema fields\n    for field in schema.get(\"fields\", []):\n        field_type = field.get(\"type\")\n        field_name = field.get(\"name\")\n\n        if field_type == \"string\":\n            node = create_text_processing_node(field_name)\n        elif field_type == \"number\":\n            node = create_numerical_processing_node(field_name)\n        elif field_type == \"date\":\n            node = create_date_processing_node(field_name)\n        else:\n            node = create_generic_processing_node(field_name)\n\n        node_id = workflow.add_node(node)\n        node_ids.append((field_name, node_id))\n\n    # Create aggregator node\n    aggregator = graphbit.Node.agent(\n        name=\"Data Aggregator\",\n        prompt=\"Combine and analyze these processed fields: {all_results}\",\n        agent_id=\"aggregator\"\n    )\n\n    agg_id = workflow.add_node(aggregator)\n\n    # Connect all field processors to aggregator\n    for field_name, node_id in node_ids:\n        workflow.connect(node_id, agg_id)\n\n    return workflow\n\ndef create_text_processing_node(field_name):\n    \"\"\"Create node for text field processing.\"\"\"\n    return graphbit.Node.agent(\n        name=f\"{field_name} Text Processor\",\n        prompt=f\"Process {field_name} text field: {{{field_name}_input}}\",\n        agent_id=f\"{field_name}_text_processor\"\n    )\n\ndef create_numerical_processing_node(field_name):\n    \"\"\"Create node for numerical field processing.\"\"\"\n    return graphbit.Node.agent(\n        name=f\"{field_name} Numerical Processor\",\n        prompt=f\"Analyze {field_name} numerical data: {{{field_name}_input}}\",\n        agent_id=f\"{field_name}_num_processor\"\n    )\n\ndef create_date_processing_node(field_name):\n    \"\"\"Create node for date field processing.\"\"\"\n    return graphbit.Node.agent(\n        name=f\"{field_name} Date Processor\",\n        prompt=f\"Analyze {field_name} date patterns: {{{field_name}_input}}\",\n        agent_id=f\"{field_name}_date_processor\"\n    )\n\ndef create_generic_processing_node(field_name):\n    \"\"\"Create generic processing node.\"\"\"\n    return graphbit.Node.agent(\n        name=f\"{field_name} Generic Processor\",\n        prompt=f\"Process {field_name} field: {{{field_name}_input}}\",\n        agent_id=f\"{field_name}_generic_processor\"\n    )\n</code></pre>"},{"location":"user-guide/dynamics-graph/#conditional-workflow-generation","title":"Conditional Workflow Generation","text":"<pre><code>def create_conditional_workflow(requirements):\n    \"\"\"Create workflow with conditional branches.\"\"\"\n\n    workflow = graphbit.Workflow(\"Conditional Workflow\")\n\n    # Input processor\n    input_processor = graphbit.Node.agent(\n        name=\"Input Processor\",\n        prompt=\"Process and analyze input: {input}\",\n        agent_id=\"input_processor\"\n    )\n\n    input_id = workflow.add_node(input_processor)\n\n    # Create conditional branches based on requirements\n    for requirement in requirements:\n        condition_type = requirement.get(\"type\")\n        condition_value = requirement.get(\"value\")\n\n        if condition_type == \"quality_check\":\n            branch = create_quality_branch(condition_value)\n        elif condition_type == \"complexity_check\":\n            branch = create_complexity_branch(condition_value)\n        elif condition_type == \"priority_check\":\n            branch = create_priority_branch(condition_value)\n        else:\n            branch = create_default_branch()\n\n        # Add branch to workflow\n        for node in branch[\"nodes\"]:\n            node_id = workflow.add_node(node)\n            if branch[\"condition\"]:\n                # Add condition node\n                condition_id = workflow.add_node(branch[\"condition\"])\n                workflow.connect(input_id, condition_id)\n                workflow.connect(condition_id, node_id)\n            else:\n                workflow.connect(input_id, node_id)\n\n    return workflow\n\ndef create_quality_branch(threshold):\n    \"\"\"Create quality checking branch.\"\"\"\n\n    condition = graphbit.Node.condition(\n        name=\"Quality Gate\",\n        expression=f\"quality_score &gt; {threshold}\"\n    )\n\n    high_quality_processor = graphbit.Node.agent(\n        name=\"High Quality Processor\",\n        prompt=\"Process high-quality input: {input}\",\n        agent_id=\"high_quality_proc\"\n    )\n\n    low_quality_processor = graphbit.Node.agent(\n        name=\"Low Quality Processor\",\n        prompt=\"Enhance and process low-quality input: {input}\",\n        agent_id=\"low_quality_proc\"\n    )\n\n    return {\n        \"condition\": condition,\n        \"nodes\": [high_quality_processor, low_quality_processor]\n    }\n\ndef create_complexity_branch(complexity_level):\n    \"\"\"Create complexity-based branch.\"\"\"\n\n    condition = graphbit.Node.condition(\n        name=\"Complexity Check\",\n        expression=f\"complexity_level &lt;= {complexity_level}\"\n    )\n\n    simple_processor = graphbit.Node.agent(\n        name=\"Simple Processor\",\n        prompt=\"Quick processing for simple input: {input}\",\n        agent_id=\"simple_proc\"\n    )\n\n    complex_processor = graphbit.Node.agent(\n        name=\"Complex Processor\",\n        prompt=\"Detailed processing for complex input: {input}\",\n        agent_id=\"complex_proc\"\n    )\n\n    return {\n        \"condition\": condition,\n        \"nodes\": [simple_processor, complex_processor]\n    }\n\ndef create_priority_branch(priority_level):\n    \"\"\"Create priority-based branch.\"\"\"\n\n    condition = graphbit.Node.condition(\n        name=\"Priority Check\",\n        expression=f\"priority &gt;= {priority_level}\"\n    )\n\n    urgent_processor = graphbit.Node.agent(\n        name=\"Urgent Processor\",\n        prompt=\"Fast processing for urgent input: {input}\",\n        agent_id=\"urgent_proc\"\n    )\n\n    standard_processor = graphbit.Node.agent(\n        name=\"Standard Processor\",\n        prompt=\"Standard processing: {input}\",\n        agent_id=\"standard_proc\"\n    )\n\n    return {\n        \"condition\": condition,\n        \"nodes\": [urgent_processor, standard_processor]\n    }\n\ndef create_default_branch():\n    \"\"\"Create default processing branch.\"\"\"\n\n    default_processor = graphbit.Node.agent(\n        name=\"Default Processor\",\n        prompt=\"Default processing: {input}\",\n        agent_id=\"default_proc\"\n    )\n\n    return {\n        \"condition\": None,\n        \"nodes\": [default_processor]\n    }\n</code></pre>"},{"location":"user-guide/dynamics-graph/#runtime-workflow-modification","title":"Runtime Workflow Modification","text":""},{"location":"user-guide/dynamics-graph/#dynamic-node-addition","title":"Dynamic Node Addition","text":"<pre><code>class DynamicWorkflowBuilder:\n    \"\"\"Builder for dynamic workflow modification.\"\"\"\n\n    def __init__(self, base_workflow=None):\n        if base_workflow:\n            self.workflow = base_workflow\n        else:\n            self.workflow = graphbit.Workflow(\"Dynamic Workflow\")\n        self.node_registry = {}\n\n    def add_processing_stage(self, stage_type, stage_config):\n        \"\"\"Add a processing stage dynamically.\"\"\"\n\n        if stage_type == \"validation\":\n            node = self._create_validation_node(stage_config)\n        elif stage_type == \"transformation\":\n            node = self._create_transformation_node(stage_config)\n        elif stage_type == \"analysis\":\n            node = self._create_analysis_node(stage_config)\n        elif stage_type == \"aggregation\":\n            node = self._create_aggregation_node(stage_config)\n        else:\n            node = self._create_generic_node(stage_config)\n\n        node_id = self.workflow.add_node(node)\n        self.node_registry[stage_config.get(\"name\", f\"node_{len(self.node_registry)}\")] = node_id\n\n        return node_id\n\n    def connect_stages(self, source_stage, target_stage):\n        \"\"\"Connect two stages dynamically.\"\"\"\n\n        source_id = self.node_registry.get(source_stage)\n        target_id = self.node_registry.get(target_stage)\n\n        if source_id and target_id:\n            self.workflow.connect(source_id, target_id)\n            return True\n        return False\n\n    def add_conditional_branch(self, condition_expr, true_stage, false_stage):\n        \"\"\"Add conditional branch to workflow.\"\"\"\n\n        condition = graphbit.Node.condition(\n            name=\"Dynamic Condition\",\n            expression=condition_expr\n        )\n\n        condition_id = self.workflow.add_node(condition)\n\n        # Connect to true and false branches\n        if true_stage in self.node_registry:\n            self.workflow.connect(condition_id, self.node_registry[true_stage])\n\n        if false_stage in self.node_registry:\n            self.workflow.connect(condition_id, self.node_registry[false_stage])\n\n        return condition_id\n\n    def _create_validation_node(self, config):\n        \"\"\"Create validation node.\"\"\"\n        return graphbit.Node.agent(\n            name=config.get(\"name\", \"Validator\"),\n            prompt=f\"Validate input according to rules: {config.get('rules', 'standard validation')} - Input: {{input}}\",\n            agent_id=config.get(\"agent_id\", \"validator\")\n        )\n\n    def _create_transformation_node(self, config):\n        \"\"\"Create transformation node.\"\"\"\n        transformation_type = config.get(\"transformation\", \"uppercase\")\n        return graphbit.Node.transform(\n            name=config.get(\"name\", \"Transformer\"),\n            transformation=transformation_type\n        )\n\n    def _create_analysis_node(self, config):\n        \"\"\"Create analysis node.\"\"\"\n        return graphbit.Node.agent(\n            name=config.get(\"name\", \"Analyzer\"),\n            prompt=f\"Analyze input for: {config.get('analysis_type', 'general analysis')} - Input: {{input}}\",\n            agent_id=config.get(\"agent_id\", \"analyzer\")\n        )\n\n    def _create_aggregation_node(self, config):\n        \"\"\"Create aggregation node.\"\"\"\n        return graphbit.Node.agent(\n            name=config.get(\"name\", \"Aggregator\"),\n            prompt=f\"Aggregate multiple inputs: {config.get('aggregation_method', 'combine all')} - Inputs: {{inputs}}\",\n            agent_id=config.get(\"agent_id\", \"aggregator\")\n        )\n\n    def _create_generic_node(self, config):\n        \"\"\"Create generic node.\"\"\"\n        return graphbit.Node.agent(\n            name=config.get(\"name\", \"Generic Node\"),\n            prompt=config.get(\"prompt\", \"Process input: {input}\"),\n            agent_id=config.get(\"agent_id\", \"generic\")\n        )\n\n    def get_workflow(self):\n        \"\"\"Get the built workflow.\"\"\"\n        return self.workflow\n\ndef create_dynamic_pipeline(pipeline_config):\n    \"\"\"Create a dynamic processing pipeline.\"\"\"\n\n    builder = DynamicWorkflowBuilder()\n\n    # Add stages from configuration\n    for stage in pipeline_config.get(\"stages\", []):\n        builder.add_processing_stage(stage[\"type\"], stage[\"config\"])\n\n    # Add connections from configuration\n    for connection in pipeline_config.get(\"connections\", []):\n        builder.connect_stages(connection[\"source\"], connection[\"target\"])\n\n    # Add conditional branches\n    for branch in pipeline_config.get(\"branches\", []):\n        builder.add_conditional_branch(\n            branch[\"condition\"],\n            branch[\"true_stage\"],\n            branch[\"false_stage\"]\n        )\n\n    return builder.get_workflow()\n\n# Example usage\ndef example_dynamic_pipeline():\n    \"\"\"Example of creating a dynamic pipeline.\"\"\"\n\n    pipeline_config = {\n        \"stages\": [\n            {\n                \"type\": \"validation\",\n                \"config\": {\n                    \"name\": \"input_validator\",\n                    \"rules\": \"check data completeness and format\",\n                    \"agent_id\": \"validator\"\n                }\n            },\n            {\n                \"type\": \"analysis\", \n                \"config\": {\n                    \"name\": \"content_analyzer\",\n                    \"analysis_type\": \"content quality and relevance\",\n                    \"agent_id\": \"content_analyzer\"\n                }\n            },\n            {\n                \"type\": \"transformation\",\n                \"config\": {\n                    \"name\": \"data_transformer\",\n                    \"transformation\": \"uppercase\"\n                }\n            },\n            {\n                \"type\": \"aggregation\",\n                \"config\": {\n                    \"name\": \"result_aggregator\",\n                    \"aggregation_method\": \"combine analysis and transformation results\",\n                    \"agent_id\": \"aggregator\"\n                }\n            }\n        ],\n        \"connections\": [\n            {\"source\": \"input_validator\", \"target\": \"content_analyzer\"},\n            {\"source\": \"content_analyzer\", \"target\": \"data_transformer\"},\n            {\"source\": \"data_transformer\", \"target\": \"result_aggregator\"}\n        ],\n        \"branches\": []\n    }\n\n    workflow = create_dynamic_pipeline(pipeline_config)\n    return workflow\n</code></pre>"},{"location":"user-guide/dynamics-graph/#adaptive-workflow-patterns","title":"Adaptive Workflow Patterns","text":""},{"location":"user-guide/dynamics-graph/#self-optimizing-workflows","title":"Self-Optimizing Workflows","text":"<pre><code>class AdaptiveWorkflow:\n    \"\"\"Workflow that adapts based on execution history.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n        self.workflow = graphbit.Workflow(name)\n        self.execution_history = []\n        self.performance_metrics = {}\n        self.optimization_rules = []\n\n    def add_optimization_rule(self, condition, action):\n        \"\"\"Add optimization rule.\"\"\"\n        self.optimization_rules.append({\n            \"condition\": condition,\n            \"action\": action\n        })\n\n    def execute_and_adapt(self, executor, input_data):\n        \"\"\"Execute workflow and adapt based on results.\"\"\"\n\n        import time\n\n        # Record execution start\n        start_time = time.time()\n\n        # Execute workflow\n        result = executor.execute(self.workflow)\n\n        # Record execution metrics\n        execution_time = (time.time() - start_time) * 1000\n\n        execution_record = {\n            \"timestamp\": time.time(),\n            \"execution_time_ms\": execution_time,\n            \"success\": result.is_completed(),\n            \"input_size\": len(str(input_data)) if input_data else 0,\n            \"output_size\": len(result.output()) if result.is_completed() else 0\n        }\n\n        self.execution_history.append(execution_record)\n\n        # Update performance metrics\n        self._update_performance_metrics()\n\n        # Apply optimization rules\n        self._apply_optimizations()\n\n        return result\n\n    def _update_performance_metrics(self):\n        \"\"\"Update performance metrics based on execution history.\"\"\"\n\n        if not self.execution_history:\n            return\n\n        recent_executions = self.execution_history[-10:]  # Last 10 executions\n\n        self.performance_metrics = {\n            \"average_execution_time\": sum(e[\"execution_time_ms\"] for e in recent_executions) / len(recent_executions),\n            \"success_rate\": sum(1 for e in recent_executions if e[\"success\"]) / len(recent_executions),\n            \"total_executions\": len(self.execution_history),\n            \"throughput\": len(recent_executions) / (recent_executions[-1][\"timestamp\"] - recent_executions[0][\"timestamp\"]) if len(recent_executions) &gt; 1 else 0\n        }\n\n    def _apply_optimizations(self):\n        \"\"\"Apply optimization rules based on current metrics.\"\"\"\n\n        for rule in self.optimization_rules:\n            if self._evaluate_condition(rule[\"condition\"]):\n                self._execute_action(rule[\"action\"])\n\n    def _evaluate_condition(self, condition):\n        \"\"\"Evaluate optimization condition.\"\"\"\n\n        metrics = self.performance_metrics\n\n        if condition[\"type\"] == \"performance_threshold\":\n            metric_value = metrics.get(condition[\"metric\"], 0)\n            return self._compare_values(metric_value, condition[\"operator\"], condition[\"threshold\"])\n\n        elif condition[\"type\"] == \"execution_count\":\n            return metrics.get(\"total_executions\", 0) &gt;= condition[\"count\"]\n\n        return False\n\n    def _compare_values(self, value, operator, threshold):\n        \"\"\"Compare values based on operator.\"\"\"\n\n        if operator == \"&gt;\":\n            return value &gt; threshold\n        elif operator == \"&lt;\":\n            return value &lt; threshold\n        elif operator == \"&gt;=\":\n            return value &gt;= threshold\n        elif operator == \"&lt;=\":\n            return value &lt;= threshold\n        elif operator == \"==\":\n            return value == threshold\n\n        return False\n\n    def _execute_action(self, action):\n        \"\"\"Execute optimization action.\"\"\"\n\n        if action[\"type\"] == \"add_caching_layer\":\n            self._add_caching_layer()\n        elif action[\"type\"] == \"add_parallel_processing\":\n            self._add_parallel_processing()\n        elif action[\"type\"] == \"optimize_prompts\":\n            self._optimize_prompts()\n\n    def _add_caching_layer(self):\n        \"\"\"Add caching layer to workflow.\"\"\"\n\n        cache_node = graphbit.Node.agent(\n            name=\"Cache Manager\",\n            prompt=\"Check cache for input: {input}. If found, return cached result, otherwise process normally.\",\n            agent_id=\"cache_manager\"\n        )\n\n        # Insert cache node at the beginning\n        cache_id = self.workflow.add_node(cache_node)\n        print(f\"Added caching layer to workflow {self.name}\")\n\n    def _add_parallel_processing(self):\n        \"\"\"Add parallel processing capability.\"\"\"\n\n        # Create parallel branch\n        parallel_processor = graphbit.Node.agent(\n            name=\"Parallel Processor\",\n            prompt=\"Process input in parallel: {input}\",\n            agent_id=\"parallel_proc\"\n        )\n\n        parallel_id = self.workflow.add_node(parallel_processor)\n        print(f\"Added parallel processing to workflow {self.name}\")\n\n    def _optimize_prompts(self):\n        \"\"\"Optimize prompts for better performance.\"\"\"\n\n        # This would involve modifying existing nodes with optimized prompts\n        print(f\"Optimized prompts for workflow {self.name}\")\n\ndef create_adaptive_text_processor():\n    \"\"\"Create an adaptive text processing workflow.\"\"\"\n\n    adaptive_workflow = AdaptiveWorkflow(\"Adaptive Text Processor\")\n\n    # Build initial workflow\n    processor = graphbit.Node.agent(\n        name=\"Text Processor\",\n        prompt=\"Process and analyze this text: {input}\",\n        agent_id=\"text_proc\"\n    )\n\n    adaptive_workflow.workflow.add_node(processor)\n\n    # Add optimization rules\n    adaptive_workflow.add_optimization_rule(\n        condition={\n            \"type\": \"performance_threshold\",\n            \"metric\": \"average_execution_time\",\n            \"operator\": \"&gt;\",\n            \"threshold\": 5000  # 5 seconds\n        },\n        action={\n            \"type\": \"add_caching_layer\"\n        }\n    )\n\n    adaptive_workflow.add_optimization_rule(\n        condition={\n            \"type\": \"execution_count\",\n            \"count\": 10\n        },\n        action={\n            \"type\": \"optimize_prompts\"\n        }\n    )\n\n    return adaptive_workflow\n</code></pre>"},{"location":"user-guide/dynamics-graph/#dynamic-workflow-templates","title":"Dynamic Workflow Templates","text":""},{"location":"user-guide/dynamics-graph/#template-based-generation","title":"Template-Based Generation","text":"<pre><code>class WorkflowTemplate:\n    \"\"\"Template for generating similar workflows.\"\"\"\n\n    def __init__(self, template_name):\n        self.template_name = template_name\n        self.template_structure = {}\n        self.parameter_mappings = {}\n\n    def define_template(self, structure, parameter_mappings):\n        \"\"\"Define workflow template structure.\"\"\"\n        self.template_structure = structure\n        self.parameter_mappings = parameter_mappings\n\n    def instantiate(self, parameters):\n        \"\"\"Create workflow instance from template.\"\"\"\n\n        workflow = graphbit.Workflow(f\"{self.template_name}_{parameters.get('instance_id', 'default')}\")\n\n        node_map = {}\n\n        # Create nodes from template\n        for node_config in self.template_structure.get(\"nodes\", []):\n            node = self._create_node_from_template(node_config, parameters)\n            node_id = workflow.add_node(node)\n            node_map[node_config[\"id\"]] = node_id\n\n        # Create connections from template\n        for connection in self.template_structure.get(\"connections\", []):\n            source_id = node_map.get(connection[\"source\"])\n            target_id = node_map.get(connection[\"target\"])\n\n            if source_id and target_id:\n                workflow.connect(source_id, target_id)\n\n        return workflow\n\n    def _create_node_from_template(self, node_config, parameters):\n        \"\"\"Create node from template configuration.\"\"\"\n\n        node_type = node_config.get(\"type\")\n\n        if node_type == \"agent\":\n            # Replace template parameters in prompt\n            prompt = node_config.get(\"prompt\", \"\")\n            for param, value in parameters.items():\n                prompt = prompt.replace(f\"${{{param}}}\", str(value))\n\n            return graphbit.Node.agent(\n                name=node_config.get(\"name\", \"Agent\"),\n                prompt=prompt,\n                agent_id=node_config.get(\"agent_id\", \"agent\")\n            )\n\n        elif node_type == \"transform\":\n            return graphbit.Node.transform(\n                name=node_config.get(\"name\", \"Transform\"),\n                transformation=node_config.get(\"transformation\", \"uppercase\")\n            )\n\n        elif node_type == \"condition\":\n            # Replace template parameters in expression\n            expression = node_config.get(\"expression\", \"true\")\n            for param, value in parameters.items():\n                expression = expression.replace(f\"${{{param}}}\", str(value))\n\n            return graphbit.Node.condition(\n                name=node_config.get(\"name\", \"Condition\"),\n                expression=expression\n            )\n\n        # Default to agent node\n        return graphbit.Node.agent(\n            name=\"Default Agent\",\n            prompt=\"Process input: {input}\",\n            agent_id=\"default\"\n        )\n\ndef create_data_processing_template():\n    \"\"\"Create a template for data processing workflows.\"\"\"\n\n    template = WorkflowTemplate(\"Data Processing Template\")\n\n    template_structure = {\n        \"nodes\": [\n            {\n                \"id\": \"validator\",\n                \"type\": \"agent\",\n                \"name\": \"${domain} Data Validator\",\n                \"prompt\": \"Validate ${domain} data according to ${validation_rules}: {input}\",\n                \"agent_id\": \"validator\"\n            },\n            {\n                \"id\": \"processor\",\n                \"type\": \"agent\", \n                \"name\": \"${domain} Processor\",\n                \"prompt\": \"Process ${domain} data using ${processing_method}: {validated_data}\",\n                \"agent_id\": \"processor\"\n            },\n            {\n                \"id\": \"quality_check\",\n                \"type\": \"condition\",\n                \"name\": \"Quality Gate\",\n                \"expression\": \"quality_score &gt;= ${quality_threshold}\"\n            },\n            {\n                \"id\": \"formatter\",\n                \"type\": \"transform\",\n                \"name\": \"Output Formatter\",\n                \"transformation\": \"${output_format}\"\n            }\n        ],\n        \"connections\": [\n            {\"source\": \"validator\", \"target\": \"processor\"},\n            {\"source\": \"processor\", \"target\": \"quality_check\"},\n            {\"source\": \"quality_check\", \"target\": \"formatter\"}\n        ]\n    }\n\n    parameter_mappings = {\n        \"domain\": \"Application domain (e.g., financial, medical, scientific)\",\n        \"validation_rules\": \"Specific validation rules for the domain\",\n        \"processing_method\": \"Method used for processing data\",\n        \"quality_threshold\": \"Minimum quality score threshold\",\n        \"output_format\": \"Format for output transformation\"\n    }\n\n    template.define_template(template_structure, parameter_mappings)\n\n    return template\n\ndef create_workflows_from_template():\n    \"\"\"Create multiple workflows from template.\"\"\"\n\n    template = create_data_processing_template()\n\n    # Financial data processing workflow\n    financial_workflow = template.instantiate({\n        \"instance_id\": \"financial\",\n        \"domain\": \"financial\",\n        \"validation_rules\": \"GAAP compliance and data integrity checks\",\n        \"processing_method\": \"financial analysis algorithms\",\n        \"quality_threshold\": \"0.95\",\n        \"output_format\": \"uppercase\"\n    })\n\n    # Medical data processing workflow\n    medical_workflow = template.instantiate({\n        \"instance_id\": \"medical\",\n        \"domain\": \"medical\",\n        \"validation_rules\": \"HIPAA compliance and medical data standards\",\n        \"processing_method\": \"clinical analysis procedures\",\n        \"quality_threshold\": \"0.98\",\n        \"output_format\": \"lowercase\"\n    })\n\n    return {\n        \"financial\": financial_workflow,\n        \"medical\": medical_workflow\n    }\n</code></pre>"},{"location":"user-guide/dynamics-graph/#configuration-driven-workflows","title":"Configuration-Driven Workflows","text":""},{"location":"user-guide/dynamics-graph/#json-based-workflow-definition","title":"JSON-Based Workflow Definition","text":"<pre><code>import json\n\ndef create_workflow_from_json(json_config):\n    \"\"\"Create workflow from JSON configuration.\"\"\"\n\n    if isinstance(json_config, str):\n        config = json.loads(json_config)\n    else:\n        config = json_config\n\n    workflow = graphbit.Workflow(config.get(\"name\", \"JSON Workflow\"))\n\n    node_map = {}\n\n    # Create nodes from configuration\n    for node_config in config.get(\"nodes\", []):\n        node = _create_node_from_json(node_config)\n        node_id = workflow.add_node(node)\n        node_map[node_config[\"id\"]] = node_id\n\n    # Create connections from configuration\n    for connection in config.get(\"connections\", []):\n        source_id = node_map.get(connection[\"source\"])\n        target_id = node_map.get(connection[\"target\"])\n\n        if source_id and target_id:\n            workflow.connect(source_id, target_id)\n\n    return workflow\n\ndef _create_node_from_json(node_config):\n    \"\"\"Create node from JSON configuration.\"\"\"\n\n    node_type = node_config.get(\"type\")\n\n    if node_type == \"agent\":\n        return graphbit.Node.agent(\n            name=node_config.get(\"name\", \"Agent\"),\n            prompt=node_config.get(\"prompt\", \"Process input: {input}\"),\n            agent_id=node_config.get(\"agent_id\", \"agent\")\n        )\n\n    elif node_type == \"transform\":\n        return graphbit.Node.transform(\n            name=node_config.get(\"name\", \"Transform\"),\n            transformation=node_config.get(\"transformation\", \"uppercase\")\n        )\n\n    elif node_type == \"condition\":\n        return graphbit.Node.condition(\n            name=node_config.get(\"name\", \"Condition\"),\n            expression=node_config.get(\"expression\", \"true\")\n        )\n\n    # Default to agent node\n    return graphbit.Node.agent(\n        name=\"Default Agent\",\n        prompt=\"Process input: {input}\",\n        agent_id=\"default\"\n    )\n\n# Example JSON configurations\ndef get_example_workflow_configs():\n    \"\"\"Get example workflow configurations.\"\"\"\n\n    simple_config = {\n        \"name\": \"Simple Analysis Workflow\",\n        \"nodes\": [\n            {\n                \"id\": \"analyzer\",\n                \"type\": \"agent\",\n                \"name\": \"Data Analyzer\",\n                \"prompt\": \"Analyze this data: {input}\",\n                \"agent_id\": \"analyzer\"\n            },\n            {\n                \"id\": \"formatter\",\n                \"type\": \"transform\",\n                \"name\": \"Output Formatter\",\n                \"transformation\": \"uppercase\"\n            }\n        ],\n        \"connections\": [\n            {\"source\": \"analyzer\", \"target\": \"formatter\"}\n        ]\n    }\n\n    complex_config = {\n        \"name\": \"Complex Processing Workflow\",\n        \"nodes\": [\n            {\n                \"id\": \"input_processor\",\n                \"type\": \"agent\",\n                \"name\": \"Input Processor\",\n                \"prompt\": \"Process and prepare input: {input}\",\n                \"agent_id\": \"input_proc\"\n            },\n            {\n                \"id\": \"quality_check\",\n                \"type\": \"condition\",\n                \"name\": \"Quality Gate\",\n                \"expression\": \"quality_score &gt; 0.8\"\n            },\n            {\n                \"id\": \"high_quality_processor\",\n                \"type\": \"agent\",\n                \"name\": \"High Quality Processor\",\n                \"prompt\": \"Process high-quality data: {processed_input}\",\n                \"agent_id\": \"hq_proc\"\n            },\n            {\n                \"id\": \"enhancement_processor\",\n                \"type\": \"agent\",\n                \"name\": \"Enhancement Processor\",\n                \"prompt\": \"Enhance and process lower-quality data: {processed_input}\",\n                \"agent_id\": \"enhancement_proc\"\n            },\n            {\n                \"id\": \"aggregator\",\n                \"type\": \"agent\",\n                \"name\": \"Result Aggregator\",\n                \"prompt\": \"Combine processing results: {results}\",\n                \"agent_id\": \"aggregator\"\n            }\n        ],\n        \"connections\": [\n            {\"source\": \"input_processor\", \"target\": \"quality_check\"},\n            {\"source\": \"quality_check\", \"target\": \"high_quality_processor\"},\n            {\"source\": \"quality_check\", \"target\": \"enhancement_processor\"},\n            {\"source\": \"high_quality_processor\", \"target\": \"aggregator\"},\n            {\"source\": \"enhancement_processor\", \"target\": \"aggregator\"}\n        ]\n    }\n\n    return {\n        \"simple\": simple_config,\n        \"complex\": complex_config\n    }\n</code></pre>"},{"location":"user-guide/dynamics-graph/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/dynamics-graph/#1-dynamic-workflow-design-principles","title":"1. Dynamic Workflow Design Principles","text":"<pre><code>def get_dynamic_workflow_best_practices():\n    \"\"\"Get best practices for dynamic workflow creation.\"\"\"\n\n    best_practices = {\n        \"modularity\": \"Design workflows with modular, reusable components\",\n        \"parameterization\": \"Use parameters and templates for flexibility\",\n        \"validation\": \"Always validate dynamically created workflows\",\n        \"performance\": \"Monitor and optimize dynamic workflow performance\",\n        \"maintainability\": \"Keep dynamic generation logic simple and readable\",\n        \"error_handling\": \"Implement robust error handling for dynamic creation\",\n        \"testing\": \"Thoroughly test dynamic workflows with various inputs\"\n    }\n\n    for practice, description in best_practices.items():\n        print(f\"\u2705 {practice.title()}: {description}\")\n\n    return best_practices\n</code></pre>"},{"location":"user-guide/dynamics-graph/#2-error-handling-and-validation","title":"2. Error Handling and Validation","text":"<pre><code>def validate_dynamic_workflow(workflow):\n    \"\"\"Validate dynamically created workflow.\"\"\"\n\n    try:\n        # Basic validation\n        workflow.validate()\n        print(\"\u2705 Dynamic workflow validation passed\")\n        return True\n\n    except Exception as e:\n        print(f\"\u274c Dynamic workflow validation failed: {e}\")\n        return False\n\ndef safe_dynamic_workflow_creation(creation_func, *args, **kwargs):\n    \"\"\"Safely create dynamic workflow with error handling.\"\"\"\n\n    try:\n        workflow = creation_func(*args, **kwargs)\n\n        if validate_dynamic_workflow(workflow):\n            return workflow\n        else:\n            raise ValueError(\"Dynamic workflow validation failed\")\n\n    except Exception as e:\n        print(f\"Error creating dynamic workflow: {e}\")\n\n        # Return a simple fallback workflow\n        fallback_workflow = graphbit.Workflow(\"Fallback Workflow\")\n        fallback_node = graphbit.Node.agent(\n            name=\"Fallback Processor\",\n            prompt=\"Process input safely: {input}\",\n            agent_id=\"fallback\"\n        )\n        fallback_workflow.add_node(fallback_node)\n\n        return fallback_workflow\n</code></pre>"},{"location":"user-guide/dynamics-graph/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/dynamics-graph/#complete-dynamic-workflow-example","title":"Complete Dynamic Workflow Example","text":"<pre><code>def example_complete_dynamic_workflow():\n    \"\"\"Complete example of dynamic workflow creation and execution.\"\"\"\n\n    # Initialize GraphBit\n    graphbit.init()\n\n    # Create dynamic workflow based on input\n    input_data = {\n        \"type\": \"mixed\",\n        \"content\": \"Sample text with numerical data: 123, 456\",\n        \"requirements\": [\"quality_check\", \"fast_processing\"]\n    }\n\n    # Create workflow dynamically\n    workflow = create_dynamic_workflow(input_data)\n\n    # Validate the workflow\n    if validate_dynamic_workflow(workflow):\n        print(\"\u2705 Dynamic workflow created and validated successfully\")\n\n        # Create executor\n        llm_config = graphbit.LlmConfig.openai(\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n            model=\"gpt-4o-mini\"\n        )\n        executor = graphbit.Executor(llm_config)\n\n        # Execute workflow\n        result = executor.execute(workflow)\n\n        if result.is_completed():\n            print(f\"\u2705 Dynamic workflow executed successfully\")\n            print(f\"Output: {result.output()}\")\n        else:\n            print(f\"\u274c Dynamic workflow execution failed: {result.error()}\")\n\n    else:\n        print(\"\u274c Dynamic workflow validation failed\")\n\nif __name__ == \"__main__\":\n    example_complete_dynamic_workflow()\n</code></pre>"},{"location":"user-guide/dynamics-graph/#whats-next","title":"What's Next","text":"<ul> <li>Learn about Performance optimization for dynamic workflows</li> <li>Explore Monitoring for tracking dynamic workflow execution</li> <li>Check Validation for comprehensive dynamic workflow testing</li> <li>See Workflow Builder for static workflow patterns </li> </ul>"},{"location":"user-guide/embeddings/","title":"Embeddings","text":"<p>GraphBit provides vector embedding capabilities for semantic search, similarity analysis, and other AI-powered text operations. This guide covers configuration and usage for working with embeddings.</p>"},{"location":"user-guide/embeddings/#overview","title":"Overview","text":"<p>GraphBit's embedding system supports: - Multiple Providers - OpenAI and HuggingFace embedding models - Unified Interface - Consistent API across all providers - Batch Processing - Efficient processing of multiple texts - Similarity Calculations - Built-in cosine similarity functions</p>"},{"location":"user-guide/embeddings/#configuration","title":"Configuration","text":""},{"location":"user-guide/embeddings/#openai-configuration","title":"OpenAI Configuration","text":"<p>Configure OpenAI embedding provider:</p> <pre><code>import graphbit\nimport os\n\n# Initialize GraphBit\ngraphbit.init()\n\n# Basic OpenAI configuration\nembedding_config = graphbit.EmbeddingConfig.openai(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    model=\"text-embedding-3-small\"  # Optional - defaults to text-embedding-3-small\n)\n\nprint(f\"Provider: OpenAI\")\nprint(f\"Model: {embedding_config.model}\")\n</code></pre>"},{"location":"user-guide/embeddings/#huggingface-configuration","title":"HuggingFace Configuration","text":"<p>Configure HuggingFace embedding provider:</p> <pre><code># HuggingFace configuration\nembedding_config = graphbit.EmbeddingConfig.huggingface(\n    api_key=os.getenv(\"HUGGINGFACE_API_KEY\"),\n    model=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n\nprint(f\"Provider: HuggingFace\")\nprint(f\"Model: {embedding_config.model}\")\n</code></pre>"},{"location":"user-guide/embeddings/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/embeddings/#creating-embedding-client","title":"Creating Embedding Client","text":"<pre><code># Create embedding client\nembedding_client = graphbit.EmbeddingClient(embedding_config)\n</code></pre>"},{"location":"user-guide/embeddings/#single-text-embedding","title":"Single Text Embedding","text":"<p>Generate embeddings for individual texts:</p> <pre><code># Embed single text\ntext = \"GraphBit is a powerful framework for AI agent workflows\"\nvector = embedding_client.embed(text)\n\nprint(f\"Text: {text}\")\nprint(f\"Vector dimension: {len(vector)}\")\nprint(f\"First 5 values: {vector[:5]}\")\n</code></pre>"},{"location":"user-guide/embeddings/#batch-text-embeddings","title":"Batch Text Embeddings","text":"<p>Process multiple texts efficiently:</p> <pre><code># Embed multiple texts\ntexts = [\n    \"Machine learning is transforming industries\",\n    \"Natural language processing enables computers to understand text\", \n    \"Deep learning models require large datasets\",\n    \"AI ethics is becoming increasingly important\",\n    \"Transformer architectures revolutionized NLP\"\n]\n\nvectors = embedding_client.embed_many(texts)\n\nprint(f\"Generated {len(vectors)} embeddings\")\nfor i, (text, vector) in enumerate(zip(texts, vectors)):\n    print(f\"Text {i+1}: {text[:50]}...\")\n    print(f\"Vector dimension: {len(vector)}\")\n</code></pre>"},{"location":"user-guide/embeddings/#similarity-calculations","title":"Similarity Calculations","text":""},{"location":"user-guide/embeddings/#cosine-similarity","title":"Cosine Similarity","text":"<p>Calculate similarity between vectors:</p> <pre><code># Generate embeddings for comparison\ntext1 = \"Artificial intelligence and machine learning\"\ntext2 = \"AI and ML technologies\"\n\nvector1 = embedding_client.embed(text1)\nvector2 = embedding_client.embed(text2)\n\n# Calculate similarities\nsimilarity_1_2 = graphbit.EmbeddingClient.similarity(vector1, vector2)\n\nprint(f\"Similarity between text1 and text2: {similarity_1_2:.3f}\")\n</code></pre>"},{"location":"user-guide/embeddings/#finding-most-similar-texts","title":"Finding Most Similar Texts","text":"<pre><code>def find_most_similar(query_text, candidate_texts, embedding_client, threshold=0.7):\n    \"\"\"Find most similar texts to a query\"\"\"\n    query_vector = embedding_client.embed(query_text)\n    candidate_vectors = embedding_client.embed_many(candidate_texts)\n\n    similarities = []\n    for i, candidate_vector in enumerate(candidate_vectors):\n        similarity = graphbit.EmbeddingClient.similarity(query_vector, candidate_vector)\n        similarities.append((i, candidate_texts[i], similarity))\n\n    # Sort by similarity (highest first)\n    similarities.sort(key=lambda x: x[2], reverse=True)\n\n    # Filter by threshold\n    results = [(text, sim) for _, text, sim in similarities if sim &gt;= threshold]\n\n    return results\n\n# Example usage\nquery = \"machine learning algorithms\"\ncandidates = [\n    \"Deep learning neural networks\",\n    \"Supervised learning models\",\n    \"Recipe for chocolate cake\",\n    \"Natural language processing\",\n    \"Computer vision techniques\",\n    \"Sports news update\"\n]\n\nsimilar_texts = find_most_similar(query, candidates, embedding_client, threshold=0.5)\n\nprint(f\"Query: {query}\")\nprint(\"Most similar texts:\")\nfor text, similarity in similar_texts:\n    print(f\"- {text} (similarity: {similarity:.3f})\")\n</code></pre>"},{"location":"user-guide/embeddings/#whats-next","title":"What's Next","text":"<ul> <li>Learn about Performance for optimization techniques</li> <li>Explore Monitoring for production monitoring  </li> <li>Check Validation for input validation strategies</li> <li>See LLM Providers for language model integration</li> </ul>"},{"location":"user-guide/llm-providers/","title":"LLM Providers","text":"<p>GraphBit supports multiple Large Language Model providers through a unified client interface. This guide covers configuration, usage, and optimization for each supported provider.</p>"},{"location":"user-guide/llm-providers/#supported-providers","title":"Supported Providers","text":"<p>GraphBit supports these LLM providers: - OpenAI - GPT models including GPT-4o, GPT-4o-mini - Anthropic - Claude models including Claude-3.5-Sonnet - Perplexity - Real-time search-enabled models including Sonar models ======= - Anthropic - Claude models including Claude-3.5-Sonnet - DeepSeek - High-performance models including DeepSeek-Chat, DeepSeek-Coder, and DeepSeek-Reasoner - HuggingFace - Access to thousands of models via HuggingFace Inference API - Ollama - Local model execution with various open-source models</p>"},{"location":"user-guide/llm-providers/#configuration","title":"Configuration","text":""},{"location":"user-guide/llm-providers/#openai-configuration","title":"OpenAI Configuration","text":"<p>Configure OpenAI provider with API key and model selection:</p> <pre><code>import graphbit\nimport os\n\n# Basic OpenAI configuration\nconfig = graphbit.LlmConfig.openai(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    model=\"gpt-4o-mini\"  # Optional - defaults to gpt-4o-mini\n)\n\n# Access configuration details\nprint(f\"Provider: {config.provider()}\")  # \"OpenAI\"\nprint(f\"Model: {config.model()}\")        # \"gpt-4o-mini\"\n</code></pre>"},{"location":"user-guide/llm-providers/#available-openai-models","title":"Available OpenAI Models","text":"Model Best For Context Length Performance <code>gpt-4o</code> Complex reasoning, latest features 128K High quality, slower <code>gpt-4o-mini</code> Balanced performance and cost 128K Good quality, faster <code>gpt-4-turbo</code> High-quality outputs 128K High quality <code>gpt-3.5-turbo</code> Fast, cost-effective tasks 16K Fast, economical <pre><code># Model selection examples\ncreative_config = graphbit.LlmConfig.openai(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    model=\"gpt-4o\"  # For creative and complex tasks\n)\n\nproduction_config = graphbit.LlmConfig.openai(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    model=\"gpt-4o-mini\"  # Balanced for production\n)\n\nfast_config = graphbit.LlmConfig.openai(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    model=\"gpt-3.5-turbo\"  # For high-volume, simple tasks\n)\n</code></pre>"},{"location":"user-guide/llm-providers/#anthropic-configuration","title":"Anthropic Configuration","text":"<p>Configure Anthropic provider for Claude models:</p> <pre><code># Basic Anthropic configuration\nconfig = graphbit.LlmConfig.anthropic(\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n    model=\"claude-3-5-sonnet-20241022\"  # Optional - defaults to claude-3-5-sonnet-20241022\n)\n\nprint(f\"Provider: {config.provider()}\")  # \"Anthropic\"\nprint(f\"Model: {config.model()}\")        # \"claude-3-5-sonnet-20241022\"\n</code></pre>"},{"location":"user-guide/llm-providers/#available-anthropic-models","title":"Available Anthropic Models","text":"Model Best For Context Length Speed <code>claude-3-opus-20240229</code> Most capable, complex analysis 200K Slowest, highest quality <code>claude-3-sonnet-20240229</code> Balanced performance 200K Medium speed and quality <code>claude-3-haiku-20240307</code> Fast, cost-effective 200K Fastest, good quality <code>claude-3-5-sonnet-20241022</code> Latest, improved reasoning 200K Good speed, high quality <pre><code># Model selection for different use cases\ncomplex_config = graphbit.LlmConfig.anthropic(\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n    model=\"claude-3-opus-20240229\"  # For complex analysis\n)\n\nbalanced_config = graphbit.LlmConfig.anthropic(\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n    model=\"claude-3-5-sonnet-20241022\"  # For balanced workloads\n)\n\nfast_config = graphbit.LlmConfig.anthropic(\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n    model=\"claude-3-haiku-20240307\"  # For speed and efficiency\n)\n</code></pre>"},{"location":"user-guide/llm-providers/#perplexity-configuration","title":"Perplexity Configuration","text":"<p>Configure Perplexity provider to access real-time search-enabled models:</p> <pre><code># Basic Perplexity configuration\nconfig = graphbit.LlmConfig.perplexity(\n    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n    model=\"sonar\"  # Optional - defaults to sonar\n)\n\nprint(f\"Provider: {config.provider()}\")  # \"perplexity\"\nprint(f\"Model: {config.model()}\")        # \"sonar\"\n</code></pre>"},{"location":"user-guide/llm-providers/#available-perplexity-models","title":"Available Perplexity Models","text":"Model Best For Context Length Special Features <code>sonar</code> General purpose with search 8K Real-time web search, citations <code>sonar-reasoning</code> Complex reasoning with search 8K Multi-step reasoning, web research <code>sonar-deep-research</code> Comprehensive research 32K Exhaustive research, detailed analysis <code>pplx-7b-online</code> Fast online inference 4K Quick responses with web data <code>pplx-70b-online</code> High-quality online inference 4K Better quality with web data <code>pplx-7b-chat</code> General chat without search 8K Standard chat functionality <code>pplx-70b-chat</code> High-quality chat 8K Better chat quality <pre><code># Model selection for different use cases\nresearch_config = graphbit.LlmConfig.perplexity(\n    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n    model=\"sonar-deep-research\"  # For comprehensive research\n)\n\nreasoning_config = graphbit.LlmConfig.perplexity(\n    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n    model=\"sonar-reasoning\"  # For complex problem solving\n)\n\nfast_search_config = graphbit.LlmConfig.perplexity(\n    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n    model=\"pplx-7b-online\"  # For fast web-enabled responses\n)\n\nchat_config = graphbit.LlmConfig.perplexity(\n    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n    model=\"pplx-70b-chat\"  # For high-quality chat without search\n)\n=======\n### DeepSeek Configuration\n\nConfigure DeepSeek provider for high-performance, cost-effective AI models:\n\n```python\n# Basic DeepSeek configuration\nconfig = graphbit.LlmConfig.deepseek(\n    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n    model=\"deepseek-chat\"  # Optional - defaults to deepseek-chat\n)\n\nprint(f\"Provider: {config.provider()}\")  # \"deepseek\"\nprint(f\"Model: {config.model()}\")        # \"deepseek-chat\"\n</code></pre>"},{"location":"user-guide/llm-providers/#available-deepseek-models","title":"Available DeepSeek Models","text":"Model Best For Context Length Performance Cost <code>deepseek-chat</code> General conversation, instruction following 128K High quality, fast $0.14/$0.28 per 1M tokens <code>deepseek-coder</code> Code generation, programming tasks 128K Specialized for code $0.14/$0.28 per 1M tokens <code>deepseek-reasoner</code> Complex reasoning, mathematics 128K Advanced reasoning $0.55/$2.19 per 1M tokens"},{"location":"user-guide/llm-providers/#key-features","title":"Key Features","text":"<ul> <li>Cost-Effective: Among the most competitive pricing in the market</li> <li>Function Calling: Full support for tool/function calling</li> <li>Large Context: 128K token context window for all models</li> <li>OpenAI Compatible: Uses OpenAI-compatible API format</li> <li>High Performance: Optimized for speed and quality</li> </ul> <pre><code># Model selection for different use cases\ngeneral_config = graphbit.LlmConfig.deepseek(\n    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n    model=\"deepseek-chat\"  # For general tasks and conversation\n)\n\ncoding_config = graphbit.LlmConfig.deepseek(\n    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n    model=\"deepseek-coder\"  # For code generation and programming\n)\n\nreasoning_config = graphbit.LlmConfig.deepseek(\n    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n    model=\"deepseek-reasoner\"  # For complex reasoning tasks\n)\n</code></pre>"},{"location":"user-guide/llm-providers/#deepseek-api-key-setup","title":"DeepSeek API Key Setup","text":"<p>To use DeepSeek models, you need an API key:</p> <ol> <li>Create an account at DeepSeek</li> <li>Generate an API key in your dashboard</li> <li>Set the environment variable:</li> </ol> <pre><code>export DEEPSEEK_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"user-guide/llm-providers/#huggingface-configuration","title":"HuggingFace Configuration","text":"<p>Configure HuggingFace provider to access thousands of models via the Inference API:</p> <pre><code># Basic HuggingFace configuration\nconfig = graphbit.LlmConfig.huggingface(\n    api_key=os.getenv(\"HUGGINGFACE_API_KEY\"),\n    model=\"microsoft/DialoGPT-medium\"  # Optional - defaults to microsoft/DialoGPT-medium\n)\n\nprint(f\"Provider: {config.provider()}\")  # \"huggingface\"\nprint(f\"Model: {config.model()}\")        # \"microsoft/DialoGPT-medium\"\n\n# Custom endpoint configuration\ncustom_config = graphbit.LlmConfig.huggingface(\n    api_key=os.getenv(\"HUGGINGFACE_API_KEY\"),\n    model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n    base_url=\"https://my-custom-endpoint.huggingface.co\"  # Optional custom endpoint\n)\n</code></pre>"},{"location":"user-guide/llm-providers/#popular-huggingface-models","title":"Popular HuggingFace Models","text":"Model Best For Size Performance <code>microsoft/DialoGPT-medium</code> Conversational AI, chat 345M Fast, good dialogue <code>mistralai/Mistral-7B-Instruct-v0.1</code> General instruction following 7B High quality, versatile <code>microsoft/CodeBERT-base</code> Code understanding 125M Specialized for code <code>facebook/blenderbot-400M-distill</code> Conversational AI 400M Balanced dialogue <code>huggingface/CodeBERTa-small-v1</code> Code generation 84M Fast code tasks <code>microsoft/DialoGPT-large</code> Advanced dialogue 762M Higher quality chat <pre><code># Model selection for different use cases\ndialogue_config = graphbit.LlmConfig.huggingface(\n    api_key=os.getenv(\"HUGGINGFACE_API_KEY\"),\n    model=\"microsoft/DialoGPT-large\"  # For high-quality dialogue\n)\n\ninstruction_config = graphbit.LlmConfig.huggingface(\n    api_key=os.getenv(\"HUGGINGFACE_API_KEY\"),\n    model=\"mistralai/Mistral-7B-Instruct-v0.1\"  # For instruction following\n)\n\ncode_config = graphbit.LlmConfig.huggingface(\n    api_key=os.getenv(\"HUGGINGFACE_API_KEY\"),\n    model=\"microsoft/CodeBERT-base\"  # For code-related tasks\n)\n\n# Fast and lightweight option\nlightweight_config = graphbit.LlmConfig.huggingface(\n    api_key=os.getenv(\"HUGGINGFACE_API_KEY\"),\n    model=\"microsoft/DialoGPT-medium\"  # Balanced performance\n)\n</code></pre>"},{"location":"user-guide/llm-providers/#huggingface-api-key-setup","title":"HuggingFace API Key Setup","text":"<p>To use HuggingFace models, you need an API key:</p> <ol> <li>Create an account at HuggingFace</li> <li>Generate an API token in your settings</li> <li>Set the environment variable:</li> </ol> <pre><code>export HUGGINGFACE_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"user-guide/llm-providers/#model-selection-tips","title":"Model Selection Tips","text":"<ul> <li>Free Tier: Most models work with free HuggingFace accounts</li> <li>Custom Models: You can use any public model from the HuggingFace Hub</li> <li>Private Models: Use your own fine-tuned models with appropriate permissions</li> <li>Performance: Larger models (7B+) provide better quality but slower responses</li> <li>Cost: HuggingFace Inference API has competitive pricing for hosted inference</li> </ul>"},{"location":"user-guide/llm-providers/#ollama-configuration","title":"Ollama Configuration","text":"<p>Configure Ollama for local model execution:</p> <pre><code># Basic Ollama configuration (no API key required)\nconfig = graphbit.LlmConfig.ollama(\n    model=\"llama3.2\"  # Optional - defaults to llama3.2\n)\n\nprint(f\"Provider: {config.provider()}\")  # \"Ollama\"\nprint(f\"Model: {config.model()}\")        # \"llama3.2\"\n\n# Other popular models\nmistral_config = graphbit.LlmConfig.ollama(model=\"mistral\")\ncodellama_config = graphbit.LlmConfig.ollama(model=\"codellama\")\nphi_config = graphbit.LlmConfig.ollama(model=\"phi\")\n</code></pre>"},{"location":"user-guide/llm-providers/#llm-client-usage","title":"LLM Client Usage","text":""},{"location":"user-guide/llm-providers/#creating-and-using-clients","title":"Creating and Using Clients","text":"<pre><code># Create client with configuration\nclient = graphbit.LlmClient(config, debug=False)\n\n# Basic text completion\nresponse = client.complete(\n    prompt=\"Explain the concept of machine learning\",\n    max_tokens=500,     # Optional - controls response length\n    temperature=0.7     # Optional - controls randomness (0.0-1.0)\n)\n\nprint(f\"Response: {response}\")\n</code></pre>"},{"location":"user-guide/llm-providers/#asynchronous-operations","title":"Asynchronous Operations","text":"<p>GraphBit provides async methods for non-blocking operations:</p> <pre><code>import asyncio\n\nasync def async_completion():\n    # Async completion\n    response = await client.complete_async(\n        prompt=\"Write a short story about AI\",\n        max_tokens=300,\n        temperature=0.8\n    )\n    return response\n\n# Run async operation\nresponse = asyncio.run(async_completion())\n</code></pre>"},{"location":"user-guide/llm-providers/#batch-processing","title":"Batch Processing","text":"<p>Process multiple prompts efficiently:</p> <pre><code>async def batch_processing():\n    prompts = [\n        \"Summarize quantum computing\",\n        \"Explain blockchain technology\", \n        \"Describe neural networks\",\n        \"What is machine learning?\"\n    ]\n\n    responses = await client.complete_batch(\n        prompts=prompts,\n        max_tokens=200,\n        temperature=0.5,\n        max_concurrency=3  # Process 3 at a time\n    )\n\n    for i, response in enumerate(responses):\n        print(f\"Response {i+1}: {response}\")\n\nasyncio.run(batch_processing())\n</code></pre>"},{"location":"user-guide/llm-providers/#chat-style-interactions","title":"Chat-Style Interactions","text":"<p>Use chat-optimized methods for conversational interactions:</p> <pre><code>async def chat_example():\n    # Chat with message history\n    response = await client.chat_optimized(\n        messages=[\n            (\"user\", \"Hello, how are you?\"),\n            (\"assistant\", \"I'm doing well, thank you!\"),\n            (\"user\", \"Can you help me with Python programming?\"),\n            (\"user\", \"Specifically, how do I handle exceptions?\")\n        ],\n        max_tokens=400,\n        temperature=0.3\n    )\n\n    print(f\"Chat response: {response}\")\n\nasyncio.run(chat_example())\n</code></pre>"},{"location":"user-guide/llm-providers/#streaming-responses","title":"Streaming Responses","text":"<p>Get real-time streaming responses:</p> <pre><code>async def streaming_example():\n    print(\"Streaming response:\")\n\n    async for chunk in client.complete_stream(\n        prompt=\"Tell me a detailed story about space exploration\",\n        max_tokens=1000,\n        temperature=0.7\n    ):\n        print(chunk, end=\"\", flush=True)\n\n    print(\"\\n--- Stream complete ---\")\n\nasyncio.run(streaming_example())\n</code></pre>"},{"location":"user-guide/llm-providers/#client-management-and-monitoring","title":"Client Management and Monitoring","text":""},{"location":"user-guide/llm-providers/#client-statistics","title":"Client Statistics","text":"<p>Monitor client performance and usage:</p> <pre><code># Get comprehensive statistics\nstats = client.get_stats()\n\nprint(f\"Total requests: {stats['total_requests']}\")\nprint(f\"Successful requests: {stats['successful_requests']}\")\nprint(f\"Failed requests: {stats['failed_requests']}\")\nprint(f\"Average response time: {stats['average_response_time_ms']}ms\")\nprint(f\"Circuit breaker state: {stats['circuit_breaker_state']}\")\nprint(f\"Client uptime: {stats['uptime']}\")\n\n# Calculate success rate\nif stats['total_requests'] &gt; 0:\n    success_rate = stats['successful_requests'] / stats['total_requests']\n    print(f\"Success rate: {success_rate:.2%}\")\n</code></pre>"},{"location":"user-guide/llm-providers/#client-warmup","title":"Client Warmup","text":"<p>Pre-initialize connections for better performance:</p> <pre><code>async def warmup_client():\n    # Warmup client to reduce cold start latency\n    await client.warmup()\n    print(\"Client warmed up and ready\")\n\n# Warmup before production use\nasyncio.run(warmup_client())\n</code></pre>"},{"location":"user-guide/llm-providers/#reset-statistics","title":"Reset Statistics","text":"<p>Reset client statistics for monitoring periods:</p> <pre><code># Reset statistics\nclient.reset_stats()\nprint(\"Client statistics reset\")\n</code></pre>"},{"location":"user-guide/llm-providers/#provider-specific-examples","title":"Provider-Specific Examples","text":""},{"location":"user-guide/llm-providers/#openai-workflow-example","title":"OpenAI Workflow Example","text":"<pre><code>def create_openai_workflow():\n    \"\"\"Create workflow using OpenAI\"\"\"\n    # Initialize GraphBit\n    graphbit.init()\n\n    # Configure OpenAI\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    # Create workflow\n    workflow = graphbit.Workflow(\"OpenAI Analysis Pipeline\")\n\n    # Create analyzer node\n    analyzer = graphbit.Node.agent(\n        name=\"GPT Content Analyzer\",\n        prompt=\"Analyze the following content for sentiment, key themes, and quality:\\n\\n{input}\",\n        agent_id=\"gpt_analyzer\"\n    )\n\n    # Add to workflow\n    analyzer_id = workflow.add_node(analyzer)\n    workflow.validate()\n\n    # Create executor and run\n    executor = graphbit.Executor(config, timeout_seconds=60)\n    return workflow, executor\n\n# Usage\nworkflow, executor = create_openai_workflow()\nresult = executor.execute(workflow)\n</code></pre>"},{"location":"user-guide/llm-providers/#anthropic-workflow-example","title":"Anthropic Workflow Example","text":"<pre><code>def create_anthropic_workflow():\n    \"\"\"Create workflow using Anthropic Claude\"\"\"\n    graphbit.init()\n\n    # Configure Anthropic\n    config = graphbit.LlmConfig.anthropic(\n        api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n        model=\"claude-3-5-sonnet-20241022\"\n    )\n\n    # Create workflow\n    workflow = graphbit.Workflow(\"Claude Analysis Pipeline\")\n\n    # Create analyzer with detailed prompt\n    analyzer = graphbit.Node.agent(\n        name=\"Claude Content Analyzer\",\n        prompt=\"\"\"\n        Analyze the following content with attention to:\n        - Factual accuracy and logical consistency\n        - Potential biases or assumptions\n        - Clarity and structure\n        - Key insights and recommendations\n\n        Content: {input}\n\n        Provide your analysis in a structured format.\n        \"\"\",\n        agent_id=\"claude_analyzer\"\n    )\n\n    workflow.add_node(analyzer)\n    workflow.validate()\n\n    # Create executor with longer timeout for Claude\n    executor = graphbit.Executor(config, timeout_seconds=120)\n    return workflow, executor\n\n# Usage\nworkflow, executor = create_anthropic_workflow()\n</code></pre>"},{"location":"user-guide/llm-providers/#deepseek-workflow-example","title":"DeepSeek Workflow Example","text":"<pre><code>def create_deepseek_workflow():\n    \"\"\"Create workflow using DeepSeek models\"\"\"\n    graphbit.init()\n\n    # Configure DeepSeek\n    config = graphbit.LlmConfig.deepseek(\n        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n        model=\"deepseek-chat\"\n    )\n\n    # Create workflow\n    workflow = graphbit.Workflow(\"DeepSeek Analysis Pipeline\")\n\n    # Create analyzer optimized for DeepSeek's capabilities\n    analyzer = graphbit.Node.agent(\n        name=\"DeepSeek Content Analyzer\",\n        prompt=\"\"\"\n        Analyze the following content efficiently and accurately:\n        - Main topics and themes\n        - Key insights and takeaways\n        - Actionable recommendations\n        - Potential concerns or limitations\n\n        Content: {input}\n\n        Provide a clear, structured analysis.\n        \"\"\",\n        agent_id=\"deepseek_analyzer\"\n    )\n\n    workflow.add_node(analyzer)\n    workflow.validate()\n\n    # Create executor optimized for DeepSeek's fast inference\n    executor = graphbit.Executor(config, timeout_seconds=90)\n    return workflow, executor\n\n# Usage for different DeepSeek models\ndef create_deepseek_coding_workflow():\n    \"\"\"Create workflow for code analysis using DeepSeek Coder\"\"\"\n    graphbit.init()\n\n    config = graphbit.LlmConfig.deepseek(\n        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n        model=\"deepseek-coder\"\n    )\n\n    workflow = graphbit.Workflow(\"DeepSeek Code Analysis\")\n\n    code_analyzer = graphbit.Node.agent(\n        name=\"DeepSeek Code Reviewer\",\n        prompt=\"\"\"\n        Review this code for:\n        - Code quality and best practices\n        - Potential bugs or issues\n        - Performance improvements\n        - Security considerations\n\n        Code: {input}\n        \"\"\",\n        agent_id=\"deepseek_code_analyzer\"\n    )\n\n    workflow.add_node(code_analyzer)\n    workflow.validate()\n\n    executor = graphbit.Executor(config, timeout_seconds=90)\n    return workflow, executor\n\n# Usage\nworkflow, executor = create_deepseek_workflow()\n</code></pre>"},{"location":"user-guide/llm-providers/#ollama-workflow-example","title":"Ollama Workflow Example","text":"<pre><code>def create_ollama_workflow():\n    \"\"\"Create workflow using local Ollama models\"\"\"\n    graphbit.init()\n\n    # Configure Ollama (no API key needed)\n    config = graphbit.LlmConfig.ollama(model=\"llama3.2\")\n\n    # Create workflow\n    workflow = graphbit.Workflow(\"Local LLM Pipeline\")\n\n    # Create analyzer optimized for local models\n    analyzer = graphbit.Node.agent(\n        name=\"Local Model Analyzer\",\n        prompt=\"Analyze this text briefly: {input}\",\n        agent_id=\"local_analyzer\"\n    )\n\n    workflow.add_node(analyzer)\n    workflow.validate()\n\n    # Create executor with longer timeout for local processing\n    executor = graphbit.Executor(config, timeout_seconds=180)\n    return workflow, executor\n\n# Usage\nworkflow, executor = create_ollama_workflow()\n</code></pre>"},{"location":"user-guide/llm-providers/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/llm-providers/#timeout-configuration","title":"Timeout Configuration","text":"<p>Configure appropriate timeouts for different providers:</p> <pre><code># OpenAI - typically faster\nopenai_executor = graphbit.Executor(\n    openai_config, \n    timeout_seconds=60\n)\n\n\nanthropic_executor = graphbit.Executor(\n    anthropic_config, \n    timeout_seconds=120\n)\n\ndeepseek_executor = graphbit.Executor(\n    deepseek_config, \n    timeout_seconds=90\n)\n\n\nollama_executor = graphbit.Executor(\n    ollama_config, \n    timeout_seconds=180\n)\n</code></pre>"},{"location":"user-guide/llm-providers/#executor-types-for-different-providers","title":"Executor Types for Different Providers","text":"<p>Choose appropriate executor types based on provider characteristics:</p> <pre><code># High-throughput for cloud providers\ncloud_executor = graphbit.Executor.new_high_throughput(\n    llm_config=openai_config,\n    timeout_seconds=60\n)\n\n# Low-latency for fast providers\nrealtime_executor = graphbit.Executor.new_low_latency(\n    llm_config=anthropic_config,\n    timeout_seconds=30\n)\n\n# Memory-optimized for local models\nlocal_executor = graphbit.Executor.new_memory_optimized(\n    llm_config=ollama_config,\n    timeout_seconds=180\n)\n</code></pre>"},{"location":"user-guide/llm-providers/#error-handling","title":"Error Handling","text":""},{"location":"user-guide/llm-providers/#provider-specific-error-handling","title":"Provider-Specific Error Handling","text":"<pre><code>def robust_llm_usage():\n    try:\n        # Initialize and configure\n        graphbit.init()\n        config = graphbit.LlmConfig.openai(\n            api_key=os.getenv(\"OPENAI_API_KEY\")\n        )\n        client = graphbit.LlmClient(config)\n\n        # Execute with error handling\n        response = client.complete(\n            prompt=\"Test prompt\",\n            max_tokens=100\n        )\n\n        return response\n\n    except Exception as e:\n        print(f\"LLM operation failed: {e}\")\n        return None\n</code></pre>"},{"location":"user-guide/llm-providers/#workflow-error-handling","title":"Workflow Error Handling","text":"<pre><code>def execute_with_error_handling(workflow, executor):\n    try:\n        result = executor.execute(workflow)\n\n        if result.is_completed():\n            return result.output()\n        elif result.is_failed():\n            error_msg = result.error()\n            print(f\"Workflow failed: {error_msg}\")\n            return None\n\n    except Exception as e:\n        print(f\"Execution error: {e}\")\n        return None\n</code></pre>"},{"location":"user-guide/llm-providers/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/llm-providers/#1-provider-selection","title":"1. Provider Selection","text":"<p>Choose providers based on your requirements:</p> <pre><code>def get_optimal_config(use_case):\n    \"\"\"Select optimal provider for use case\"\"\"\n    if use_case == \"creative\":\n        return graphbit.LlmConfig.openai(\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n            model=\"gpt-4o\"\n        )\n    elif use_case == \"analytical\":\n        return graphbit.LlmConfig.anthropic(\n            api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n            model=\"claude-3-5-sonnet-20241022\"\n        )\n    elif use_case == \"cost_effective\":\n        return graphbit.LlmConfig.deepseek(\n            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n            model=\"deepseek-chat\"\n        )\n    elif use_case == \"coding\":\n        return graphbit.LlmConfig.deepseek(\n            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n            model=\"deepseek-coder\"\n        )\n    elif use_case == \"reasoning\":\n        return graphbit.LlmConfig.deepseek(\n            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n            model=\"deepseek-reasoner\"\n        )\n    elif use_case == \"local\":\n        return graphbit.LlmConfig.ollama(model=\"llama3.2\")\n    else:\n        return graphbit.LlmConfig.openai(\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n            model=\"gpt-4o-mini\"\n        )\n</code></pre>"},{"location":"user-guide/llm-providers/#2-api-key-management","title":"2. API Key Management","text":"<p>Securely manage API keys:</p> <pre><code>import os\nfrom pathlib import Path\n\ndef get_api_key(provider):\n    \"\"\"Securely retrieve API keys\"\"\"\n    key_mapping = {\n        \"openai\": \"OPENAI_API_KEY\",\n        \"anthropic\": \"ANTHROPIC_API_KEY\",\n        \"deepseek\": \"DEEPSEEK_API_KEY\"\n    }\n\n    env_var = key_mapping.get(provider)\n    if not env_var:\n        raise ValueError(f\"Unknown provider: {provider}\")\n\n    api_key = os.getenv(env_var)\n    if not api_key:\n        raise ValueError(f\"Missing {env_var} environment variable\")\n\n    return api_key\n\n# Usage\ntry:\n    openai_config = graphbit.LlmConfig.openai(\n        api_key=get_api_key(\"openai\")\n    )\nexcept ValueError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"user-guide/llm-providers/#3-client-reuse","title":"3. Client Reuse","text":"<p>Reuse clients for better performance:</p> <pre><code>class LLMManager:\n    def __init__(self):\n        self.clients = {}\n\n    def get_client(self, provider, model=None):\n        \"\"\"Get or create client for provider\"\"\"\n        key = f\"{provider}_{model or 'default'}\"\n\n        if key not in self.clients:\n            if provider == \"openai\":\n                config = graphbit.LlmConfig.openai(\n                    api_key=get_api_key(\"openai\"),\n                    model=model\n                )\n            elif provider == \"anthropic\":\n                config = graphbit.LlmConfig.anthropic(\n                    api_key=get_api_key(\"anthropic\"),\n                    model=model\n                )\n            elif provider == \"deepseek\":\n                config = graphbit.LlmConfig.deepseek(\n                    api_key=get_api_key(\"deepseek\"),\n                    model=model\n                )\n            elif provider == \"ollama\":\n                config = graphbit.LlmConfig.ollama(model=model)\n            else:\n                raise ValueError(f\"Unknown provider: {provider}\")\n\n            self.clients[key] = graphbit.LlmClient(config)\n\n        return self.clients[key]\n\n# Usage\nllm_manager = LLMManager()\nopenai_client = llm_manager.get_client(\"openai\", \"gpt-4o-mini\")\n</code></pre>"},{"location":"user-guide/llm-providers/#4-monitoring-and-logging","title":"4. Monitoring and Logging","text":"<p>Monitor LLM usage and performance:</p> <pre><code>def monitor_llm_usage(client, operation_name):\n    \"\"\"Monitor LLM client usage\"\"\"\n    stats_before = client.get_stats()\n\n    # Perform operation here\n\n    stats_after = client.get_stats()\n\n    requests_made = stats_after['total_requests'] - stats_before['total_requests']\n    print(f\"{operation_name}: {requests_made} requests made\")\n\n    if stats_after['total_requests'] &gt; 0:\n        success_rate = stats_after['successful_requests'] / stats_after['total_requests']\n        print(f\"Overall success rate: {success_rate:.2%}\")\n</code></pre>"},{"location":"user-guide/llm-providers/#whats-next","title":"What's Next","text":"<ul> <li>Learn about Embeddings for vector operations</li> <li>Explore Workflow Builder for complex workflows</li> <li>Check Performance for optimization techniques</li> <li>See Monitoring for production monitoring</li> </ul>"},{"location":"user-guide/monitoring/","title":"Monitoring and Observability","text":"<p>GraphBit provides comprehensive monitoring capabilities to track workflow performance, health, and reliability. This guide covers metrics collection, alerting, logging, and observability best practices.</p>"},{"location":"user-guide/monitoring/#overview","title":"Overview","text":"<p>GraphBit monitoring encompasses: - Execution Metrics: Performance, success rates, and timing data - System Health: Resource usage and availability monitoring - Error Tracking: Failure detection and analysis - Business Metrics: Custom metrics for business logic - Real-time Dashboards: Live monitoring and visualization</p>"},{"location":"user-guide/monitoring/#basic-monitoring-setup","title":"Basic Monitoring Setup","text":""},{"location":"user-guide/monitoring/#core-metrics-collection","title":"Core Metrics Collection","text":"<pre><code>import graphbit\nimport time\nimport json\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nfrom datetime import datetime, timedelta\nimport os\n\n@dataclass\nclass WorkflowMetrics:\n    \"\"\"Core workflow execution metrics.\"\"\"\n    workflow_id: str\n    workflow_name: str\n    execution_id: str\n    start_time: datetime\n    end_time: Optional[datetime] = None\n    duration_ms: Optional[int] = None\n    status: str = \"running\"  # running, completed, failed\n    node_count: int = 0\n    nodes_executed: int = 0\n    error_message: Optional[str] = None\n\nclass WorkflowMonitor:\n    \"\"\"Monitors workflow execution and collects metrics.\"\"\"\n\n    def __init__(self):\n        self.metrics_store: List[WorkflowMetrics] = []\n        self.active_executions: Dict[str, WorkflowMetrics] = {}\n\n    def start_execution(self, workflow, execution_id=None):\n        \"\"\"Start monitoring a workflow execution.\"\"\"\n\n        if execution_id is None:\n            execution_id = f\"exec_{int(time.time())}\"\n\n        # Count nodes in workflow (basic implementation)\n        node_count = 0\n        try:\n            # Attempt to get node count if workflow has such a method\n            node_count = len(workflow.nodes()) if hasattr(workflow, 'nodes') else 1\n        except:\n            node_count = 1  # Default assumption\n\n        metrics = WorkflowMetrics(\n            workflow_id=str(hash(str(workflow))),\n            workflow_name=getattr(workflow, 'name', 'Unknown'),\n            execution_id=execution_id,\n            start_time=datetime.now(),\n            node_count=node_count\n        )\n\n        self.active_executions[execution_id] = metrics\n        return execution_id\n\n    def end_execution(self, execution_id, status=\"completed\", error_message=None):\n        \"\"\"End monitoring a workflow execution.\"\"\"\n\n        if execution_id not in self.active_executions:\n            return\n\n        metrics = self.active_executions[execution_id]\n        metrics.end_time = datetime.now()\n        metrics.duration_ms = int(\n            (metrics.end_time - metrics.start_time).total_seconds() * 1000\n        )\n        metrics.status = status\n        metrics.error_message = error_message\n\n        # Move to permanent storage\n        self.metrics_store.append(metrics)\n        del self.active_executions[execution_id]\n\n    def get_metrics_summary(self, time_window_hours=24):\n        \"\"\"Get metrics summary for specified time window.\"\"\"\n\n        cutoff_time = datetime.now() - timedelta(hours=time_window_hours)\n        recent_metrics = [\n            m for m in self.metrics_store \n            if m.start_time &gt; cutoff_time\n        ]\n\n        if not recent_metrics:\n            return {\"message\": \"No metrics in time window\"}\n\n        total_executions = len(recent_metrics)\n        successful_executions = len([m for m in recent_metrics if m.status == \"completed\"])\n        failed_executions = len([m for m in recent_metrics if m.status == \"failed\"])\n\n        return {\n            \"time_window_hours\": time_window_hours,\n            \"total_executions\": total_executions,\n            \"successful_executions\": successful_executions,\n            \"failed_executions\": failed_executions,\n            \"success_rate\": (successful_executions / total_executions) * 100 if total_executions &gt; 0 else 0,\n            \"active_executions\": len(self.active_executions)\n        }\n\ndef create_monitored_workflow():\n    \"\"\"Create workflow with built-in monitoring.\"\"\"\n\n    # Initialize GraphBit\n    graphbit.init()\n\n    workflow = graphbit.Workflow(\"Monitored Workflow\")\n\n    # Monitoring agent\n    monitor = graphbit.Node.agent(\n        name=\"Execution Monitor\",\n        prompt=\"\"\"\n        Monitor this workflow execution:\n\n        Input: {input}\n\n        Report:\n        - Processing status\n        - Performance metrics\n        - Any issues detected\n        - Resource usage estimates\n        \"\"\",\n        agent_id=\"monitor\"\n    )\n\n    # Main processor\n    processor = graphbit.Node.agent(\n        name=\"Main Processor\",\n        prompt=\"Process this data: {input}\",\n        agent_id=\"processor\"\n    )\n\n    # Build monitored workflow\n    monitor_id = workflow.add_node(monitor)\n    processor_id = workflow.add_node(processor)\n\n    workflow.connect(monitor_id, processor_id)\n\n    return workflow\n</code></pre>"},{"location":"user-guide/monitoring/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"user-guide/monitoring/#execution-time-tracking","title":"Execution Time Tracking","text":"<pre><code>import statistics\nimport threading\nfrom collections import defaultdict\n\nclass PerformanceMonitor:\n    \"\"\"Monitors workflow performance metrics.\"\"\"\n\n    def __init__(self):\n        self.execution_times: List[float] = []\n        self.node_execution_times: Dict[str, List[float]] = defaultdict(list)\n        self.client_stats: Dict[str, Dict] = {}\n        self._lock = threading.Lock()\n\n    def record_execution_time(self, duration_ms, workflow_name=\"unknown\"):\n        \"\"\"Record workflow execution time.\"\"\"\n        with self._lock:\n            self.execution_times.append(duration_ms)\n\n    def record_node_execution_time(self, node_name, duration_ms):\n        \"\"\"Record individual node execution time.\"\"\"\n        with self._lock:\n            self.node_execution_times[node_name].append(duration_ms)\n\n    def track_llm_performance(self, client):\n        \"\"\"Track LLM client performance metrics.\"\"\"\n        try:\n            stats = client.get_stats()\n            provider = stats.get(\"provider\", \"unknown\")\n\n            with self._lock:\n                if provider not in self.client_stats:\n                    self.client_stats[provider] = {\n                        \"total_requests\": 0,\n                        \"total_tokens\": 0,\n                        \"average_latency\": 0,\n                        \"error_count\": 0\n                    }\n\n                # Update stats\n                self.client_stats[provider][\"total_requests\"] = stats.get(\"total_requests\", 0)\n                self.client_stats[provider][\"total_tokens\"] = stats.get(\"total_tokens\", 0)\n                self.client_stats[provider][\"average_latency\"] = stats.get(\"average_latency_ms\", 0)\n                self.client_stats[provider][\"error_count\"] = stats.get(\"error_count\", 0)\n\n        except Exception as e:\n            print(f\"Failed to track LLM performance: {e}\")\n\n    def get_performance_stats(self):\n        \"\"\"Get comprehensive performance statistics.\"\"\"\n\n        with self._lock:\n            if not self.execution_times:\n                return {\"message\": \"No performance data available\"}\n\n            # Overall execution time stats\n            avg_time = statistics.mean(self.execution_times)\n            median_time = statistics.median(self.execution_times)\n\n            # Node performance breakdown\n            node_stats = {}\n            for node_name, times in self.node_execution_times.items():\n                if times:\n                    node_stats[node_name] = {\n                        \"average_ms\": statistics.mean(times),\n                        \"median_ms\": statistics.median(times),\n                        \"max_ms\": max(times),\n                        \"execution_count\": len(times)\n                    }\n\n            return {\n                \"overall_performance\": {\n                    \"average_execution_ms\": avg_time,\n                    \"median_execution_ms\": median_time,\n                    \"total_executions\": len(self.execution_times)\n                },\n                \"node_performance\": node_stats,\n                \"llm_performance\": dict(self.client_stats)\n            }\n\ndef monitor_workflow_execution(workflow, executor, monitor=None):\n    \"\"\"Execute workflow with performance monitoring.\"\"\"\n\n    if monitor is None:\n        monitor = PerformanceMonitor()\n\n    start_time = time.time()\n\n    try:\n        # Execute workflow\n        result = executor.execute(workflow)\n\n        # Calculate execution time\n        duration_ms = (time.time() - start_time) * 1000\n        monitor.record_execution_time(duration_ms, workflow.name if hasattr(workflow, 'name') else 'Unknown')\n\n        # Track LLM performance if possible\n        if hasattr(executor, 'client'):\n            monitor.track_llm_performance(executor.client)\n\n        return result, monitor\n\n    except Exception as e:\n        duration_ms = (time.time() - start_time) * 1000\n        monitor.record_execution_time(duration_ms, workflow.name if hasattr(workflow, 'name') else 'Unknown')\n        raise e\n</code></pre>"},{"location":"user-guide/monitoring/#system-health-monitoring","title":"System Health Monitoring","text":""},{"location":"user-guide/monitoring/#health-check-implementation","title":"Health Check Implementation","text":"<pre><code>class SystemHealthMonitor:\n    \"\"\"Monitor GraphBit system health.\"\"\"\n\n    def __init__(self, check_interval_seconds=60):\n        self.check_interval = check_interval_seconds\n        self.health_history: List[Dict] = []\n        self.alerts: List[Dict] = []\n        self._monitoring = False\n\n    def start_monitoring(self):\n        \"\"\"Start continuous health monitoring.\"\"\"\n\n        if self._monitoring:\n            return\n\n        self._monitoring = True\n\n        def monitor_loop():\n            while self._monitoring:\n                try:\n                    health = self.check_system_health()\n                    self.health_history.append({\n                        \"timestamp\": datetime.now(),\n                        \"health\": health\n                    })\n\n                    # Keep only last 100 health checks\n                    if len(self.health_history) &gt; 100:\n                        self.health_history = self.health_history[-100:]\n\n                    # Check for alerts\n                    self._check_alerts(health)\n\n                except Exception as e:\n                    print(f\"Health monitoring error: {e}\")\n\n                time.sleep(self.check_interval)\n\n        # Start monitoring in background thread\n        import threading\n        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)\n        monitor_thread.start()\n\n    def stop_monitoring(self):\n        \"\"\"Stop health monitoring.\"\"\"\n        self._monitoring = False\n\n    def check_system_health(self):\n        \"\"\"Check current system health.\"\"\"\n\n        try:\n            # Use GraphBit's built-in health check\n            health = graphbit.health_check()\n\n            # Add custom health metrics\n            custom_health = {\n                **health,\n                \"timestamp\": datetime.now().isoformat(),\n                \"custom_checks\": self._perform_custom_checks()\n            }\n\n            return custom_health\n\n        except Exception as e:\n            return {\n                \"overall_healthy\": False,\n                \"error\": str(e),\n                \"timestamp\": datetime.now().isoformat()\n            }\n\n    def _perform_custom_checks(self):\n        \"\"\"Perform custom health checks.\"\"\"\n\n        checks = {}\n\n        # Check system info availability\n        try:\n            info = graphbit.get_system_info()\n            checks[\"system_info_available\"] = True\n            checks[\"runtime_initialized\"] = info.get(\"runtime_initialized\", False)\n        except:\n            checks[\"system_info_available\"] = False\n            checks[\"runtime_initialized\"] = False\n\n        # Check memory usage\n        try:\n            import psutil\n            memory = psutil.virtual_memory()\n            checks[\"memory_usage_percent\"] = memory.percent\n            checks[\"memory_available_gb\"] = memory.available / (1024**3)\n        except:\n            checks[\"memory_usage_percent\"] = None\n            checks[\"memory_available_gb\"] = None\n\n        return checks\n\n    def _check_alerts(self, health):\n        \"\"\"Check for alert conditions.\"\"\"\n\n        alerts = []\n\n        # Check overall health\n        if not health.get(\"overall_healthy\", False):\n            alerts.append({\n                \"level\": \"critical\",\n                \"message\": \"System health check failed\",\n                \"timestamp\": datetime.now()\n            })\n\n        # Check memory usage\n        custom_checks = health.get(\"custom_checks\", {})\n        memory_usage = custom_checks.get(\"memory_usage_percent\")\n\n        if memory_usage and memory_usage &gt; 90:\n            alerts.append({\n                \"level\": \"warning\",\n                \"message\": f\"High memory usage: {memory_usage:.1f}%\",\n                \"timestamp\": datetime.now()\n            })\n\n        # Store alerts\n        self.alerts.extend(alerts)\n\n        # Keep only recent alerts (last 24 hours)\n        cutoff = datetime.now() - timedelta(hours=24)\n        self.alerts = [a for a in self.alerts if a[\"timestamp\"] &gt; cutoff]\n\n    def get_health_summary(self):\n        \"\"\"Get health monitoring summary.\"\"\"\n\n        if not self.health_history:\n            return {\"message\": \"No health data available\"}\n\n        recent_health = self.health_history[-10:]  # Last 10 checks\n        healthy_count = sum(1 for h in recent_health if h[\"health\"].get(\"overall_healthy\", False))\n\n        return {\n            \"current_health\": self.health_history[-1][\"health\"] if self.health_history else None,\n            \"recent_success_rate\": (healthy_count / len(recent_health)) * 100,\n            \"total_health_checks\": len(self.health_history),\n            \"active_alerts\": len([a for a in self.alerts if a[\"level\"] == \"critical\"]),\n            \"warnings\": len([a for a in self.alerts if a[\"level\"] == \"warning\"])\n        }\n</code></pre>"},{"location":"user-guide/monitoring/#error-tracking","title":"Error Tracking","text":""},{"location":"user-guide/monitoring/#comprehensive-error-monitoring","title":"Comprehensive Error Monitoring","text":"<pre><code>class ErrorTracker:\n    \"\"\"Track and analyze errors in GraphBit workflows.\"\"\"\n\n    def __init__(self):\n        self.errors: List[Dict] = []\n        self.error_patterns: Dict[str, int] = defaultdict(int)\n\n    def record_error(self, error, context=None):\n        \"\"\"Record an error with context.\"\"\"\n\n        error_record = {\n            \"timestamp\": datetime.now(),\n            \"error_type\": type(error).__name__,\n            \"error_message\": str(error),\n            \"context\": context or {},\n            \"stack_trace\": None\n        }\n\n        # Capture stack trace if available\n        try:\n            import traceback\n            error_record[\"stack_trace\"] = traceback.format_exc()\n        except:\n            pass\n\n        self.errors.append(error_record)\n\n        # Track error patterns\n        self.error_patterns[error_record[\"error_type\"]] += 1\n\n        # Keep only recent errors (last 1000)\n        if len(self.errors) &gt; 1000:\n            self.errors = self.errors[-1000:]\n\n    def get_error_summary(self, time_window_hours=24):\n        \"\"\"Get error summary for time window.\"\"\"\n\n        cutoff = datetime.now() - timedelta(hours=time_window_hours)\n        recent_errors = [e for e in self.errors if e[\"timestamp\"] &gt; cutoff]\n\n        if not recent_errors:\n            return {\"message\": \"No errors in time window\"}\n\n        # Group by error type\n        error_counts = defaultdict(int)\n        for error in recent_errors:\n            error_counts[error[\"error_type\"]] += 1\n\n        return {\n            \"total_errors\": len(recent_errors),\n            \"unique_error_types\": len(error_counts),\n            \"error_breakdown\": dict(error_counts),\n            \"most_common_error\": max(error_counts.items(), key=lambda x: x[1]) if error_counts else None\n        }\n\n    def analyze_error_trends(self):\n        \"\"\"Analyze error trends over time.\"\"\"\n\n        if len(self.errors) &lt; 2:\n            return {\"message\": \"Insufficient data for trend analysis\"}\n\n        # Group errors by hour\n        hourly_errors = defaultdict(int)\n\n        for error in self.errors:\n            hour_key = error[\"timestamp\"].strftime(\"%Y-%m-%d %H:00\")\n            hourly_errors[hour_key] += 1\n\n        # Calculate trend\n        error_counts = list(hourly_errors.values())\n\n        if len(error_counts) &gt;= 2:\n            recent_avg = sum(error_counts[-3:]) / min(3, len(error_counts))\n            older_avg = sum(error_counts[:-3]) / max(1, len(error_counts) - 3)\n\n            trend = \"increasing\" if recent_avg &gt; older_avg else \"decreasing\"\n        else:\n            trend = \"stable\"\n\n        return {\n            \"trend\": trend,\n            \"hourly_breakdown\": dict(hourly_errors),\n            \"peak_error_hour\": max(hourly_errors.items(), key=lambda x: x[1])[0] if hourly_errors else None\n        }\n\ndef execute_with_error_tracking(workflow, executor, error_tracker=None):\n    \"\"\"Execute workflow with comprehensive error tracking.\"\"\"\n\n    if error_tracker is None:\n        error_tracker = ErrorTracker()\n\n    try:\n        result = executor.execute(workflow)\n\n        # Check if execution failed\n        if result.is_failed():\n            error_context = {\n                \"workflow_name\": getattr(workflow, 'name', 'Unknown'),\n                \"execution_failed\": True\n            }\n\n            try:\n                error_message = result.error()\n                error_tracker.record_error(\n                    Exception(f\"Workflow execution failed: {error_message}\"),\n                    error_context\n                )\n            except Exception as e:\n                error_tracker.record_error(e, error_context)\n\n        return result, error_tracker\n\n    except Exception as e:\n        error_context = {\n            \"workflow_name\": getattr(workflow, 'name', 'Unknown'),\n            \"execution_exception\": True\n        }\n        error_tracker.record_error(e, error_context)\n        raise e\n</code></pre>"},{"location":"user-guide/monitoring/#custom-metrics-and-dashboards","title":"Custom Metrics and Dashboards","text":""},{"location":"user-guide/monitoring/#business-metrics-collection","title":"Business Metrics Collection","text":"<pre><code>class BusinessMetricsCollector:\n    \"\"\"Collect custom business metrics.\"\"\"\n\n    def __init__(self):\n        self.metrics: Dict[str, List] = defaultdict(list)\n\n    def record_metric(self, metric_name, value, tags=None):\n        \"\"\"Record a custom business metric.\"\"\"\n\n        metric_record = {\n            \"timestamp\": datetime.now(),\n            \"value\": value,\n            \"tags\": tags or {}\n        }\n\n        self.metrics[metric_name].append(metric_record)\n\n        # Keep only recent metrics (last 10000 per metric)\n        if len(self.metrics[metric_name]) &gt; 10000:\n            self.metrics[metric_name] = self.metrics[metric_name][-10000:]\n\n    def get_metric_summary(self, metric_name, time_window_hours=24):\n        \"\"\"Get summary for a specific metric.\"\"\"\n\n        if metric_name not in self.metrics:\n            return {\"message\": f\"Metric '{metric_name}' not found\"}\n\n        cutoff = datetime.now() - timedelta(hours=time_window_hours)\n        recent_values = [\n            m[\"value\"] for m in self.metrics[metric_name]\n            if m[\"timestamp\"] &gt; cutoff\n        ]\n\n        if not recent_values:\n            return {\"message\": \"No recent data for metric\"}\n\n        return {\n            \"metric_name\": metric_name,\n            \"count\": len(recent_values),\n            \"average\": statistics.mean(recent_values),\n            \"median\": statistics.median(recent_values),\n            \"min\": min(recent_values),\n            \"max\": max(recent_values),\n            \"total\": sum(recent_values)\n        }\n\n# Example usage with workflow execution\ndef execute_with_business_metrics(workflow, executor, metrics_collector=None):\n    \"\"\"Execute workflow with business metrics collection.\"\"\"\n\n    if metrics_collector is None:\n        metrics_collector = BusinessMetricsCollector()\n\n    # Record execution start\n    start_time = time.time()\n    metrics_collector.record_metric(\"workflow_executions_started\", 1)\n\n    try:\n        result = executor.execute(workflow)\n\n        # Record execution time\n        execution_time = (time.time() - start_time) * 1000\n        metrics_collector.record_metric(\"workflow_execution_time_ms\", execution_time)\n\n        if result.is_completed():\n            metrics_collector.record_metric(\"workflow_executions_completed\", 1)\n\n            # Record output length if available\n            try:\n                output = result.output()\n                if isinstance(output, str):\n                    metrics_collector.record_metric(\"workflow_output_length\", len(output))\n            except:\n                pass\n        else:\n            metrics_collector.record_metric(\"workflow_executions_failed\", 1)\n\n        return result, metrics_collector\n\n    except Exception as e:\n        metrics_collector.record_metric(\"workflow_executions_failed\", 1)\n        execution_time = (time.time() - start_time) * 1000\n        metrics_collector.record_metric(\"workflow_execution_time_ms\", execution_time)\n        raise e\n</code></pre>"},{"location":"user-guide/monitoring/#real-time-monitoring-dashboard","title":"Real-time Monitoring Dashboard","text":""},{"location":"user-guide/monitoring/#simple-text-based-dashboard","title":"Simple Text-based Dashboard","text":"<pre><code>class MonitoringDashboard:\n    \"\"\"Simple text-based monitoring dashboard.\"\"\"\n\n    def __init__(self, workflow_monitor, performance_monitor, error_tracker, health_monitor):\n        self.workflow_monitor = workflow_monitor\n        self.performance_monitor = performance_monitor\n        self.error_tracker = error_tracker\n        self.health_monitor = health_monitor\n\n    def display_dashboard(self):\n        \"\"\"Display comprehensive monitoring dashboard.\"\"\"\n\n        print(\"=\" * 60)\n        print(\"GraphBit Monitoring Dashboard\")\n        print(\"=\" * 60)\n        print(f\"Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        print()\n\n        # System Health\n        print(\"\ud83c\udfe5 System Health\")\n        print(\"-\" * 20)\n        health_summary = self.health_monitor.get_health_summary()\n        current_health = health_summary.get(\"current_health\", {})\n\n        overall_healthy = current_health.get(\"overall_healthy\", False)\n        health_icon = \"\u2705\" if overall_healthy else \"\u274c\"\n        print(f\"{health_icon} Overall Health: {'Healthy' if overall_healthy else 'Unhealthy'}\")\n\n        success_rate = health_summary.get(\"recent_success_rate\", 0)\n        print(f\"\ud83d\udcca Recent Success Rate: {success_rate:.1f}%\")\n\n        alerts = health_summary.get(\"active_alerts\", 0)\n        warnings = health_summary.get(\"warnings\", 0)\n        if alerts &gt; 0:\n            print(f\"\ud83d\udea8 Active Alerts: {alerts}\")\n        if warnings &gt; 0:\n            print(f\"\u26a0\ufe0f  Warnings: {warnings}\")\n        print()\n\n        # Workflow Metrics\n        print(\"\u26a1 Workflow Metrics (Last 24 Hours)\")\n        print(\"-\" * 35)\n        workflow_summary = self.workflow_monitor.get_metrics_summary(24)\n\n        if \"message\" not in workflow_summary:\n            total_executions = workflow_summary.get(\"total_executions\", 0)\n            success_rate = workflow_summary.get(\"success_rate\", 0)\n            active_executions = workflow_summary.get(\"active_executions\", 0)\n\n            print(f\"\ud83d\udcc8 Total Executions: {total_executions}\")\n            print(f\"\ud83c\udfaf Success Rate: {success_rate:.1f}%\")\n            print(f\"\ud83d\udd04 Active Executions: {active_executions}\")\n        else:\n            print(\"\ud83d\udcdd No workflow data available\")\n        print()\n\n        # Performance Metrics\n        print(\"\ud83d\ude80 Performance Metrics\")\n        print(\"-\" * 22)\n        perf_stats = self.performance_monitor.get_performance_stats()\n\n        if \"message\" not in perf_stats:\n            overall_perf = perf_stats.get(\"overall_performance\", {})\n            avg_time = overall_perf.get(\"average_execution_ms\", 0)\n            total_executions = overall_perf.get(\"total_executions\", 0)\n\n            print(f\"\u23f1\ufe0f  Average Execution Time: {avg_time:.1f}ms\")\n            print(f\"\ud83d\udcca Total Executions: {total_executions}\")\n\n            # LLM Performance\n            llm_perf = perf_stats.get(\"llm_performance\", {})\n            for provider, stats in llm_perf.items():\n                print(f\"\ud83e\udd16 {provider} - Requests: {stats.get('total_requests', 0)}, \"\n                      f\"Avg Latency: {stats.get('average_latency', 0):.1f}ms\")\n        else:\n            print(\"\ud83d\udcdd No performance data available\")\n        print()\n\n        # Error Summary\n        print(\"\ud83d\udc1b Error Summary (Last 24 Hours)\")\n        print(\"-\" * 30)\n        error_summary = self.error_tracker.get_error_summary(24)\n\n        if \"message\" not in error_summary:\n            total_errors = error_summary.get(\"total_errors\", 0)\n            unique_types = error_summary.get(\"unique_error_types\", 0)\n            most_common = error_summary.get(\"most_common_error\")\n\n            print(f\"\ud83d\udd34 Total Errors: {total_errors}\")\n            print(f\"\ud83d\udd22 Unique Error Types: {unique_types}\")\n\n            if most_common:\n                print(f\"\ud83d\udcc8 Most Common: {most_common[0]} ({most_common[1]} occurrences)\")\n        else:\n            print(\"\ud83d\udcdd No error data available\")\n\n        print(\"=\" * 60)\n\n    def start_live_dashboard(self, refresh_interval=30):\n        \"\"\"Start live dashboard with auto-refresh.\"\"\"\n\n        import os\n\n        try:\n            while True:\n                # Clear screen (works on most terminals)\n                os.system('clear' if os.name == 'posix' else 'cls')\n\n                self.display_dashboard()\n\n                print(f\"\\nRefreshing in {refresh_interval} seconds... (Ctrl+C to stop)\")\n                time.sleep(refresh_interval)\n\n        except KeyboardInterrupt:\n            print(\"\\n\\nDashboard stopped.\")\n\n# Complete monitoring setup\ndef setup_comprehensive_monitoring():\n    \"\"\"Set up comprehensive monitoring for GraphBit workflows.\"\"\"\n\n    # Initialize all monitors\n    workflow_monitor = WorkflowMonitor()\n    performance_monitor = PerformanceMonitor()\n    error_tracker = ErrorTracker()\n    health_monitor = SystemHealthMonitor()\n\n    # Start health monitoring\n    health_monitor.start_monitoring()\n\n    # Create dashboard\n    dashboard = MonitoringDashboard(\n        workflow_monitor, \n        performance_monitor, \n        error_tracker, \n        health_monitor\n    )\n\n    return {\n        \"workflow_monitor\": workflow_monitor,\n        \"performance_monitor\": performance_monitor,\n        \"error_tracker\": error_tracker,\n        \"health_monitor\": health_monitor,\n        \"dashboard\": dashboard\n    }\n\n# Usage example\nif __name__ == \"__main__\":\n    # Initialize GraphBit\n    graphbit.init()\n\n    # Set up monitoring\n    monitoring = setup_comprehensive_monitoring()\n\n    # Display dashboard once\n    monitoring[\"dashboard\"].display_dashboard()\n\n    # Or start live dashboard\n    # monitoring[\"dashboard\"].start_live_dashboard(refresh_interval=30)\n</code></pre>"},{"location":"user-guide/monitoring/#integration-with-external-systems","title":"Integration with External Systems","text":""},{"location":"user-guide/monitoring/#exporting-metrics","title":"Exporting Metrics","text":"<pre><code>def export_metrics_to_json(monitors, output_file=\"graphbit_metrics.json\"):\n    \"\"\"Export all metrics to JSON file.\"\"\"\n\n    workflow_monitor, performance_monitor, error_tracker, health_monitor = monitors\n\n    export_data = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"workflow_metrics\": workflow_monitor.get_metrics_summary(24),\n        \"performance_metrics\": performance_monitor.get_performance_stats(),\n        \"error_summary\": error_tracker.get_error_summary(24),\n        \"health_summary\": health_monitor.get_health_summary(),\n        \"system_info\": None\n    }\n\n    # Add system info if available\n    try:\n        export_data[\"system_info\"] = graphbit.get_system_info()\n    except:\n        pass\n\n    with open(output_file, 'w') as f:\n        json.dump(export_data, f, indent=2, default=str)\n\n    print(f\"Metrics exported to {output_file}\")\n\ndef send_alerts_to_webhook(health_monitor, webhook_url):\n    \"\"\"Send alerts to external webhook.\"\"\"\n\n    try:\n        import requests\n\n        health_summary = health_monitor.get_health_summary()\n        alerts = health_summary.get(\"active_alerts\", 0)\n\n        if alerts &gt; 0:\n            payload = {\n                \"message\": f\"GraphBit Health Alert: {alerts} active alerts\",\n                \"timestamp\": datetime.now().isoformat(),\n                \"health_summary\": health_summary\n            }\n\n            response = requests.post(webhook_url, json=payload, timeout=10)\n\n            if response.status_code == 200:\n                print(\"Alert sent successfully\")\n            else:\n                print(f\"Failed to send alert: {response.status_code}\")\n\n    except Exception as e:\n        print(f\"Failed to send alert: {e}\")\n</code></pre>"},{"location":"user-guide/monitoring/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/monitoring/#1-monitoring-strategy","title":"1. Monitoring Strategy","text":"<p>Implement layered monitoring:</p> <pre><code>def create_production_monitoring_setup():\n    \"\"\"Create production-ready monitoring setup.\"\"\"\n\n    # Core monitoring components\n    monitors = setup_comprehensive_monitoring()\n\n    # Configure alerts\n    health_monitor = monitors[\"health_monitor\"]\n\n    # Set up metric exports\n    def export_metrics_periodically():\n        while True:\n            try:\n                export_metrics_to_json([\n                    monitors[\"workflow_monitor\"],\n                    monitors[\"performance_monitor\"], \n                    monitors[\"error_tracker\"],\n                    monitors[\"health_monitor\"]\n                ])\n                time.sleep(300)  # Export every 5 minutes\n            except Exception as e:\n                print(f\"Metric export failed: {e}\")\n                time.sleep(60)  # Retry in 1 minute\n\n    # Start background export\n    import threading\n    export_thread = threading.Thread(target=export_metrics_periodically, daemon=True)\n    export_thread.start()\n\n    return monitors\n</code></pre>"},{"location":"user-guide/monitoring/#2-performance-baselines","title":"2. Performance Baselines","text":"<p>Establish performance baselines:</p> <pre><code>def establish_performance_baseline(workflow, executor, iterations=10):\n    \"\"\"Establish performance baseline for a workflow.\"\"\"\n\n    performance_monitor = PerformanceMonitor()\n    execution_times = []\n\n    print(f\"Establishing baseline with {iterations} iterations...\")\n\n    for i in range(iterations):\n        start_time = time.time()\n\n        try:\n            result = executor.execute(workflow)\n            duration_ms = (time.time() - start_time) * 1000\n            execution_times.append(duration_ms)\n            performance_monitor.record_execution_time(duration_ms)\n\n            print(f\"  Iteration {i+1}: {duration_ms:.1f}ms\")\n\n        except Exception as e:\n            print(f\"  Iteration {i+1}: FAILED - {e}\")\n\n    if execution_times:\n        baseline = {\n            \"average_ms\": statistics.mean(execution_times),\n            \"median_ms\": statistics.median(execution_times),\n            \"p95_ms\": sorted(execution_times)[int(len(execution_times) * 0.95)],\n            \"iterations\": len(execution_times)\n        }\n\n        print(f\"\\nBaseline established:\")\n        print(f\"  Average: {baseline['average_ms']:.1f}ms\")\n        print(f\"  Median: {baseline['median_ms']:.1f}ms\")\n        print(f\"  P95: {baseline['p95_ms']:.1f}ms\")\n\n        return baseline\n    else:\n        print(\"Failed to establish baseline - no successful executions\")\n        return None\n</code></pre>"},{"location":"user-guide/monitoring/#3-automated-alerting","title":"3. Automated Alerting","text":"<p>Set up automated alerting:</p> <pre><code>def setup_automated_alerting(health_monitor, thresholds=None):\n    \"\"\"Set up automated alerting based on health metrics.\"\"\"\n\n    if thresholds is None:\n        thresholds = {\n            \"error_rate_percent\": 10,\n            \"memory_usage_percent\": 85,\n            \"success_rate_percent\": 95\n        }\n\n    def check_and_alert():\n        health_summary = health_monitor.get_health_summary()\n        current_health = health_summary.get(\"current_health\", {})\n\n        alerts = []\n\n        # Check memory usage\n        custom_checks = current_health.get(\"custom_checks\", {})\n        memory_usage = custom_checks.get(\"memory_usage_percent\")\n\n        if memory_usage and memory_usage &gt; thresholds[\"memory_usage_percent\"]:\n            alerts.append(f\"High memory usage: {memory_usage:.1f}%\")\n\n        # Check success rate\n        success_rate = health_summary.get(\"recent_success_rate\", 100)\n        if success_rate &lt; thresholds[\"success_rate_percent\"]:\n            alerts.append(f\"Low success rate: {success_rate:.1f}%\")\n\n        # Send alerts if any\n        if alerts:\n            print(\"\ud83d\udea8 ALERTS TRIGGERED:\")\n            for alert in alerts:\n                print(f\"  - {alert}\")\n\n        return alerts\n\n    return check_and_alert\n</code></pre>"},{"location":"user-guide/monitoring/#whats-next","title":"What's Next","text":"<ul> <li>Learn about Performance optimization techniques</li> <li>Explore Reliability patterns for production systems</li> <li>Check Validation for comprehensive testing strategies</li> <li>See LLM Providers for provider-specific monitoring</li> </ul>"},{"location":"user-guide/performance/","title":"Performance Optimization","text":"<p>GraphBit provides multiple strategies for optimizing workflow performance, from execution patterns to resource management. This guide covers techniques to maximize throughput, minimize latency, and optimize resource usage.</p>"},{"location":"user-guide/performance/#overview","title":"Overview","text":"<p>Performance optimization in GraphBit focuses on: - Execution Optimization: Parallel processing and efficient node execution - Resource Management: Memory, CPU, and network optimization - Caching Strategies: Reducing redundant computations - Configuration Tuning: Optimal settings for different scenarios - Monitoring &amp; Profiling: Identifying and resolving bottlenecks</p>"},{"location":"user-guide/performance/#execution-optimization","title":"Execution Optimization","text":""},{"location":"user-guide/performance/#parallel-processing","title":"Parallel Processing","text":"<pre><code>import graphbit\nimport time\nimport os\n\n# Initialize GraphBit\ngraphbit.init()\n\ndef create_parallel_workflow():\n    \"\"\"Create workflow optimized for parallel execution.\"\"\"\n\n    workflow = graphbit.Workflow(\"Parallel Processing Workflow\")\n\n    # Input processor\n    input_processor = graphbit.Node.agent(\n        name=\"Input Processor\",\n        prompt=\"Prepare data for parallel processing: {input}\",\n        agent_id=\"input_processor\"\n    )\n\n    # Parallel processing branches\n    branch_nodes = []\n    for i in range(4):  # Create 4 parallel branches\n        branch = graphbit.Node.agent(\n            name=f\"Parallel Branch {i+1}\",\n            prompt=f\"Process branch {i+1} data: {{prepared_data}}\",\n            agent_id=f\"branch_{i+1}\"\n        )\n        branch_nodes.append(branch)\n\n    # Results aggregator\n    aggregator = graphbit.Node.agent(\n        name=\"Results Aggregator\",\n        prompt=\"\"\"\n        Combine results from parallel branches:\n        Branch 1: {branch_1_output}\n        Branch 2: {branch_2_output}\n        Branch 3: {branch_3_output}\n        Branch 4: {branch_4_output}\n        \"\"\",\n        agent_id=\"aggregator\"\n    )\n\n    # Build parallel structure\n    input_id = workflow.add_node(input_processor)\n    branch_ids = [workflow.add_node(branch) for branch in branch_nodes]\n    agg_id = workflow.add_node(aggregator)\n\n    # Connect input to all branches (fan-out)\n    for branch_id in branch_ids:\n        workflow.connect(input_id, branch_id)\n\n    # Connect all branches to aggregator (fan-in)\n    for branch_id in branch_ids:\n        workflow.connect(branch_id, agg_id)\n\n    return workflow\n\ndef create_performance_optimized_executor():\n    \"\"\"Create executor optimized for performance.\"\"\"\n\n    # Use fast, cost-effective model\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"  # Fast and efficient model\n    )\n\n    # Create high-performance executor\n    executor = graphbit.Executor.new_high_throughput(\n        llm_config=config,\n        timeout_seconds=60,\n        debug=False\n    )\n\n    return executor\n</code></pre>"},{"location":"user-guide/performance/#executor-types-for-performance","title":"Executor Types for Performance","text":""},{"location":"user-guide/performance/#different-executor-configurations","title":"Different Executor Configurations","text":"<pre><code>def create_optimized_executors():\n    \"\"\"Create different executor types for various performance needs.\"\"\"\n\n    # Base LLM configuration\n    llm_config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    executors = {}\n\n    # Standard executor - balanced performance\n    executors[\"standard\"] = graphbit.Executor(\n        config=llm_config,\n        timeout_seconds=60,\n        debug=False\n    )\n\n    # High-throughput executor - maximize parallel processing\n    executors[\"high_throughput\"] = graphbit.Executor.new_high_throughput(\n        llm_config=llm_config,\n        timeout_seconds=120,\n        debug=False\n    )\n\n    # Low-latency executor - minimize response time\n    executors[\"low_latency\"] = graphbit.Executor.new_low_latency(\n        llm_config=llm_config,\n        timeout_seconds=30,\n        debug=False\n    )\n\n    # Memory-optimized executor - minimize memory usage\n    executors[\"memory_optimized\"] = graphbit.Executor.new_memory_optimized(\n        llm_config=llm_config,\n        timeout_seconds=90,\n        debug=False\n    )\n\n    return executors\n\ndef benchmark_executor_types(workflow, test_input):\n    \"\"\"Benchmark different executor types.\"\"\"\n\n    executors = create_optimized_executors()\n    results = {}\n\n    for executor_type, executor in executors.items():\n        print(f\"Benchmarking {executor_type} executor...\")\n\n        start_time = time.time()\n\n        try:\n            result = executor.execute(workflow)\n            duration_ms = (time.time() - start_time) * 1000\n\n            results[executor_type] = {\n                \"duration_ms\": duration_ms,\n                \"success\": result.is_completed(),\n                \"output_length\": len(result.output()) if result.is_completed() else 0\n            }\n\n            print(f\"  \u2705 {executor_type}: {duration_ms:.1f}ms\")\n\n        except Exception as e:\n            duration_ms = (time.time() - start_time) * 1000\n            results[executor_type] = {\n                \"duration_ms\": duration_ms,\n                \"success\": False,\n                \"error\": str(e)\n            }\n\n            print(f\"  \u274c {executor_type}: Failed after {duration_ms:.1f}ms - {e}\")\n\n    return results\n</code></pre>"},{"location":"user-guide/performance/#resource-management","title":"Resource Management","text":""},{"location":"user-guide/performance/#memory-optimization","title":"Memory Optimization","text":"<pre><code>def create_memory_optimized_workflow():\n    \"\"\"Create workflow optimized for memory usage.\"\"\"\n\n    workflow = graphbit.Workflow(\"Memory Optimized Workflow\")\n\n    # Use concise prompts to reduce memory usage\n    data_processor = graphbit.Node.agent(\n        name=\"Memory Efficient Processor\",\n        prompt=\"Analyze: {input}\",  # Concise prompt\n        agent_id=\"mem_processor\"\n    )\n\n    # Data compressor using transform node\n    compressor = graphbit.Node.transform(\n        name=\"Data Compressor\",\n        transformation=\"uppercase\"  # Simple transformation to reduce data size\n    )\n\n    # Build memory-efficient workflow\n    proc_id = workflow.add_node(data_processor)\n    comp_id = workflow.add_node(compressor)\n\n    workflow.connect(proc_id, comp_id)\n\n    return workflow\n\ndef monitor_memory_usage():\n    \"\"\"Monitor workflow memory usage.\"\"\"\n\n    try:\n        import psutil\n        import os\n\n        def get_memory_usage():\n            \"\"\"Get current memory usage.\"\"\"\n            process = psutil.Process(os.getpid())\n            memory_info = process.memory_info()\n            return {\n                \"rss_mb\": memory_info.rss / 1024 / 1024,  # MB\n                \"vms_mb\": memory_info.vms / 1024 / 1024,  # MB\n                \"percent\": process.memory_percent()\n            }\n\n        # Baseline memory\n        baseline = get_memory_usage()\n        print(f\"Baseline memory: {baseline['rss_mb']:.2f} MB\")\n\n        return baseline\n\n    except ImportError:\n        print(\"psutil not available for memory monitoring\")\n        return None\n\ndef execute_with_memory_monitoring(workflow, executor):\n    \"\"\"Execute workflow with memory monitoring.\"\"\"\n\n    baseline_memory = monitor_memory_usage()\n\n    if baseline_memory:\n        print(f\"Starting execution with {baseline_memory['rss_mb']:.2f} MB\")\n\n    # Execute workflow\n    start_time = time.time()\n    result = executor.execute(workflow)\n    duration_ms = (time.time() - start_time) * 1000\n\n    # Check memory after execution\n    final_memory = monitor_memory_usage()\n\n    if baseline_memory and final_memory:\n        memory_increase = final_memory['rss_mb'] - baseline_memory['rss_mb']\n        print(f\"Memory increase: {memory_increase:.2f} MB\")\n        print(f\"Execution completed in {duration_ms:.1f}ms\")\n\n    return result\n</code></pre>"},{"location":"user-guide/performance/#configuration-tuning","title":"Configuration Tuning","text":""},{"location":"user-guide/performance/#environment-specific-optimization","title":"Environment-Specific Optimization","text":"<pre><code>def get_optimized_config(environment=\"production\", workload_type=\"balanced\"):\n    \"\"\"Get optimized configuration for specific environment and workload.\"\"\"\n\n    configs = {\n        \"development\": {\n            \"model\": \"gpt-4o-mini\",\n            \"timeout\": 30\n        },\n        \"production\": {\n            \"model\": \"gpt-4o-mini\",\n            \"timeout\": 120\n        }\n    }\n\n    workload_adjustments = {\n        \"high_throughput\": {\n            \"model\": \"gpt-4o-mini\",  # Faster model\n            \"timeout\": 15\n        },\n        \"high_quality\": {\n            \"model\": \"gpt-4o\",  # Best quality model\n            \"timeout\": 180\n        },\n        \"low_latency\": {\n            \"model\": \"gpt-4o-mini\",\n            \"timeout\": 10\n        }\n    }\n\n    base_config = configs.get(environment, configs[\"production\"])\n    workload_config = workload_adjustments.get(workload_type, {})\n\n    # Merge configurations\n    final_config = {**base_config, **workload_config}\n\n    return final_config\n\ndef create_optimized_executor(environment=\"production\", workload_type=\"balanced\"):\n    \"\"\"Create executor with optimized configuration.\"\"\"\n\n    config_params = get_optimized_config(environment, workload_type)\n\n    # LLM configuration\n    llm_config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=config_params[\"model\"]\n    )\n\n    # Create executor based on workload type\n    if workload_type == \"high_throughput\":\n        executor = graphbit.Executor.new_high_throughput(\n            llm_config=llm_config,\n            timeout_seconds=config_params[\"timeout\"],\n            debug=False\n        )\n    elif workload_type == \"low_latency\":\n        executor = graphbit.Executor.new_low_latency(\n            llm_config=llm_config,\n            timeout_seconds=config_params[\"timeout\"],\n            debug=False\n        )\n    elif workload_type == \"memory_optimized\":\n        executor = graphbit.Executor.new_memory_optimized(\n            llm_config=llm_config,\n            timeout_seconds=config_params[\"timeout\"],\n            debug=False\n        )\n    else:\n        executor = graphbit.Executor(\n            config=llm_config,\n            timeout_seconds=config_params[\"timeout\"],\n            debug=False\n        )\n\n    return executor\n</code></pre>"},{"location":"user-guide/performance/#llm-client-optimization","title":"LLM Client Optimization","text":""},{"location":"user-guide/performance/#client-configuration-and-performance","title":"Client Configuration and Performance","text":"<pre><code>def create_optimized_llm_client():\n    \"\"\"Create optimized LLM client for best performance.\"\"\"\n\n    # OpenAI configuration with performance settings\n    openai_config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"  # Fast and cost-effective\n    )\n\n    # Create client\n    client = graphbit.LlmClient(openai_config)\n\n    # Warm up the client\n    try:\n        client.warmup()\n        print(\"\u2705 Client warmed up successfully\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Client warmup failed: {e}\")\n\n    return client\n\ndef benchmark_llm_providers():\n    \"\"\"Benchmark different LLM providers for performance.\"\"\"\n\n    providers = {\n        \"openai_gpt4o_mini\": graphbit.LlmConfig.openai(\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n            model=\"gpt-4o-mini\"\n        ),\n        \"anthropic_claude\": graphbit.LlmConfig.anthropic(\n            api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n            model=\"claude-3-5-sonnet-20241022\"\n        ),\n        \"ollama_llama\": graphbit.LlmConfig.ollama(\n            model=\"llama3.2\"\n        )\n    }\n\n    test_prompt = \"Summarize this text in one sentence: {input}\"\n    test_input = \"The quick brown fox jumps over the lazy dog.\"\n\n    results = {}\n\n    for provider_name, config in providers.items():\n        print(f\"Testing {provider_name}...\")\n\n        try:\n            client = graphbit.LlmClient(config)\n\n            # Warm up\n            client.warmup()\n\n            # Time the completion\n            start_time = time.time()\n\n            response = client.complete(test_prompt, {\"input\": test_input})\n\n            duration_ms = (time.time() - start_time) * 1000\n\n            # Get client statistics\n            stats = client.get_stats()\n\n            results[provider_name] = {\n                \"duration_ms\": duration_ms,\n                \"success\": True,\n                \"response_length\": len(response),\n                \"stats\": stats\n            }\n\n            print(f\"  \u2705 {provider_name}: {duration_ms:.1f}ms\")\n\n        except Exception as e:\n            results[provider_name] = {\n                \"success\": False,\n                \"error\": str(e)\n            }\n            print(f\"  \u274c {provider_name}: Failed - {e}\")\n\n    return results\n\ndef optimize_client_batch_processing():\n    \"\"\"Demonstrate optimized batch processing.\"\"\"\n\n    config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    client = graphbit.LlmClient(config)\n    client.warmup()\n\n    # Test data\n    test_prompts = [\n        \"Summarize: {text}\",\n        \"Extract key points: {text}\",\n        \"Analyze sentiment: {text}\",\n        \"Generate title: {text}\"\n    ]\n\n    test_texts = [\n        \"The weather is beautiful today.\",\n        \"I'm frustrated with the slow service.\",\n        \"This product exceeds all expectations.\",\n        \"The meeting was productive and insightful.\"\n    ]\n\n    # Batch processing\n    start_time = time.time()\n\n    batch_inputs = []\n    for prompt in test_prompts:\n        for text in test_texts:\n            batch_inputs.append({\"prompt\": prompt, \"text\": text})\n\n    # Process batch\n    batch_results = client.batch_complete(\n        [(item[\"prompt\"], {\"text\": item[\"text\"]}) for item in batch_inputs]\n    )\n\n    batch_duration_ms = (time.time() - start_time) * 1000\n\n    print(f\"Batch processing ({len(batch_inputs)} items): {batch_duration_ms:.1f}ms\")\n    print(f\"Average per item: {batch_duration_ms / len(batch_inputs):.1f}ms\")\n\n    return batch_results\n</code></pre>"},{"location":"user-guide/performance/#workflow-design-patterns-for-performance","title":"Workflow Design Patterns for Performance","text":""},{"location":"user-guide/performance/#efficient-workflow-patterns","title":"Efficient Workflow Patterns","text":"<pre><code>def create_high_performance_patterns():\n    \"\"\"Create various high-performance workflow patterns.\"\"\"\n\n    patterns = {}\n\n    # 1. Fan-out/Fan-in Pattern\n    patterns[\"fan_out_in\"] = create_fan_out_fan_in_workflow()\n\n    # 2. Pipeline Pattern\n    patterns[\"pipeline\"] = create_pipeline_workflow()\n\n    # 3. Conditional Processing Pattern\n    patterns[\"conditional\"] = create_conditional_workflow()\n\n    return patterns\n\ndef create_fan_out_fan_in_workflow():\n    \"\"\"Create efficient fan-out/fan-in workflow.\"\"\"\n\n    workflow = graphbit.Workflow(\"Fan-Out Fan-In Workflow\")\n\n    # Single input processor\n    input_node = graphbit.Node.agent(\n        name=\"Input Splitter\",\n        prompt=\"Split this input for parallel processing: {input}\",\n        agent_id=\"splitter\"\n    )\n\n    # Multiple parallel processors\n    processors = []\n    for i in range(3):\n        processor = graphbit.Node.agent(\n            name=f\"Processor {i+1}\",\n            prompt=f\"Process part {i+1}: {{split_input}}\",\n            agent_id=f\"processor_{i+1}\"\n        )\n        processors.append(processor)\n\n    # Single aggregator\n    aggregator = graphbit.Node.agent(\n        name=\"Result Aggregator\",\n        prompt=\"Combine results: {processor_1_output}, {processor_2_output}, {processor_3_output}\",\n        agent_id=\"aggregator\"\n    )\n\n    # Build workflow\n    input_id = workflow.add_node(input_node)\n    processor_ids = [workflow.add_node(p) for p in processors]\n    agg_id = workflow.add_node(aggregator)\n\n    # Connect: input -&gt; all processors -&gt; aggregator\n    for proc_id in processor_ids:\n        workflow.connect(input_id, proc_id)\n        workflow.connect(proc_id, agg_id)\n\n    return workflow\n\ndef create_pipeline_workflow():\n    \"\"\"Create efficient pipeline workflow.\"\"\"\n\n    workflow = graphbit.Workflow(\"Pipeline Workflow\")\n\n    # Sequential processing stages\n    stages = [\n        (\"Data Validator\", \"Validate input data: {input}\"),\n        (\"Data Processor\", \"Process validated data: {validated_data}\"),\n        (\"Data Formatter\", \"Format processed data: {processed_data}\"),\n        (\"Quality Checker\", \"Check quality: {formatted_data}\")\n    ]\n\n    nodes = []\n    for i, (name, prompt) in enumerate(stages):\n        node = graphbit.Node.agent(\n            name=name,\n            prompt=prompt,\n            agent_id=f\"stage_{i+1}\"\n        )\n        nodes.append(node)\n\n    # Build pipeline\n    node_ids = [workflow.add_node(node) for node in nodes]\n\n    # Connect sequentially\n    for i in range(len(node_ids) - 1):\n        workflow.connect(node_ids[i], node_ids[i + 1])\n\n    return workflow\n\ndef create_conditional_workflow():\n    \"\"\"Create efficient conditional workflow.\"\"\"\n\n    workflow = graphbit.Workflow(\"Conditional Workflow\")\n\n    # Input analyzer\n    analyzer = graphbit.Node.agent(\n        name=\"Input Analyzer\",\n        prompt=\"Analyze input type and complexity: {input}\",\n        agent_id=\"analyzer\"\n    )\n\n    # Condition node\n    condition = graphbit.Node.condition(\n        name=\"Complexity Check\",\n        expression=\"complexity == 'simple'\"\n    )\n\n    # Simple processor\n    simple_processor = graphbit.Node.agent(\n        name=\"Simple Processor\",\n        prompt=\"Quick processing: {input}\",\n        agent_id=\"simple_proc\"\n    )\n\n    # Complex processor\n    complex_processor = graphbit.Node.agent(\n        name=\"Complex Processor\", \n        prompt=\"Detailed processing: {input}\",\n        agent_id=\"complex_proc\"\n    )\n\n    # Build conditional workflow\n    analyzer_id = workflow.add_node(analyzer)\n    condition_id = workflow.add_node(condition)\n    simple_id = workflow.add_node(simple_processor)\n    complex_id = workflow.add_node(complex_processor)\n\n    # Connect: analyzer -&gt; condition -&gt; appropriate processor\n    workflow.connect(analyzer_id, condition_id)\n    workflow.connect(condition_id, simple_id)   # true branch\n    workflow.connect(condition_id, complex_id)  # false branch\n\n    return workflow\n</code></pre>"},{"location":"user-guide/performance/#performance-monitoring-and-profiling","title":"Performance Monitoring and Profiling","text":""},{"location":"user-guide/performance/#real-time-performance-tracking","title":"Real-time Performance Tracking","text":"<pre><code>def create_performance_profiler():\n    \"\"\"Create performance profiler for workflows.\"\"\"\n\n    class WorkflowProfiler:\n        def __init__(self):\n            self.execution_times = []\n            self.node_times = {}\n            self.memory_usage = []\n            self.throughput_data = []\n\n        def profile_execution(self, workflow, executor, iterations=1):\n            \"\"\"Profile workflow execution.\"\"\"\n\n            results = {\n                \"total_iterations\": iterations,\n                \"execution_times\": [],\n                \"average_time\": 0,\n                \"throughput_per_second\": 0,\n                \"memory_peak\": 0\n            }\n\n            print(f\"Profiling workflow with {iterations} iterations...\")\n\n            start_overall = time.time()\n\n            for i in range(iterations):\n                # Monitor memory before execution\n                baseline_memory = monitor_memory_usage()\n\n                # Time the execution\n                start_time = time.time()\n                result = executor.execute(workflow)\n                execution_time = (time.time() - start_time) * 1000\n\n                results[\"execution_times\"].append(execution_time)\n\n                # Monitor memory after execution\n                final_memory = monitor_memory_usage()\n\n                if baseline_memory and final_memory:\n                    memory_used = final_memory['rss_mb'] - baseline_memory['rss_mb']\n                    results[\"memory_peak\"] = max(results[\"memory_peak\"], memory_used)\n\n                print(f\"  Iteration {i+1}: {execution_time:.1f}ms\")\n\n            # Calculate statistics\n            total_time_seconds = time.time() - start_overall\n            results[\"average_time\"] = sum(results[\"execution_times\"]) / len(results[\"execution_times\"])\n            results[\"throughput_per_second\"] = iterations / total_time_seconds\n\n            return results\n\n        def compare_configurations(self, workflow, configs):\n            \"\"\"Compare different executor configurations.\"\"\"\n\n            comparison_results = {}\n\n            for config_name, executor in configs.items():\n                print(f\"\\nTesting configuration: {config_name}\")\n\n                profile_result = self.profile_execution(workflow, executor, iterations=3)\n                comparison_results[config_name] = profile_result\n\n            # Print comparison summary\n            print(\"\\n\" + \"=\"*50)\n            print(\"Configuration Comparison Summary\")\n            print(\"=\"*50)\n\n            for config_name, result in comparison_results.items():\n                print(f\"{config_name}:\")\n                print(f\"  Average Time: {result['average_time']:.1f}ms\")\n                print(f\"  Throughput: {result['throughput_per_second']:.2f}/sec\")\n                print(f\"  Peak Memory: {result['memory_peak']:.2f}MB\")\n\n            return comparison_results\n\n    return WorkflowProfiler()\n\ndef run_comprehensive_performance_test():\n    \"\"\"Run comprehensive performance test.\"\"\"\n\n    # Create test workflow\n    workflow = create_parallel_workflow()\n\n    # Create different executor configurations\n    executors = create_optimized_executors()\n\n    # Create profiler\n    profiler = create_performance_profiler()\n\n    # Run comparison\n    results = profiler.compare_configurations(workflow, executors)\n\n    # Find best performer\n    best_config = min(results.keys(), key=lambda k: results[k][\"average_time\"])\n\n    print(f\"\\n\ud83c\udfc6 Best performing configuration: {best_config}\")\n    print(f\"   Average time: {results[best_config]['average_time']:.1f}ms\")\n    print(f\"   Throughput: {results[best_config]['throughput_per_second']:.2f}/sec\")\n\n    return results\n</code></pre>"},{"location":"user-guide/performance/#caching-and-optimization-strategies","title":"Caching and Optimization Strategies","text":""},{"location":"user-guide/performance/#response-caching","title":"Response Caching","text":"<pre><code>def implement_response_caching():\n    \"\"\"Implement response caching for repeated queries.\"\"\"\n\n    import hashlib\n    import json\n    from typing import Dict, Any\n\n    class CachedExecutor:\n        def __init__(self, base_executor, cache_size=1000):\n            self.base_executor = base_executor\n            self.cache: Dict[str, Any] = {}\n            self.cache_size = cache_size\n            self.cache_hits = 0\n            self.cache_misses = 0\n\n        def _generate_cache_key(self, workflow, input_data=None):\n            \"\"\"Generate cache key for workflow and input.\"\"\"\n\n            # Create a hash of workflow structure and input\n            workflow_str = str(workflow)\n            input_str = json.dumps(input_data or {}, sort_keys=True)\n            combined = workflow_str + input_str\n\n            return hashlib.md5(combined.encode()).hexdigest()\n\n        def execute(self, workflow, input_data=None):\n            \"\"\"Execute workflow with caching.\"\"\"\n\n            cache_key = self._generate_cache_key(workflow, input_data)\n\n            # Check cache first\n            if cache_key in self.cache:\n                self.cache_hits += 1\n                print(f\"\ud83d\udcbe Cache hit for key: {cache_key[:8]}...\")\n                return self.cache[cache_key]\n\n            # Execute workflow\n            self.cache_misses += 1\n            print(f\"\ud83d\udd04 Cache miss, executing workflow...\")\n\n            result = self.base_executor.execute(workflow)\n\n            # Store in cache\n            if len(self.cache) &gt;= self.cache_size:\n                # Remove oldest entry (simple FIFO)\n                oldest_key = next(iter(self.cache))\n                del self.cache[oldest_key]\n\n            self.cache[cache_key] = result\n\n            return result\n\n        def get_cache_stats(self):\n            \"\"\"Get cache performance statistics.\"\"\"\n\n            total_requests = self.cache_hits + self.cache_misses\n            hit_rate = (self.cache_hits / total_requests) * 100 if total_requests &gt; 0 else 0\n\n            return {\n                \"cache_hits\": self.cache_hits,\n                \"cache_misses\": self.cache_misses,\n                \"hit_rate_percent\": hit_rate,\n                \"cache_size\": len(self.cache)\n            }\n\n    return CachedExecutor\n\ndef test_caching_performance():\n    \"\"\"Test caching performance improvement.\"\"\"\n\n    # Create base executor\n    base_executor = create_performance_optimized_executor()\n\n    # Create cached executor\n    CachedExecutorClass = implement_response_caching()\n    cached_executor = CachedExecutorClass(base_executor)\n\n    # Create simple test workflow\n    workflow = graphbit.Workflow(\"Cache Test Workflow\")\n\n    test_agent = graphbit.Node.agent(\n        name=\"Test Agent\",\n        prompt=\"Process this input: {input}\",\n        agent_id=\"test_agent\"\n    )\n\n    workflow.add_node(test_agent)\n\n    # Test with repeated executions\n    test_inputs = [\"test input 1\", \"test input 2\", \"test input 1\"]  # Repeat first input\n\n    print(\"Testing caching performance...\")\n\n    for i, test_input in enumerate(test_inputs):\n        print(f\"\\nExecution {i+1} with input: '{test_input}'\")\n\n        start_time = time.time()\n        result = cached_executor.execute(workflow)\n        duration_ms = (time.time() - start_time) * 1000\n\n        print(f\"Execution time: {duration_ms:.1f}ms\")\n\n    # Print cache statistics\n    stats = cached_executor.get_cache_stats()\n    print(f\"\\nCache Statistics:\")\n    print(f\"  Hits: {stats['cache_hits']}\")\n    print(f\"  Misses: {stats['cache_misses']}\")\n    print(f\"  Hit Rate: {stats['hit_rate_percent']:.1f}%\")\n    print(f\"  Cache Size: {stats['cache_size']}\")\n\n    return stats\n</code></pre>"},{"location":"user-guide/performance/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/performance/#1-choose-the-right-executor-type","title":"1. Choose the Right Executor Type","text":"<pre><code>def select_optimal_executor(workload_characteristics):\n    \"\"\"Select optimal executor based on workload characteristics.\"\"\"\n\n    recommendations = {\n        \"high_volume_simple\": \"high_throughput\",\n        \"real_time_interactive\": \"low_latency\", \n        \"resource_constrained\": \"memory_optimized\",\n        \"balanced_workload\": \"standard\"\n    }\n\n    workload_type = workload_characteristics.get(\"type\", \"balanced_workload\")\n    recommended_executor = recommendations.get(workload_type, \"standard\")\n\n    print(f\"Recommended executor for '{workload_type}': {recommended_executor}\")\n\n    return recommended_executor\n</code></pre>"},{"location":"user-guide/performance/#2-model-selection-for-performance","title":"2. Model Selection for Performance","text":"<pre><code>def select_optimal_model(use_case):\n    \"\"\"Select optimal model based on use case.\"\"\"\n\n    model_recommendations = {\n        \"simple_tasks\": \"gpt-4o-mini\",\n        \"complex_analysis\": \"gpt-4o\", \n        \"cost_sensitive\": \"gpt-4o-mini\",\n        \"highest_quality\": \"gpt-4o\"\n    }\n\n    recommended_model = model_recommendations.get(use_case, \"gpt-4o-mini\")\n\n    print(f\"Recommended model for '{use_case}': {recommended_model}\")\n\n    return recommended_model\n</code></pre>"},{"location":"user-guide/performance/#3-workflow-design-for-performance","title":"3. Workflow Design for Performance","text":"<pre><code>def optimize_workflow_design():\n    \"\"\"Guidelines for optimizing workflow design.\"\"\"\n\n    guidelines = {\n        \"minimize_sequential_dependencies\": \"Use parallel processing where possible\",\n        \"optimize_prompt_length\": \"Keep prompts concise and focused\",\n        \"batch_similar_operations\": \"Group similar operations together\",\n        \"use_conditional_logic\": \"Skip unnecessary processing with conditions\",\n        \"cache_common_operations\": \"Cache frequently used results\"\n    }\n\n    for guideline, description in guidelines.items():\n        print(f\"\u2705 {guideline}: {description}\")\n\n    return guidelines\n</code></pre>"},{"location":"user-guide/performance/#performance-testing-framework","title":"Performance Testing Framework","text":""},{"location":"user-guide/performance/#automated-performance-testing","title":"Automated Performance Testing","text":"<pre><code>def create_performance_test_suite():\n    \"\"\"Create comprehensive performance test suite.\"\"\"\n\n    class PerformanceTestSuite:\n        def __init__(self):\n            self.test_results = {}\n\n        def run_latency_test(self, workflow, executor, iterations=10):\n            \"\"\"Test execution latency.\"\"\"\n\n            execution_times = []\n\n            for i in range(iterations):\n                start_time = time.time()\n                result = executor.execute(workflow)\n                duration_ms = (time.time() - start_time) * 1000\n                execution_times.append(duration_ms)\n\n            return {\n                \"average_latency_ms\": sum(execution_times) / len(execution_times),\n                \"min_latency_ms\": min(execution_times),\n                \"max_latency_ms\": max(execution_times),\n                \"p95_latency_ms\": sorted(execution_times)[int(len(execution_times) * 0.95)]\n            }\n\n        def run_throughput_test(self, workflow, executor, duration_seconds=60):\n            \"\"\"Test execution throughput.\"\"\"\n\n            start_time = time.time()\n            executions = 0\n\n            while (time.time() - start_time) &lt; duration_seconds:\n                try:\n                    result = executor.execute(workflow)\n                    executions += 1\n                except Exception as e:\n                    print(f\"Execution failed: {e}\")\n\n            actual_duration = time.time() - start_time\n            throughput = executions / actual_duration\n\n            return {\n                \"executions_completed\": executions,\n                \"test_duration_seconds\": actual_duration,\n                \"throughput_per_second\": throughput\n            }\n\n        def run_stress_test(self, workflow, executor, max_concurrent=10):\n            \"\"\"Test system under stress.\"\"\"\n\n            import threading\n            import queue\n\n            results_queue = queue.Queue()\n\n            def execute_workflow():\n                try:\n                    start_time = time.time()\n                    result = executor.execute(workflow)\n                    duration_ms = (time.time() - start_time) * 1000\n                    results_queue.put({\"success\": True, \"duration_ms\": duration_ms})\n                except Exception as e:\n                    results_queue.put({\"success\": False, \"error\": str(e)})\n\n            # Launch concurrent executions\n            threads = []\n            for i in range(max_concurrent):\n                thread = threading.Thread(target=execute_workflow)\n                threads.append(thread)\n                thread.start()\n\n            # Wait for all threads to complete\n            for thread in threads:\n                thread.join()\n\n            # Collect results\n            results = []\n            while not results_queue.empty():\n                results.append(results_queue.get())\n\n            successful_executions = [r for r in results if r[\"success\"]]\n            failed_executions = [r for r in results if not r[\"success\"]]\n\n            return {\n                \"total_executions\": len(results),\n                \"successful_executions\": len(successful_executions),\n                \"failed_executions\": len(failed_executions),\n                \"success_rate_percent\": (len(successful_executions) / len(results)) * 100,\n                \"average_duration_ms\": sum(r[\"duration_ms\"] for r in successful_executions) / len(successful_executions) if successful_executions else 0\n            }\n\n        def run_complete_test_suite(self, workflow, executor):\n            \"\"\"Run complete performance test suite.\"\"\"\n\n            print(\"\ud83e\uddea Running Performance Test Suite\")\n            print(\"=\"*40)\n\n            # Latency test\n            print(\"1. Latency Test...\")\n            latency_results = self.run_latency_test(workflow, executor)\n            print(f\"   Average latency: {latency_results['average_latency_ms']:.1f}ms\")\n\n            # Throughput test\n            print(\"2. Throughput Test...\")\n            throughput_results = self.run_throughput_test(workflow, executor, duration_seconds=30)\n            print(f\"   Throughput: {throughput_results['throughput_per_second']:.2f}/sec\")\n\n            # Stress test\n            print(\"3. Stress Test...\")\n            stress_results = self.run_stress_test(workflow, executor, max_concurrent=5)\n            print(f\"   Success rate: {stress_results['success_rate_percent']:.1f}%\")\n\n            self.test_results = {\n                \"latency\": latency_results,\n                \"throughput\": throughput_results,\n                \"stress\": stress_results\n            }\n\n            return self.test_results\n\n    return PerformanceTestSuite()\n\n# Usage example\nif __name__ == \"__main__\":\n    # Initialize GraphBit\n    graphbit.init()\n\n    # Create test workflow and executor\n    test_workflow = create_memory_optimized_workflow()\n    test_executor = create_performance_optimized_executor()\n\n    # Run performance tests\n    test_suite = create_performance_test_suite()\n    results = test_suite.run_complete_test_suite(test_workflow, test_executor)\n\n    print(\"\\n\ud83d\udcca Performance Test Results Summary:\")\n    print(f\"Average Latency: {results['latency']['average_latency_ms']:.1f}ms\")\n    print(f\"Throughput: {results['throughput']['throughput_per_second']:.2f}/sec\")\n    print(f\"Stress Test Success Rate: {results['stress']['success_rate_percent']:.1f}%\")\n</code></pre>"},{"location":"user-guide/performance/#whats-next","title":"What's Next","text":"<ul> <li>Learn about Monitoring for tracking performance in production</li> <li>Explore Reliability for robust production systems</li> <li>Check Validation for ensuring performance requirements</li> <li>See LLM Providers for provider-specific optimizations </li> </ul>"},{"location":"user-guide/reliability/","title":"Reliability and Fault Tolerance","text":"<p>GraphBit provides comprehensive reliability features to ensure robust production workflows. This guide covers error handling, fault tolerance, recovery strategies, and building resilient workflow systems.</p>"},{"location":"user-guide/reliability/#overview","title":"Overview","text":"<p>Reliability in GraphBit encompasses: - Error Handling: Graceful handling of failures and exceptions - Fault Tolerance: Continuing operation despite component failures - Recovery Strategies: Automatic and manual recovery mechanisms - Circuit Breakers: Preventing cascading failures - Retry Logic: Intelligent retry patterns for transient failures - Health Monitoring: Continuous system health assessment</p>"},{"location":"user-guide/reliability/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"user-guide/reliability/#basic-error-handling","title":"Basic Error Handling","text":"<pre><code>import graphbit\nimport time\nimport os\n\n# Initialize GraphBit\ngraphbit.init()\n\ndef safe_workflow_execution(workflow, executor, max_retries=3):\n    \"\"\"Execute workflow with comprehensive error handling.\"\"\"\n\n    for attempt in range(max_retries + 1):\n        try:\n            print(f\"Execution attempt {attempt + 1}/{max_retries + 1}\")\n\n            result = executor.execute(workflow)\n\n            if result.is_completed():\n                print(\"\u2705 Workflow executed successfully\")\n                return result\n            else:\n                error_msg = result.error()\n                print(f\"\u274c Workflow failed: {error_msg}\")\n\n                if attempt &lt; max_retries:\n                    wait_time = 2 ** attempt  # Exponential backoff\n                    print(f\"\u23f3 Retrying in {wait_time} seconds...\")\n                    time.sleep(wait_time)\n                else:\n                    print(\"\u274c Max retries exceeded\")\n                    return result\n\n        except Exception as e:\n            print(f\"\u274c Execution exception: {e}\")\n\n            if attempt &lt; max_retries:\n                wait_time = 2 ** attempt\n                print(f\"\u23f3 Retrying in {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                print(\"\u274c Max retries exceeded\")\n                raise e\n\n    return None\n\ndef create_fault_tolerant_workflow():\n    \"\"\"Create workflow with built-in fault tolerance.\"\"\"\n\n    workflow = graphbit.Workflow(\"Fault Tolerant Workflow\")\n\n    # Input validator with error handling\n    validator = graphbit.Node.agent(\n        name=\"Input Validator\",\n        prompt=\"\"\"\n        Validate this input and handle any issues gracefully:\n\n        Input: {input}\n\n        If the input is invalid:\n        1. Identify the specific issues\n        2. Suggest corrections if possible\n        3. Return a status indicating validation result\n\n        If valid, return the input with validation confirmation.\n        \"\"\",\n        agent_id=\"validator\"\n    )\n\n    # Robust processor with fallback logic\n    processor = graphbit.Node.agent(\n        name=\"Robust Processor\",\n        prompt=\"\"\"\n        Process this validated input with error resilience:\n\n        Input: {validated_input}\n\n        If processing encounters issues:\n        1. Try alternative processing methods\n        2. Provide partial results if possible\n        3. Report any limitations or warnings\n\n        Always return some form of useful output.\n        \"\"\",\n        agent_id=\"processor\"\n    )\n\n    # Error recovery node\n    recovery_handler = graphbit.Node.agent(\n        name=\"Recovery Handler\",\n        prompt=\"\"\"\n        Handle any errors or partial results:\n\n        Results: {processed_results}\n\n        If there are errors or incomplete results:\n        1. Attempt data recovery\n        2. Fill in missing information where possible\n        3. Flag areas that need manual review\n        \"\"\",\n        agent_id=\"recovery_handler\"\n    )\n\n    # Build fault-tolerant chain\n    validator_id = workflow.add_node(validator)\n    processor_id = workflow.add_node(processor)\n    recovery_id = workflow.add_node(recovery_handler)\n\n    workflow.connect(validator_id, processor_id)\n    workflow.connect(processor_id, recovery_id)\n\n    return workflow\n</code></pre>"},{"location":"user-guide/reliability/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":""},{"location":"user-guide/reliability/#implementing-circuit-breakers","title":"Implementing Circuit Breakers","text":"<pre><code>from enum import Enum\nfrom datetime import datetime, timedelta\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"\n    OPEN = \"open\"\n    HALF_OPEN = \"half_open\"\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker for preventing cascading failures.\"\"\"\n\n    def __init__(self, failure_threshold=5, recovery_timeout=60, timeout_seconds=30):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout  # seconds\n        self.timeout_seconds = timeout_seconds\n\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = CircuitState.CLOSED\n\n    def can_execute(self):\n        \"\"\"Check if execution is allowed.\"\"\"\n\n        if self.state == CircuitState.CLOSED:\n            return True\n        elif self.state == CircuitState.OPEN:\n            if self.last_failure_time and \\\n               (datetime.now() - self.last_failure_time).seconds &gt;= self.recovery_timeout:\n                self.state = CircuitState.HALF_OPEN\n                return True\n            return False\n        elif self.state == CircuitState.HALF_OPEN:\n            return True\n\n        return False\n\n    def record_success(self):\n        \"\"\"Record successful execution.\"\"\"\n        self.failure_count = 0\n        self.state = CircuitState.CLOSED\n\n    def record_failure(self):\n        \"\"\"Record failed execution.\"\"\"\n        self.failure_count += 1\n        self.last_failure_time = datetime.now()\n\n        if self.failure_count &gt;= self.failure_threshold:\n            self.state = CircuitState.OPEN\n\n    def get_state(self):\n        \"\"\"Get current circuit breaker state.\"\"\"\n        return self.state\n\nclass ReliableExecutor:\n    \"\"\"Executor with circuit breaker protection.\"\"\"\n\n    def __init__(self, base_executor, circuit_breaker=None):\n        self.base_executor = base_executor\n        self.circuit_breaker = circuit_breaker or CircuitBreaker()\n\n    def execute(self, workflow):\n        \"\"\"Execute workflow with circuit breaker protection.\"\"\"\n\n        if not self.circuit_breaker.can_execute():\n            raise Exception(f\"Circuit breaker is {self.circuit_breaker.get_state().value}\")\n\n        try:\n            start_time = time.time()\n            result = self.base_executor.execute(workflow)\n            duration = time.time() - start_time\n\n            if result.is_completed():\n                self.circuit_breaker.record_success()\n                return result\n            else:\n                self.circuit_breaker.record_failure()\n                return result\n\n        except Exception as e:\n            self.circuit_breaker.record_failure()\n            raise e\n\ndef create_circuit_breaker_executor():\n    \"\"\"Create executor with circuit breaker protection.\"\"\"\n\n    # Base executor\n    llm_config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    base_executor = graphbit.Executor(llm_config)\n\n    # Circuit breaker configuration\n    circuit_breaker = CircuitBreaker(\n        failure_threshold=3,  # Open after 3 failures\n        recovery_timeout=30,  # Try again after 30 seconds\n        timeout_seconds=60    # Individual execution timeout\n    )\n\n    reliable_executor = ReliableExecutor(base_executor, circuit_breaker)\n\n    return reliable_executor\n</code></pre>"},{"location":"user-guide/reliability/#retry-strategies","title":"Retry Strategies","text":""},{"location":"user-guide/reliability/#advanced-retry-logic","title":"Advanced Retry Logic","text":"<pre><code>import random\nfrom typing import Callable, Optional\n\nclass RetryStrategy:\n    \"\"\"Base class for retry strategies.\"\"\"\n\n    def get_wait_time(self, attempt: int) -&gt; float:\n        \"\"\"Get wait time for given attempt number.\"\"\"\n        raise NotImplementedError\n\nclass ExponentialBackoff(RetryStrategy):\n    \"\"\"Exponential backoff retry strategy.\"\"\"\n\n    def __init__(self, base_delay=1.0, max_delay=60.0, multiplier=2.0):\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.multiplier = multiplier\n\n    def get_wait_time(self, attempt: int) -&gt; float:\n        delay = self.base_delay * (self.multiplier ** attempt)\n        return min(delay, self.max_delay)\n\nclass JitteredBackoff(RetryStrategy):\n    \"\"\"Exponential backoff with jitter to avoid thundering herd.\"\"\"\n\n    def __init__(self, base_delay=1.0, max_delay=60.0, multiplier=2.0, jitter_ratio=0.1):\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.multiplier = multiplier\n        self.jitter_ratio = jitter_ratio\n\n    def get_wait_time(self, attempt: int) -&gt; float:\n        delay = self.base_delay * (self.multiplier ** attempt)\n\n        # Add jitter\n        jitter = delay * self.jitter_ratio * random.random()\n        delay += jitter\n\n        return min(delay, self.max_delay)\n\nclass RetryableExecutor:\n    \"\"\"Executor with configurable retry strategies.\"\"\"\n\n    def __init__(self, base_executor, retry_strategy=None, max_retries=3):\n        self.base_executor = base_executor\n        self.retry_strategy = retry_strategy or ExponentialBackoff()\n        self.max_retries = max_retries\n\n    def execute(self, workflow, retry_condition: Optional[Callable] = None):\n        \"\"\"Execute workflow with retry logic.\"\"\"\n\n        last_exception = None\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                result = self.base_executor.execute(workflow)\n\n                if result.is_completed():\n                    if attempt &gt; 0:\n                        print(f\"\u2705 Workflow succeeded on attempt {attempt + 1}\")\n                    return result\n                else:\n                    # Check if this failure should trigger a retry\n                    if retry_condition and not retry_condition(result):\n                        print(f\"\u274c Non-retryable failure: {result.error()}\")\n                        return result\n\n                    if attempt &lt; self.max_retries:\n                        wait_time = self.retry_strategy.get_wait_time(attempt)\n                        print(f\"\u23f3 Workflow failed, retrying in {wait_time:.1f}s (attempt {attempt + 1})\")\n                        time.sleep(wait_time)\n                    else:\n                        print(f\"\u274c Workflow failed after {self.max_retries + 1} attempts\")\n                        return result\n\n            except Exception as e:\n                last_exception = e\n\n                if attempt &lt; self.max_retries:\n                    wait_time = self.retry_strategy.get_wait_time(attempt)\n                    print(f\"\u23f3 Exception occurred, retrying in {wait_time:.1f}s: {e}\")\n                    time.sleep(wait_time)\n                else:\n                    print(f\"\u274c Exception after {self.max_retries + 1} attempts: {e}\")\n                    raise e\n\n        if last_exception:\n            raise last_exception\n</code></pre>"},{"location":"user-guide/reliability/#health-monitoring-and-recovery","title":"Health Monitoring and Recovery","text":""},{"location":"user-guide/reliability/#health-check-system","title":"Health Check System","text":"<pre><code>class HealthChecker:\n    \"\"\"Comprehensive health monitoring system.\"\"\"\n\n    def __init__(self):\n        self.health_checks = {}\n        self.health_history = []\n\n    def register_health_check(self, name: str, check_func: Callable, critical: bool = True):\n        \"\"\"Register a health check function.\"\"\"\n        self.health_checks[name] = {\n            \"func\": check_func,\n            \"critical\": critical,\n            \"last_result\": None,\n            \"last_check\": None\n        }\n\n    def run_health_checks(self):\n        \"\"\"Run all registered health checks.\"\"\"\n\n        results = {}\n        overall_healthy = True\n\n        for name, check_config in self.health_checks.items():\n            try:\n                start_time = time.time()\n                result = check_config[\"func\"]()\n                duration = (time.time() - start_time) * 1000\n\n                check_result = {\n                    \"healthy\": bool(result),\n                    \"duration_ms\": duration,\n                    \"timestamp\": datetime.now(),\n                    \"details\": result if isinstance(result, dict) else {}\n                }\n\n                results[name] = check_result\n                check_config[\"last_result\"] = check_result\n                check_config[\"last_check\"] = datetime.now()\n\n                # Update overall health\n                if check_config[\"critical\"] and not check_result[\"healthy\"]:\n                    overall_healthy = False\n\n            except Exception as e:\n                check_result = {\n                    \"healthy\": False,\n                    \"error\": str(e),\n                    \"timestamp\": datetime.now()\n                }\n\n                results[name] = check_result\n                check_config[\"last_result\"] = check_result\n                check_config[\"last_check\"] = datetime.now()\n\n                if check_config[\"critical\"]:\n                    overall_healthy = False\n\n        health_report = {\n            \"overall_healthy\": overall_healthy,\n            \"timestamp\": datetime.now(),\n            \"checks\": results\n        }\n\n        self.health_history.append(health_report)\n\n        # Keep only last 100 health checks\n        if len(self.health_history) &gt; 100:\n            self.health_history = self.health_history[-100:]\n\n        return health_report\n\ndef create_health_checks():\n    \"\"\"Create standard health checks for GraphBit.\"\"\"\n\n    def check_graphbit_system():\n        \"\"\"Check GraphBit system health.\"\"\"\n        try:\n            system_info = graphbit.get_system_info()\n            health_check = graphbit.health_check()\n\n            return {\n                \"system_available\": True,\n                \"runtime_initialized\": system_info.get(\"runtime_initialized\", False),\n                \"health_check\": health_check\n            }\n        except Exception as e:\n            return False\n\n    def check_llm_connectivity():\n        \"\"\"Check LLM provider connectivity.\"\"\"\n        try:\n            config = graphbit.LlmConfig.openai(\n                api_key=os.getenv(\"OPENAI_API_KEY\"),\n                model=\"gpt-4o-mini\"\n            )\n\n            client = graphbit.LlmClient(config)\n\n            # Simple test completion\n            response = client.complete(\"Test connectivity: {input}\", {\"input\": \"test\"})\n\n            return {\n                \"provider_accessible\": True,\n                \"response_received\": len(response) &gt; 0\n            }\n        except Exception as e:\n            return False\n\n    # Create health checker and register checks\n    health_checker = HealthChecker()\n\n    health_checker.register_health_check(\"graphbit_system\", check_graphbit_system, critical=True)\n    health_checker.register_health_check(\"llm_connectivity\", check_llm_connectivity, critical=True)\n\n    return health_checker\n</code></pre>"},{"location":"user-guide/reliability/#fallback-and-degradation-strategies","title":"Fallback and Degradation Strategies","text":""},{"location":"user-guide/reliability/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>class FallbackWorkflow:\n    \"\"\"Workflow with multiple fallback levels.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n        self.primary_workflow = None\n        self.fallback_workflows = []\n\n    def set_primary_workflow(self, workflow):\n        \"\"\"Set the primary workflow.\"\"\"\n        self.primary_workflow = workflow\n\n    def add_fallback_workflow(self, workflow, priority=1):\n        \"\"\"Add a fallback workflow with priority.\"\"\"\n        self.fallback_workflows.append({\n            \"workflow\": workflow,\n            \"priority\": priority\n        })\n\n        # Sort by priority (lower numbers = higher priority)\n        self.fallback_workflows.sort(key=lambda x: x[\"priority\"])\n\n    def execute(self, executor):\n        \"\"\"Execute workflow with fallback chain.\"\"\"\n\n        # Try primary workflow first\n        if self.primary_workflow:\n            try:\n                print(\"\ud83d\ude80 Attempting primary workflow...\")\n                result = executor.execute(self.primary_workflow)\n\n                if result.is_completed():\n                    print(\"\u2705 Primary workflow succeeded\")\n                    return result\n                else:\n                    print(f\"\u274c Primary workflow failed: {result.error()}\")\n\n            except Exception as e:\n                print(f\"\u274c Primary workflow exception: {e}\")\n\n        # Try fallback workflows in priority order\n        for i, fallback in enumerate(self.fallback_workflows):\n            try:\n                print(f\"\ud83d\udd04 Attempting fallback {i+1}...\")\n                result = executor.execute(fallback[\"workflow\"])\n\n                if result.is_completed():\n                    print(f\"\u2705 Fallback {i+1} succeeded\")\n                    return result\n                else:\n                    print(f\"\u274c Fallback {i+1} failed: {result.error()}\")\n\n            except Exception as e:\n                print(f\"\u274c Fallback {i+1} exception: {e}\")\n\n        print(\"\u274c All workflows failed\")\n        return None\n\ndef create_degraded_processing_workflows():\n    \"\"\"Create workflows with different levels of processing.\"\"\"\n\n    # High-quality processing (primary)\n    primary_workflow = graphbit.Workflow(\"High Quality Processing\")\n\n    primary_processor = graphbit.Node.agent(\n        name=\"Advanced Processor\",\n        prompt=\"\"\"\n        Perform comprehensive analysis of this input:\n\n        Input: {input}\n\n        Provide:\n        1. Detailed analysis\n        2. Multiple perspectives\n        3. Confidence scores\n        4. Recommendations\n        5. Risk assessment\n        \"\"\",\n        agent_id=\"advanced_processor\"\n    )\n\n    primary_workflow.add_node(primary_processor)\n\n    # Basic processing (fallback)\n    fallback_workflow = graphbit.Workflow(\"Basic Processing\")\n\n    basic_processor = graphbit.Node.agent(\n        name=\"Basic Processor\",\n        prompt=\"Provide a simple summary of: {input}\",\n        agent_id=\"basic_processor\"\n    )\n\n    fallback_workflow.add_node(basic_processor)\n\n    # Emergency processing (last resort)\n    emergency_workflow = graphbit.Workflow(\"Emergency Processing\")\n\n    emergency_processor = graphbit.Node.transform(\n        name=\"Emergency Processor\",\n        transformation=\"uppercase\"  # Simple transformation as last resort\n    )\n\n    emergency_workflow.add_node(emergency_processor)\n\n    # Create fallback workflow\n    fallback_system = FallbackWorkflow(\"Degraded Processing System\")\n    fallback_system.set_primary_workflow(primary_workflow)\n    fallback_system.add_fallback_workflow(fallback_workflow, priority=1)\n    fallback_system.add_fallback_workflow(emergency_workflow, priority=2)\n\n    return fallback_system\n</code></pre>"},{"location":"user-guide/reliability/#production-reliability-patterns","title":"Production Reliability Patterns","text":""},{"location":"user-guide/reliability/#complete-reliability-stack","title":"Complete Reliability Stack","text":"<pre><code>class ProductionExecutor:\n    \"\"\"Production-ready executor with full reliability stack.\"\"\"\n\n    def __init__(self, llm_config):\n        # Base executor\n        self.base_executor = graphbit.Executor(llm_config)\n\n        # Reliability components\n        self.circuit_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)\n        self.retry_strategy = JitteredBackoff(base_delay=1.0, max_delay=30.0)\n        self.health_checker = create_health_checks()\n\n        # Metrics\n        self.execution_count = 0\n        self.success_count = 0\n        self.failure_count = 0\n\n    def execute(self, workflow, execution_id=None):\n        \"\"\"Execute workflow with full reliability features.\"\"\"\n\n        self.execution_count += 1\n\n        # Generate execution ID if not provided\n        if execution_id is None:\n            execution_id = f\"prod_exec_{int(time.time())}_{self.execution_count}\"\n\n        # Health check before execution\n        health_report = self.health_checker.run_health_checks()\n        if not health_report[\"overall_healthy\"]:\n            self.failure_count += 1\n            raise Exception(\"System health check failed\")\n\n        # Circuit breaker check\n        if not self.circuit_breaker.can_execute():\n            self.failure_count += 1\n            raise Exception(f\"Circuit breaker is {self.circuit_breaker.get_state().value}\")\n\n        # Retry loop\n        max_retries = 3\n        for attempt in range(max_retries + 1):\n            try:\n                # Execute workflow\n                result = self.base_executor.execute(workflow)\n\n                if result.is_completed():\n                    self.success_count += 1\n                    self.circuit_breaker.record_success()\n                    return result\n                else:\n                    self.failure_count += 1\n                    self.circuit_breaker.record_failure()\n\n                    if attempt &lt; max_retries:\n                        wait_time = self.retry_strategy.get_wait_time(attempt)\n                        print(f\"\u23f3 Retrying in {wait_time:.1f}s (attempt {attempt + 1})\")\n                        time.sleep(wait_time)\n                    else:\n                        return result\n\n            except Exception as e:\n                self.failure_count += 1\n                self.circuit_breaker.record_failure()\n\n                if attempt &lt; max_retries:\n                    wait_time = self.retry_strategy.get_wait_time(attempt)\n                    print(f\"\u23f3 Retrying after exception in {wait_time:.1f}s: {e}\")\n                    time.sleep(wait_time)\n                else:\n                    raise e\n\n    def get_reliability_metrics(self):\n        \"\"\"Get reliability metrics.\"\"\"\n\n        success_rate = (self.success_count / self.execution_count) * 100 if self.execution_count &gt; 0 else 0\n\n        return {\n            \"total_executions\": self.execution_count,\n            \"successful_executions\": self.success_count,\n            \"failed_executions\": self.failure_count,\n            \"success_rate_percent\": success_rate,\n            \"circuit_breaker_state\": self.circuit_breaker.get_state().value\n        }\n\ndef create_production_executor():\n    \"\"\"Create production-ready executor.\"\"\"\n\n    llm_config = graphbit.LlmConfig.openai(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model=\"gpt-4o-mini\"\n    )\n\n    return ProductionExecutor(llm_config)\n</code></pre>"},{"location":"user-guide/reliability/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/reliability/#1-reliability-design-principles","title":"1. Reliability Design Principles","text":"<pre><code>def get_reliability_best_practices():\n    \"\"\"Get best practices for building reliable workflows.\"\"\"\n\n    best_practices = {\n        \"fail_fast\": \"Detect and report failures quickly\",\n        \"graceful_degradation\": \"Provide reduced functionality when components fail\",\n        \"idempotency\": \"Ensure operations can be safely repeated\",\n        \"timeout_management\": \"Set appropriate timeouts for all operations\",\n        \"resource_cleanup\": \"Always clean up resources, even on failure\",\n        \"monitoring\": \"Continuously monitor system health and performance\",\n        \"testing\": \"Test failure scenarios regularly\"\n    }\n\n    for practice, description in best_practices.items():\n        print(f\"\u2705 {practice.replace('_', ' ').title()}: {description}\")\n\n    return best_practices\n</code></pre>"},{"location":"user-guide/reliability/#2-error-classification","title":"2. Error Classification","text":"<pre><code>def classify_error_type(error):\n    \"\"\"Classify errors for appropriate handling.\"\"\"\n\n    error_message = str(error).lower()\n\n    # Transient errors - should retry\n    if any(keyword in error_message for keyword in [\n        \"timeout\", \"network\", \"connection\", \"temporary\", \"rate limit\"\n    ]):\n        return \"transient\"\n\n    # Permanent errors - should not retry\n    if any(keyword in error_message for keyword in [\n        \"authentication\", \"permission\", \"not found\", \"invalid\"\n    ]):\n        return \"permanent\"\n\n    # System errors - may need recovery\n    if any(keyword in error_message for keyword in [\n        \"memory\", \"disk\", \"resource\", \"capacity\"\n    ]):\n        return \"system\"\n\n    # Unknown errors - handle conservatively\n    return \"unknown\"\n</code></pre>"},{"location":"user-guide/reliability/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/reliability/#production-reliability-example","title":"Production Reliability Example","text":"<pre><code>def example_production_usage():\n    \"\"\"Example of production reliability patterns.\"\"\"\n\n    # Initialize GraphBit\n    graphbit.init()\n\n    # Create production executor\n    prod_executor = create_production_executor()\n\n    # Create reliable workflow\n    workflow = create_fault_tolerant_workflow()\n\n    try:\n        # Execute with full reliability features\n        result = prod_executor.execute(workflow, execution_id=\"example_prod_run\")\n\n        if result.is_completed():\n            print(f\"\u2705 Production execution successful: {result.output()}\")\n        else:\n            print(f\"\u274c Production execution failed: {result.error()}\")\n\n        # Print reliability metrics\n        metrics = prod_executor.get_reliability_metrics()\n        print(f\"\ud83d\udcca Reliability Metrics: {metrics}\")\n\n    except Exception as e:\n        print(f\"\u274c Production execution exception: {e}\")\n\nif __name__ == \"__main__\":\n    example_production_usage()\n</code></pre>"},{"location":"user-guide/reliability/#whats-next","title":"What's Next","text":"<ul> <li>Learn about Monitoring for tracking reliability metrics</li> <li>Explore Performance optimization for reliable systems  </li> <li>Check Validation for ensuring system correctness</li> <li>See LLM Providers for provider-specific reliability features </li> </ul>"},{"location":"user-guide/text-splitters/","title":"Text Splitters","text":"<p>Text splitters are essential components for processing large documents by breaking them into manageable chunks while maintaining context and semantic coherence. GraphBit provides various text splitting strategies optimized for different use cases.</p>"},{"location":"user-guide/text-splitters/#overview","title":"Overview","text":"<p>Text splitters help you: - Process large documents that exceed model context windows - Create embeddings for semantic search - Parallelize document processing - Maintain context across chunk boundaries with overlapping</p>"},{"location":"user-guide/text-splitters/#available-splitters","title":"Available Splitters","text":""},{"location":"user-guide/text-splitters/#character-splitter","title":"Character Splitter","text":"<p>Splits text based on character count, ideal for simple use cases where exact chunk sizes are needed.</p> <pre><code>import graphbit\n\n# Initialize GraphBit\ngraphbit.init()\n\n# Create a character splitter\nsplitter = graphbit.CharacterSplitter(\n    chunk_size=1000,      # Maximum characters per chunk\n    chunk_overlap=200     # Overlap between chunks\n)\n\n# Split text\ntext = \"Your long document text here...\"\nchunks = splitter.split_text(text)\n\n# Process chunks\nfor chunk in chunks:\n    print(f\"Chunk {chunk.chunk_index}: {len(chunk.content)} characters\")\n    print(f\"Position: {chunk.start_index} to {chunk.end_index}\")\n</code></pre>"},{"location":"user-guide/text-splitters/#token-splitter","title":"Token Splitter","text":"<p>Splits text based on token count, useful when working with language models that have token limits.</p> <pre><code># Create a token splitter\nsplitter = graphbit.TokenSplitter(\n    chunk_size=100,       # Maximum tokens per chunk\n    chunk_overlap=20,     # Token overlap\n    token_pattern=None    # Optional custom regex pattern\n)\n\n# Custom token pattern example\ncustom_splitter = graphbit.TokenSplitter(\n    chunk_size=50,\n    chunk_overlap=10,\n    token_pattern=r'\\b\\w+\\b'  # Split only on words\n)\n</code></pre>"},{"location":"user-guide/text-splitters/#sentence-splitter","title":"Sentence Splitter","text":"<p>Maintains sentence boundaries, perfect for preserving semantic units.</p> <pre><code># Create a sentence splitter\nsplitter = graphbit.SentenceSplitter(\n    chunk_size=500,       # Target size in characters\n    chunk_overlap=1       # Number of sentences to overlap\n)\n\n# Custom sentence endings for multilingual text\nmultilingual_splitter = graphbit.SentenceSplitter(\n    chunk_size=500,\n    chunk_overlap=1,\n    sentence_endings=[r\"\\.\", r\"!\", r\"\\?\", r\"\u3002\", r\"\uff01\", r\"\uff1f\"]\n)\n</code></pre>"},{"location":"user-guide/text-splitters/#recursive-splitter","title":"Recursive Splitter","text":"<p>Hierarchically splits text using multiple separators, ideal for structured documents.</p> <pre><code># Create a recursive splitter\nsplitter = graphbit.RecursiveSplitter(\n    chunk_size=1000,\n    chunk_overlap=100\n)\n\n# Custom separators for specific document types\ncustom_splitter = graphbit.RecursiveSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n)\n</code></pre>"},{"location":"user-guide/text-splitters/#configuration-based-splitters","title":"Configuration-Based Splitters","text":"<p>Use <code>TextSplitterConfig</code> for more control and flexibility:</p> <pre><code># Character configuration\nconfig = graphbit.TextSplitterConfig.character(\n    chunk_size=1000,\n    chunk_overlap=200\n)\n\n# Token configuration\nconfig = graphbit.TextSplitterConfig.token(\n    chunk_size=100,\n    chunk_overlap=20,\n    token_pattern=r'\\w+'\n)\n\n# Code splitter configuration\nconfig = graphbit.TextSplitterConfig.code(\n    chunk_size=500,\n    chunk_overlap=50,\n    language=\"python\"\n)\n\n# Markdown splitter configuration\nconfig = graphbit.TextSplitterConfig.markdown(\n    chunk_size=1000,\n    chunk_overlap=100,\n    split_by_headers=True\n)\n\n# Create splitter from config\nsplitter = graphbit.TextSplitter(config)\n</code></pre>"},{"location":"user-guide/text-splitters/#advanced-features","title":"Advanced Features","text":""},{"location":"user-guide/text-splitters/#processing-multiple-documents","title":"Processing Multiple Documents","text":"<pre><code>splitter = graphbit.CharacterSplitter(1000, 200)\n\n# Split multiple texts at once\ntexts = [\n    \"First document content...\",\n    \"Second document content...\",\n    \"Third document content...\"\n]\n\nall_chunks = splitter.split_texts(texts)\n\nfor doc_idx, chunks in enumerate(all_chunks):\n    print(f\"Document {doc_idx}: {len(chunks)} chunks\")\n</code></pre>"},{"location":"user-guide/text-splitters/#working-with-chunk-metadata","title":"Working with Chunk Metadata","text":"<pre><code>chunks = splitter.split_text(text)\n\nfor chunk in chunks:\n    # Access chunk properties\n    print(f\"Content: {chunk.content}\")\n    print(f\"Index: {chunk.chunk_index}\")\n    print(f\"Position: {chunk.start_index} to {chunk.end_index}\")\n\n    # Access metadata\n    metadata = chunk.metadata\n    print(f\"Length: {metadata['length']}\")\n</code></pre>"},{"location":"user-guide/text-splitters/#creating-documents-for-vector-stores","title":"Creating Documents for Vector Stores","text":"<pre><code>splitter = graphbit.TextSplitter(\n    graphbit.TextSplitterConfig.character(1000, 200)\n)\n\n# Create documents with metadata\ndocuments = splitter.create_documents(text)\n\n# Documents are dictionaries ready for vector stores\nfor doc in documents:\n    print(doc['content'])\n    print(doc['start_index'])\n    print(doc['end_index'])\n    print(doc['chunk_index'])\n</code></pre>"},{"location":"user-guide/text-splitters/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/text-splitters/#1-choose-the-right-splitter","title":"1. Choose the Right Splitter","text":"<ul> <li>Character Splitter: Simple documents, consistent chunk sizes</li> <li>Token Splitter: Working with LLMs, precise token control</li> <li>Sentence Splitter: Maintaining semantic boundaries</li> <li>Recursive Splitter: Structured documents, code files</li> </ul>"},{"location":"user-guide/text-splitters/#2-optimize-chunk-size","title":"2. Optimize Chunk Size","text":"<p>Consider: - Model context window limits - Embedding model requirements - Processing efficiency - Semantic coherence</p> <p>Common sizes: - Embeddings: 500-1000 characters - LLM processing: 2000-4000 characters - Summarization: 1000-2000 characters</p>"},{"location":"user-guide/text-splitters/#3-use-appropriate-overlap","title":"3. Use Appropriate Overlap","text":"<ul> <li>Small overlap (10-20%): General documents</li> <li>Medium overlap (20-30%): Technical content</li> <li>Large overlap (30-50%): Dense information</li> </ul>"},{"location":"user-guide/text-splitters/#4-handle-special-content","title":"4. Handle Special Content","text":""},{"location":"user-guide/text-splitters/#code-files","title":"Code Files","text":"<pre><code>config = graphbit.TextSplitterConfig.code(\n    chunk_size=1000,\n    chunk_overlap=100,\n    language=\"python\"\n)\nconfig.set_trim_whitespace(False)  # Preserve formatting\n</code></pre>"},{"location":"user-guide/text-splitters/#markdown-documents","title":"Markdown Documents","text":"<pre><code>config = graphbit.TextSplitterConfig.markdown(\n    chunk_size=1500,\n    chunk_overlap=200,\n    split_by_headers=True\n)\n</code></pre>"},{"location":"user-guide/text-splitters/#unicode-and-multilingual-text","title":"Unicode and Multilingual Text","text":"<pre><code># All splitters handle Unicode correctly\nsplitter = graphbit.CharacterSplitter(100, 20)\ntext = \"Hello \u4e16\u754c! Emoji support \ud83d\ude80\"\nchunks = splitter.split_text(text)  # Works seamlessly\n</code></pre>"},{"location":"user-guide/text-splitters/#integration-with-graphbit-workflows","title":"Integration with GraphBit Workflows","text":"<p>Text splitters integrate seamlessly with other GraphBit components:</p> <pre><code>import graphbit\n\n# Initialize\ngraphbit.init()\n\n# Create components\nsplitter = graphbit.RecursiveSplitter(1000, 100)\nembedder = graphbit.EmbeddingClient(\n    graphbit.EmbeddingConfig.openai(\"your-api-key\")\n)\n\n# Process document\ntext = \"Your large document...\"\nchunks = splitter.split_text(text)\n\n# Generate embeddings for each chunk\nembeddings = []\nfor chunk in chunks:\n    embedding = embedder.embed_text(chunk.content)\n    embeddings.append({\n        'content': chunk.content,\n        'embedding': embedding,\n        'metadata': chunk.metadata\n    })\n</code></pre>"},{"location":"user-guide/text-splitters/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    # Invalid configuration\n    splitter = graphbit.CharacterSplitter(\n        chunk_size=0,  # Error: must be &gt; 0\n        chunk_overlap=0\n    )\nexcept Exception as e:\n    print(f\"Configuration error: {e}\")\n\n# Safe splitting with validation\ndef safe_split(text, chunk_size=1000, chunk_overlap=200):\n    if chunk_size &lt;= 0:\n        raise ValueError(\"Chunk size must be positive\")\n    if chunk_overlap &gt;= chunk_size:\n        raise ValueError(\"Overlap must be less than chunk size\")\n\n    splitter = graphbit.CharacterSplitter(chunk_size, chunk_overlap)\n    return splitter.split_text(text)\n</code></pre>"},{"location":"user-guide/text-splitters/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Memory Usage: Text splitters process text efficiently without loading entire documents into memory</li> <li>Processing Speed: Character and recursive splitters are fastest; token splitters are slower due to regex processing</li> <li>Unicode Handling: All splitters correctly handle multi-byte characters without performance penalties</li> </ol>"},{"location":"user-guide/text-splitters/#summary","title":"Summary","text":"<p>GraphBit's text splitters provide: - Multiple splitting strategies for different use cases - Proper Unicode and multilingual support - Configurable overlap for context preservation - Integration with GraphBit's workflow system - Production-ready error handling and validation</p> <p>Choose the appropriate splitter based on your content type and processing requirements, and leverage the configuration options to fine-tune behavior for optimal results. </p>"},{"location":"user-guide/validation/","title":"Validation","text":"<p>GraphBit provides comprehensive validation capabilities to ensure workflow integrity, data quality, and execution reliability. This guide covers all aspects of validation in GraphBit workflows.</p>"},{"location":"user-guide/validation/#overview","title":"Overview","text":"<p>GraphBit validation operates at multiple levels: - Workflow Validation: Structure and connectivity validation - Input Validation: Data format and content validation - Execution Validation: Runtime validation and error handling - Configuration Validation: LLM and embedding configuration validation - Output Validation: Result quality and format validation</p>"},{"location":"user-guide/validation/#workflow-validation","title":"Workflow Validation","text":""},{"location":"user-guide/validation/#basic-workflow-validation","title":"Basic Workflow Validation","text":"<pre><code>import graphbit\n\n# Initialize GraphBit\ngraphbit.init()\n\ndef validate_workflow_basic():\n    \"\"\"Basic workflow validation example.\"\"\"\n\n    # Create a workflow\n    workflow = graphbit.Workflow(\"Basic Validation Test\")\n\n    # Add nodes\n    processor = graphbit.Node.agent(\n        name=\"Data Processor\",\n        prompt=\"Process this data: {input}\",\n        agent_id=\"processor\"\n    )\n\n    validator = graphbit.Node.agent(\n        name=\"Result Validator\",\n        prompt=\"Validate these results: {processed_data}\",\n        agent_id=\"validator\"\n    )\n\n    processor_id = workflow.add_node(processor)\n    validator_id = workflow.add_node(validator)\n\n    # Connect nodes\n    workflow.connect(processor_id, validator_id)\n\n    # Validate workflow structure\n    try:\n        workflow.validate()\n        print(\"\u2705 Workflow validation passed\")\n        return True\n    except Exception as e:\n        print(f\"\u274c Workflow validation failed: {e}\")\n        return False\n\ndef validate_workflow_comprehensive():\n    \"\"\"Comprehensive workflow validation with detailed checks.\"\"\"\n\n    workflow = graphbit.Workflow(\"Comprehensive Validation Test\")\n\n    # Create nodes with various types\n    input_node = graphbit.Node.agent(\n        name=\"Input Handler\",\n        prompt=\"Handle input: {input}\",\n        agent_id=\"input_handler\"\n    )\n\n    transform_node = graphbit.Node.transform(\n        name=\"Data Transformer\",\n        transformation=\"uppercase\"\n    )\n\n    condition_node = graphbit.Node.condition(\n        name=\"Quality Check\",\n        expression=\"quality_score &gt; 0.8\"\n    )\n\n    output_node = graphbit.Node.agent(\n        name=\"Output Generator\",\n        prompt=\"Generate output: {transformed_data}\",\n        agent_id=\"output_generator\"\n    )\n\n    # Add nodes to workflow\n    input_id = workflow.add_node(input_node)\n    transform_id = workflow.add_node(transform_node)\n    condition_id = workflow.add_node(condition_node)\n    output_id = workflow.add_node(output_node)\n\n    # Create connections\n    workflow.connect(input_id, transform_id)\n    workflow.connect(transform_id, condition_id)\n    workflow.connect(condition_id, output_id)\n\n    # Perform validation\n    validation_results = {\n        \"structure_valid\": False,\n        \"nodes_valid\": False,\n        \"connections_valid\": False,\n        \"no_cycles\": False\n    }\n\n    try:\n        # Basic structure validation\n        workflow.validate()\n        validation_results[\"structure_valid\"] = True\n\n        # Additional validation checks would go here\n        # (These are conceptual - actual implementation depends on GraphBit internals)\n        validation_results[\"nodes_valid\"] = True\n        validation_results[\"connections_valid\"] = True\n        validation_results[\"no_cycles\"] = True\n\n        print(\"\u2705 Comprehensive workflow validation passed\")\n        print(f\"Validation results: {validation_results}\")\n        return True\n\n    except Exception as e:\n        print(f\"\u274c Comprehensive workflow validation failed: {e}\")\n        print(f\"Validation results: {validation_results}\")\n        return False\n</code></pre>"},{"location":"user-guide/validation/#input-validation","title":"Input Validation","text":""},{"location":"user-guide/validation/#data-format-validation","title":"Data Format Validation","text":"<pre><code>def create_input_validation_workflow():\n    \"\"\"Create workflow with robust input validation.\"\"\"\n\n    workflow = graphbit.Workflow(\"Input Validation Workflow\")\n\n    # Input validator node\n    input_validator = graphbit.Node.agent(\n        name=\"Input Validator\",\n        prompt=\"\"\"\n        Validate the following input data:\n\n        Input: {input}\n\n        Check for:\n        1. Required fields are present\n        2. Data types are correct\n        3. Values are within expected ranges\n        4. Format follows expected patterns\n\n        Return validation results with:\n        - is_valid: true/false\n        - errors: list of validation errors\n        - warnings: list of validation warnings\n        - sanitized_input: cleaned input data\n        \"\"\",\n        agent_id=\"input_validator\"\n    )\n\n    # Data sanitizer\n    data_sanitizer = graphbit.Node.agent(\n        name=\"Data Sanitizer\",\n        prompt=\"\"\"\n        Sanitize and clean the validated input:\n\n        Validation Results: {validation_results}\n\n        If validation passed:\n        1. Remove any unsafe content\n        2. Normalize data formats\n        3. Apply standard transformations\n        4. Return clean data\n\n        If validation failed:\n        1. Attempt data recovery where possible\n        2. Provide fallback values for missing data\n        3. Flag critical issues that need attention\n        \"\"\",\n        agent_id=\"data_sanitizer\"\n    )\n\n    # Build validation workflow\n    validator_id = workflow.add_node(input_validator)\n    sanitizer_id = workflow.add_node(data_sanitizer)\n\n    workflow.connect(validator_id, sanitizer_id)\n\n    return workflow\n\ndef validate_input_data(data, expected_schema=None):\n    \"\"\"Validate input data against expected schema.\"\"\"\n\n    validation_results = {\n        \"is_valid\": True,\n        \"errors\": [],\n        \"warnings\": [],\n        \"sanitized_data\": data\n    }\n\n    # Basic type validation\n    if data is None:\n        validation_results[\"is_valid\"] = False\n        validation_results[\"errors\"].append(\"Input data is None\")\n        return validation_results\n\n    # String validation\n    if isinstance(data, str):\n        if len(data.strip()) == 0:\n            validation_results[\"is_valid\"] = False\n            validation_results[\"errors\"].append(\"Input string is empty\")\n        elif len(data) &gt; 10000:  # Arbitrary limit\n            validation_results[\"warnings\"].append(\"Input string is very long\")\n\n        # Sanitize string\n        validation_results[\"sanitized_data\"] = data.strip()\n\n    # Dictionary validation\n    elif isinstance(data, dict):\n        if expected_schema:\n            for required_field in expected_schema.get(\"required\", []):\n                if required_field not in data:\n                    validation_results[\"is_valid\"] = False\n                    validation_results[\"errors\"].append(f\"Required field '{required_field}' missing\")\n\n        # Sanitize dictionary\n        sanitized_dict = {}\n        for key, value in data.items():\n            if isinstance(value, str):\n                sanitized_dict[key] = value.strip()\n            else:\n                sanitized_dict[key] = value\n\n        validation_results[\"sanitized_data\"] = sanitized_dict\n\n    # List validation\n    elif isinstance(data, list):\n        if len(data) == 0:\n            validation_results[\"warnings\"].append(\"Input list is empty\")\n        elif len(data) &gt; 1000:  # Arbitrary limit\n            validation_results[\"warnings\"].append(\"Input list is very large\")\n\n    return validation_results\n\ndef example_input_validation():\n    \"\"\"Example of input validation in practice.\"\"\"\n\n    # Test various input types\n    test_inputs = [\n        {\"text\": \"Hello, world!\", \"priority\": 1},\n        {\"text\": \"\", \"priority\": \"high\"},  # Invalid: empty text, wrong type\n        None,  # Invalid: null input\n        {\"text\": \"Valid input\", \"priority\": 2, \"metadata\": {\"source\": \"test\"}},\n        []  # Warning: empty list\n    ]\n\n    schema = {\n        \"required\": [\"text\", \"priority\"],\n        \"types\": {\n            \"text\": str,\n            \"priority\": int\n        }\n    }\n\n    for i, test_input in enumerate(test_inputs):\n        print(f\"\\nValidating input {i + 1}: {test_input}\")\n        result = validate_input_data(test_input, schema)\n\n        if result[\"is_valid\"]:\n            print(\"\u2705 Validation passed\")\n        else:\n            print(\"\u274c Validation failed\")\n\n        if result[\"errors\"]:\n            print(f\"Errors: {result['errors']}\")\n\n        if result[\"warnings\"]:\n            print(f\"Warnings: {result['warnings']}\")\n</code></pre>"},{"location":"user-guide/validation/#configuration-validation","title":"Configuration Validation","text":""},{"location":"user-guide/validation/#llm-configuration-validation","title":"LLM Configuration Validation","text":"<pre><code>import os\n\ndef validate_llm_configuration(config):\n    \"\"\"Validate LLM configuration.\"\"\"\n\n    validation_result = {\n        \"is_valid\": True,\n        \"errors\": [],\n        \"warnings\": []\n    }\n\n    try:\n        # Test OpenAI configuration\n        if hasattr(config, 'provider') and config.provider == 'openai':\n            openai_config = graphbit.LlmConfig.openai(\n                api_key=os.getenv(\"OPENAI_API_KEY\"),\n                model=\"gpt-4o-mini\"\n            )\n\n            # Create client to test configuration\n            client = graphbit.LlmClient(openai_config)\n\n            # Test basic connectivity\n            try:\n                client.warmup()\n                print(\"\u2705 OpenAI configuration valid\")\n            except Exception as e:\n                validation_result[\"is_valid\"] = False\n                validation_result[\"errors\"].append(f\"OpenAI connection failed: {e}\")\n\n        # Test Anthropic configuration  \n        elif hasattr(config, 'provider') and config.provider == 'anthropic':\n            try:\n                anthropic_config = graphbit.LlmConfig.anthropic(\n                    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n                    model=\"claude-3-5-sonnet-20241022\"\n                )\n\n                client = graphbit.LlmClient(anthropic_config)\n                client.warmup()\n                print(\"\u2705 Anthropic configuration valid\")\n            except Exception as e:\n                validation_result[\"is_valid\"] = False\n                validation_result[\"errors\"].append(f\"Anthropic connection failed: {e}\")\n\n        # Test Ollama configuration\n        elif hasattr(config, 'provider') and config.provider == 'ollama':\n            try:\n                ollama_config = graphbit.LlmConfig.ollama(\n                    model=\"llama3.2\"\n                )\n\n                client = graphbit.LlmClient(ollama_config)\n                client.warmup()\n                print(\"\u2705 Ollama configuration valid\")\n            except Exception as e:\n                validation_result[\"warnings\"].append(f\"Ollama connection issue: {e}\")\n\n    except Exception as e:\n        validation_result[\"is_valid\"] = False\n        validation_result[\"errors\"].append(f\"Configuration validation failed: {e}\")\n\n    return validation_result\n\ndef validate_embedding_configuration():\n    \"\"\"Validate embedding configuration.\"\"\"\n\n    validation_result = {\n        \"is_valid\": True,\n        \"errors\": [],\n        \"warnings\": []\n    }\n\n    try:\n        # Test OpenAI embeddings\n        openai_config = graphbit.EmbeddingConfig.openai(\n            api_key=os.getenv(\"OPENAI_API_KEY\")\n        )\n\n        client = graphbit.EmbeddingClient(openai_config)\n\n        # Test embedding generation\n        test_embedding = client.embed(\"Test embedding\")\n\n        if test_embedding and len(test_embedding) &gt; 0:\n            print(\"\u2705 OpenAI embedding configuration valid\")\n        else:\n            validation_result[\"warnings\"].append(\"OpenAI embedding returned empty result\")\n\n        # Test HuggingFace embeddings\n        try:\n            hf_config = graphbit.EmbeddingConfig.huggingface(\n                model=\"sentence-transformers/all-MiniLM-L6-v2\"\n            )\n\n            hf_client = graphbit.EmbeddingClient(hf_config)\n            hf_embedding = hf_client.embed(\"Test embedding\")\n\n            if hf_embedding and len(hf_embedding) &gt; 0:\n                print(\"\u2705 HuggingFace embedding configuration valid\")\n            else:\n                validation_result[\"warnings\"].append(\"HuggingFace embedding returned empty result\")\n\n        except Exception as e:\n            validation_result[\"warnings\"].append(f\"HuggingFace embeddings not available: {e}\")\n\n    except Exception as e:\n        validation_result[\"is_valid\"] = False\n        validation_result[\"errors\"].append(f\"Embedding configuration validation failed: {e}\")\n\n    return validation_result\n</code></pre>"},{"location":"user-guide/validation/#execution-validation","title":"Execution Validation","text":""},{"location":"user-guide/validation/#runtime-validation","title":"Runtime Validation","text":"<pre><code>def create_execution_validation_workflow():\n    \"\"\"Create workflow with execution validation.\"\"\"\n\n    workflow = graphbit.Workflow(\"Execution Validation Workflow\")\n\n    # Pre-execution validator\n    pre_validator = graphbit.Node.agent(\n        name=\"Pre-Execution Validator\",\n        prompt=\"\"\"\n        Validate input before processing:\n\n        Input: {input}\n\n        Check:\n        1. Input is processable\n        2. Required resources are available\n        3. No security concerns\n        4. Expected format is correct\n\n        Return validation status and any concerns.\n        \"\"\",\n        agent_id=\"pre_validator\"\n    )\n\n    # Main processor\n    processor = graphbit.Node.agent(\n        name=\"Main Processor\",\n        prompt=\"\"\"\n        Process the validated input:\n\n        Validated Input: {validated_input}\n\n        Perform the main processing task and include quality metrics.\n        \"\"\",\n        agent_id=\"main_processor\"\n    )\n\n    # Post-execution validator\n    post_validator = graphbit.Node.agent(\n        name=\"Post-Execution Validator\",\n        prompt=\"\"\"\n        Validate processing results:\n\n        Processing Results: {processing_results}\n\n        Check:\n        1. Results are complete\n        2. Quality meets standards\n        3. Format is correct\n        4. No errors in output\n\n        Return final validation and any needed corrections.\n        \"\"\",\n        agent_id=\"post_validator\"\n    )\n\n    # Build validation workflow\n    pre_id = workflow.add_node(pre_validator)\n    proc_id = workflow.add_node(processor)\n    post_id = workflow.add_node(post_validator)\n\n    workflow.connect(pre_id, proc_id)\n    workflow.connect(proc_id, post_id)\n\n    return workflow\n\ndef validate_execution_result(result):\n    \"\"\"Validate workflow execution result.\"\"\"\n\n    validation_report = {\n        \"execution_successful\": False,\n        \"output_valid\": False,\n        \"quality_score\": 0.0,\n        \"issues\": [],\n        \"recommendations\": []\n    }\n\n    # Check if execution completed\n    if result.is_completed():\n        validation_report[\"execution_successful\"] = True\n\n        # Get output\n        output = result.output()\n\n        # Validate output\n        if output:\n            validation_report[\"output_valid\"] = True\n\n            # Basic quality checks\n            if isinstance(output, str):\n                if len(output.strip()) &gt; 0:\n                    validation_report[\"quality_score\"] += 0.5\n\n                if len(output) &gt; 10:  # Meaningful length\n                    validation_report[\"quality_score\"] += 0.3\n\n                # Check for common error indicators\n                error_indicators = [\"error\", \"failed\", \"invalid\", \"exception\"]\n                if not any(indicator in output.lower() for indicator in error_indicators):\n                    validation_report[\"quality_score\"] += 0.2\n                else:\n                    validation_report[\"issues\"].append(\"Output contains error indicators\")\n\n            elif isinstance(output, dict):\n                if output:  # Non-empty dict\n                    validation_report[\"quality_score\"] += 0.7\n\n                # Check for error fields\n                if \"error\" in output or \"exception\" in output:\n                    validation_report[\"issues\"].append(\"Output contains error fields\")\n                else:\n                    validation_report[\"quality_score\"] += 0.3\n\n        else:\n            validation_report[\"issues\"].append(\"Output is empty\")\n\n    else:\n        validation_report[\"issues\"].append(f\"Execution failed: {result.error()}\")\n\n    # Generate recommendations\n    if validation_report[\"quality_score\"] &lt; 0.5:\n        validation_report[\"recommendations\"].append(\"Consider improving input quality\")\n\n    if validation_report[\"issues\"]:\n        validation_report[\"recommendations\"].append(\"Review and fix identified issues\")\n\n    return validation_report\n</code></pre>"},{"location":"user-guide/validation/#output-validation","title":"Output Validation","text":""},{"location":"user-guide/validation/#result-quality-validation","title":"Result Quality Validation","text":"<pre><code>def create_output_validation_workflow():\n    \"\"\"Create workflow with comprehensive output validation.\"\"\"\n\n    workflow = graphbit.Workflow(\"Output Validation Workflow\")\n\n    # Content generator\n    generator = graphbit.Node.agent(\n        name=\"Content Generator\",\n        prompt=\"Generate content based on: {input}\",\n        agent_id=\"generator\"\n    )\n\n    # Quality checker\n    quality_checker = graphbit.Node.agent(\n        name=\"Quality Checker\",\n        prompt=\"\"\"\n        Evaluate the quality of this generated content:\n\n        Content: {generated_content}\n\n        Rate on a scale of 1-10 for:\n        1. Accuracy\n        2. Completeness\n        3. Clarity\n        4. Relevance\n        5. Coherence\n\n        Provide overall quality score and specific feedback.\n        \"\"\",\n        agent_id=\"quality_checker\"\n    )\n\n    # Format validator\n    format_validator = graphbit.Node.agent(\n        name=\"Format Validator\",\n        prompt=\"\"\"\n        Validate the format of this content:\n\n        Content: {quality_checked_content}\n\n        Check:\n        1. Proper structure\n        2. Correct formatting\n        3. Standard compliance\n        4. Readability\n\n        Return validation results and any format corrections needed.\n        \"\"\",\n        agent_id=\"format_validator\"\n    )\n\n    # Build output validation workflow\n    gen_id = workflow.add_node(generator)\n    quality_id = workflow.add_node(quality_checker)\n    format_id = workflow.add_node(format_validator)\n\n    workflow.connect(gen_id, quality_id)\n    workflow.connect(quality_id, format_id)\n\n    return workflow\n\ndef validate_output_quality(output, criteria=None):\n    \"\"\"Validate output quality against specific criteria.\"\"\"\n\n    if criteria is None:\n        criteria = {\n            \"min_length\": 10,\n            \"max_length\": 10000,\n            \"required_elements\": [],\n            \"forbidden_elements\": [\"error\", \"exception\", \"failed\"]\n        }\n\n    quality_report = {\n        \"overall_score\": 0.0,\n        \"length_valid\": False,\n        \"content_valid\": False,\n        \"format_valid\": False,\n        \"issues\": [],\n        \"suggestions\": []\n    }\n\n    if not output:\n        quality_report[\"issues\"].append(\"Output is empty\")\n        return quality_report\n\n    output_str = str(output)\n\n    # Length validation\n    if criteria[\"min_length\"] &lt;= len(output_str) &lt;= criteria[\"max_length\"]:\n        quality_report[\"length_valid\"] = True\n        quality_report[\"overall_score\"] += 0.3\n    else:\n        quality_report[\"issues\"].append(f\"Length {len(output_str)} outside range {criteria['min_length']}-{criteria['max_length']}\")\n\n    # Content validation\n    content_score = 0.0\n\n    # Check for required elements\n    for element in criteria.get(\"required_elements\", []):\n        if element.lower() in output_str.lower():\n            content_score += 0.2\n        else:\n            quality_report[\"issues\"].append(f\"Missing required element: {element}\")\n\n    # Check for forbidden elements\n    forbidden_found = False\n    for element in criteria.get(\"forbidden_elements\", []):\n        if element.lower() in output_str.lower():\n            quality_report[\"issues\"].append(f\"Contains forbidden element: {element}\")\n            forbidden_found = True\n\n    if not forbidden_found:\n        content_score += 0.3\n\n    quality_report[\"content_valid\"] = content_score &gt; 0.0\n    quality_report[\"overall_score\"] += min(content_score, 0.4)\n\n    # Format validation (basic)\n    if output_str.strip():\n        quality_report[\"format_valid\"] = True\n        quality_report[\"overall_score\"] += 0.3\n\n    # Generate suggestions\n    if quality_report[\"overall_score\"] &lt; 0.5:\n        quality_report[\"suggestions\"].append(\"Consider regenerating output with better parameters\")\n\n    if not quality_report[\"content_valid\"]:\n        quality_report[\"suggestions\"].append(\"Review content requirements and regenerate\")\n\n    return quality_report\n</code></pre>"},{"location":"user-guide/validation/#validation-testing-framework","title":"Validation Testing Framework","text":""},{"location":"user-guide/validation/#comprehensive-validation-testing","title":"Comprehensive Validation Testing","text":"<pre><code>def create_validation_test_suite():\n    \"\"\"Create comprehensive validation test suite.\"\"\"\n\n    class ValidationTestSuite:\n        def __init__(self):\n            self.test_results = {}\n\n        def run_workflow_validation_tests(self):\n            \"\"\"Run workflow validation tests.\"\"\"\n\n            print(\"\ud83e\uddea Running Workflow Validation Tests\")\n\n            tests = {\n                \"basic_workflow\": self.test_basic_workflow_validation,\n                \"complex_workflow\": self.test_complex_workflow_validation,\n                \"invalid_workflow\": self.test_invalid_workflow_validation\n            }\n\n            for test_name, test_func in tests.items():\n                try:\n                    result = test_func()\n                    self.test_results[test_name] = {\"passed\": result, \"error\": None}\n                    status = \"\u2705 PASSED\" if result else \"\u274c FAILED\"\n                    print(f\"  {test_name}: {status}\")\n                except Exception as e:\n                    self.test_results[test_name] = {\"passed\": False, \"error\": str(e)}\n                    print(f\"  {test_name}: \u274c ERROR - {e}\")\n\n        def test_basic_workflow_validation(self):\n            \"\"\"Test basic workflow validation.\"\"\"\n            return validate_workflow_basic()\n\n        def test_complex_workflow_validation(self):\n            \"\"\"Test complex workflow validation.\"\"\"\n            return validate_workflow_comprehensive()\n\n        def test_invalid_workflow_validation(self):\n            \"\"\"Test validation of invalid workflow.\"\"\"\n\n            # Create intentionally invalid workflow\n            workflow = graphbit.Workflow(\"Invalid Test Workflow\")\n\n            # Add node but don't connect it properly (this may or may not be invalid depending on GraphBit's rules)\n            node = graphbit.Node.agent(\n                name=\"Isolated Node\",\n                prompt=\"Process: {input}\",\n                agent_id=\"isolated\"\n            )\n            workflow.add_node(node)\n\n            # Try to validate - this should pass since a single node workflow is valid\n            try:\n                workflow.validate()\n                return True  # Single nodes are actually valid\n            except Exception:\n                return True  # Expected to fail\n\n        def run_input_validation_tests(self):\n            \"\"\"Run input validation tests.\"\"\"\n\n            print(\"\ud83e\uddea Running Input Validation Tests\")\n\n            test_cases = [\n                {\"input\": \"Valid string\", \"expected\": True},\n                {\"input\": \"\", \"expected\": False},\n                {\"input\": None, \"expected\": False},\n                {\"input\": {\"text\": \"Valid\", \"priority\": 1}, \"expected\": True},\n                {\"input\": {\"text\": \"\", \"priority\": \"invalid\"}, \"expected\": False}\n            ]\n\n            passed = 0\n            total = len(test_cases)\n\n            for i, case in enumerate(test_cases):\n                result = validate_input_data(case[\"input\"])\n                actual_valid = result[\"is_valid\"]\n                expected_valid = case[\"expected\"]\n\n                if actual_valid == expected_valid:\n                    print(f\"  Test {i+1}: \u2705 PASSED\")\n                    passed += 1\n                else:\n                    print(f\"  Test {i+1}: \u274c FAILED - Expected {expected_valid}, got {actual_valid}\")\n\n            self.test_results[\"input_validation\"] = {\"passed\": passed, \"total\": total}\n            print(f\"Input validation: {passed}/{total} tests passed\")\n\n        def run_configuration_validation_tests(self):\n            \"\"\"Run configuration validation tests.\"\"\"\n\n            print(\"\ud83e\uddea Running Configuration Validation Tests\")\n\n            # Test LLM configuration\n            try:\n                config = graphbit.LlmConfig.openai(\n                    api_key=os.getenv(\"OPENAI_API_KEY\") or \"test-key\",\n                    model=\"gpt-4o-mini\"\n                )\n\n                # This is a conceptual test - actual validation depends on implementation\n                print(\"  LLM Config: \u2705 PASSED\")\n                self.test_results[\"llm_config\"] = {\"passed\": True}\n\n            except Exception as e:\n                print(f\"  LLM Config: \u274c FAILED - {e}\")\n                self.test_results[\"llm_config\"] = {\"passed\": False, \"error\": str(e)}\n\n            # Test embedding configuration\n            try:\n                embed_config = graphbit.EmbeddingConfig.openai(\n                    api_key=os.getenv(\"OPENAI_API_KEY\") or \"test-key\"\n                )\n\n                print(\"  Embedding Config: \u2705 PASSED\")\n                self.test_results[\"embedding_config\"] = {\"passed\": True}\n\n            except Exception as e:\n                print(f\"  Embedding Config: \u274c FAILED - {e}\")\n                self.test_results[\"embedding_config\"] = {\"passed\": False, \"error\": str(e)}\n\n        def run_all_tests(self):\n            \"\"\"Run all validation tests.\"\"\"\n\n            print(\"\ud83d\ude80 Starting Comprehensive Validation Test Suite\")\n            print(\"=\" * 50)\n\n            self.run_workflow_validation_tests()\n            print()\n            self.run_input_validation_tests()\n            print()\n            self.run_configuration_validation_tests()\n\n            print(\"\\n\" + \"=\" * 50)\n            print(\"\ud83d\udcca Test Suite Results Summary\")\n\n            total_passed = 0\n            total_tests = 0\n\n            for test_name, result in self.test_results.items():\n                if isinstance(result, dict) and \"passed\" in result:\n                    if isinstance(result[\"passed\"], bool):\n                        total_tests += 1\n                        if result[\"passed\"]:\n                            total_passed += 1\n                        status = \"\u2705\" if result[\"passed\"] else \"\u274c\"\n                    else:\n                        # For input validation which has passed/total format\n                        total_tests += result.get(\"total\", 0)\n                        total_passed += result.get(\"passed\", 0)\n                        status = \"\u2705\" if result[\"passed\"] == result.get(\"total\", 0) else \"\u274c\"\n\n                    print(f\"{status} {test_name}: {result}\")\n\n            success_rate = (total_passed / total_tests * 100) if total_tests &gt; 0 else 0\n            print(f\"\\nOverall Success Rate: {success_rate:.1f}% ({total_passed}/{total_tests})\")\n\n            return self.test_results\n\n    return ValidationTestSuite()\n\ndef example_comprehensive_validation():\n    \"\"\"Example of comprehensive validation testing.\"\"\"\n\n    # Initialize GraphBit\n    graphbit.init()\n\n    # Create test suite\n    test_suite = create_validation_test_suite()\n\n    # Run all tests\n    results = test_suite.run_all_tests()\n\n    return results\n</code></pre>"},{"location":"user-guide/validation/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/validation/#1-validation-strategy","title":"1. Validation Strategy","text":"<pre><code>def get_validation_best_practices():\n    \"\"\"Get best practices for validation.\"\"\"\n\n    best_practices = {\n        \"early_validation\": \"Validate inputs as early as possible\",\n        \"comprehensive_checks\": \"Use multiple validation layers\",\n        \"clear_error_messages\": \"Provide actionable error messages\",\n        \"graceful_degradation\": \"Handle validation failures gracefully\",\n        \"performance_balance\": \"Balance validation thoroughness with performance\",\n        \"automated_testing\": \"Automate validation testing in CI/CD\",\n        \"continuous_monitoring\": \"Monitor validation metrics in production\"\n    }\n\n    for practice, description in best_practices.items():\n        print(f\"\u2705 {practice.replace('_', ' ').title()}: {description}\")\n\n    return best_practices\n</code></pre>"},{"location":"user-guide/validation/#2-error-handling","title":"2. Error Handling","text":"<pre><code>def handle_validation_error(validation_result, context=\"validation\"):\n    \"\"\"Handle validation errors appropriately.\"\"\"\n\n    if validation_result.get(\"is_valid\", True):\n        return True\n\n    errors = validation_result.get(\"errors\", [])\n    warnings = validation_result.get(\"warnings\", [])\n\n    # Log errors\n    print(f\"\u274c {context.title()} failed:\")\n    for error in errors:\n        print(f\"  - {error}\")\n\n    # Log warnings\n    if warnings:\n        print(f\"\u26a0\ufe0f {context.title()} warnings:\")\n        for warning in warnings:\n            print(f\"  - {warning}\")\n\n    # Determine if execution should continue\n    critical_errors = [error for error in errors if \"critical\" in error.lower()]\n\n    if critical_errors:\n        print(\"\u274c Critical errors found, stopping execution\")\n        return False\n    elif len(errors) &gt; 0:\n        print(\"\u26a0\ufe0f Non-critical errors found, proceeding with caution\")\n        return True\n    else:\n        print(\"\u2705 Only warnings found, continuing execution\")\n        return True\n</code></pre>"},{"location":"user-guide/validation/#usage-examples","title":"Usage Examples","text":""},{"location":"user-guide/validation/#complete-validation-example","title":"Complete Validation Example","text":"<pre><code>def example_complete_validation():\n    \"\"\"Complete example of validation in practice.\"\"\"\n\n    # Initialize GraphBit\n    graphbit.init()\n\n    print(\"\ud83d\ude80 Starting Complete Validation Example\")\n\n    # 1. Input validation\n    test_input = {\"text\": \"Hello, world!\", \"priority\": 1}\n    input_validation = validate_input_data(test_input)\n\n    if not handle_validation_error(input_validation, \"input validation\"):\n        return False\n\n    # 2. Workflow validation\n    workflow = create_input_validation_workflow()\n\n    if not validate_workflow_basic():\n        print(\"\u274c Workflow validation failed\")\n        return False\n\n    # 3. Configuration validation\n    try:\n        llm_config = graphbit.LlmConfig.openai(\n            api_key=os.getenv(\"OPENAI_API_KEY\"),\n            model=\"gpt-4o-mini\"\n        )\n\n        executor = graphbit.Executor(llm_config)\n        print(\"\u2705 Configuration validation passed\")\n\n    except Exception as e:\n        print(f\"\u274c Configuration validation failed: {e}\")\n        return False\n\n    # 4. Execution validation\n    try:\n        result = executor.execute(workflow)\n        execution_validation = validate_execution_result(result)\n\n        if not execution_validation[\"execution_successful\"]:\n            print(\"\u274c Execution validation failed\")\n            return False\n\n        # 5. Output validation\n        output_validation = validate_output_quality(result.output())\n\n        if output_validation[\"overall_score\"] &lt; 0.5:\n            print(f\"\u26a0\ufe0f Output quality score low: {output_validation['overall_score']}\")\n        else:\n            print(f\"\u2705 Output quality good: {output_validation['overall_score']}\")\n\n        print(\"\u2705 Complete validation example passed\")\n        return True\n\n    except Exception as e:\n        print(f\"\u274c Execution failed: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    example_complete_validation()\n</code></pre>"},{"location":"user-guide/validation/#whats-next","title":"What's Next","text":"<ul> <li>Learn about Reliability for building robust validated systems</li> <li>Explore Monitoring for tracking validation metrics  </li> <li>Check Performance for optimizing validation overhead</li> <li>See Error Handling for comprehensive error management </li> </ul>"},{"location":"user-guide/workflow-builder/","title":"Workflow Builder","text":"<p>The Workflow Builder in GraphBit provides a simple, direct approach to creating AI agent workflows. Unlike complex builder patterns, GraphBit uses straightforward workflow and node creation methods.</p>"},{"location":"user-guide/workflow-builder/#overview","title":"Overview","text":"<p>GraphBit workflows are built using: - Workflow - Container for nodes and connections - Node - Individual processing units with different types - Connections - Links between nodes that define data flow</p>"},{"location":"user-guide/workflow-builder/#basic-usage","title":"Basic Usage","text":""},{"location":"user-guide/workflow-builder/#creating-a-workflow","title":"Creating a Workflow","text":"<pre><code>import graphbit\n\n# Initialize GraphBit\ngraphbit.init()\n\n# Create a new workflow\nworkflow = graphbit.Workflow(\"My AI Pipeline\")\n</code></pre>"},{"location":"user-guide/workflow-builder/#creating-nodes","title":"Creating Nodes","text":"<p>GraphBit supports several node types:</p>"},{"location":"user-guide/workflow-builder/#agent-nodes","title":"Agent Nodes","text":"<p>Execute AI tasks using LLM providers:</p> <pre><code># Basic agent node\nanalyzer = graphbit.Node.agent(\n    name=\"Data Analyzer\",\n    prompt=\"Analyze this data for patterns: {input}\",\n    agent_id=\"analyzer_001\"  # Optional - auto-generated if not provided\n)\n\n# Agent with explicit ID\nsummarizer = graphbit.Node.agent(\n    name=\"Content Summarizer\", \n    prompt=\"Summarize the following content: {analysis}\",\n    agent_id=\"summarizer\"\n)\n</code></pre>"},{"location":"user-guide/workflow-builder/#transform-nodes","title":"Transform Nodes","text":"<p>Process and modify data:</p> <pre><code># Text transformation\nformatter = graphbit.Node.transform(\n    name=\"Text Formatter\",\n    transformation=\"uppercase\"\n)\n\n# Other available transformations\nlowercase_node = graphbit.Node.transform(\"Lowercase\", \"lowercase\")\n</code></pre>"},{"location":"user-guide/workflow-builder/#condition-nodes","title":"Condition Nodes","text":"<p>Make decisions based on data evaluation:</p> <pre><code># Quality check\nquality_gate = graphbit.Node.condition(\n    name=\"Quality Gate\",\n    expression=\"quality_score &gt; 0.8 and confidence &gt; 0.7\"\n)\n\n# Simple condition\nthreshold_check = graphbit.Node.condition(\n    name=\"Threshold Check\", \n    expression=\"value &gt; 100\"\n)\n</code></pre>"},{"location":"user-guide/workflow-builder/#building-the-workflow","title":"Building the Workflow","text":"<pre><code># Add nodes to workflow and get their IDs\nanalyzer_id = workflow.add_node(analyzer)\nformatter_id = workflow.add_node(formatter)\nquality_id = workflow.add_node(quality_gate)\nsummarizer_id = workflow.add_node(summarizer)\n\n# Connect nodes to define data flow\nworkflow.connect(analyzer_id, formatter_id)\nworkflow.connect(formatter_id, quality_id)\nworkflow.connect(quality_id, summarizer_id)\n\n# Validate workflow structure\nworkflow.validate()\n</code></pre>"},{"location":"user-guide/workflow-builder/#workflow-patterns","title":"Workflow Patterns","text":""},{"location":"user-guide/workflow-builder/#sequential-processing","title":"Sequential Processing","text":"<p>Create linear processing pipelines:</p> <pre><code># Create workflow\nworkflow = graphbit.Workflow(\"Sequential Pipeline\")\n\n# Create processing steps\nstep1 = graphbit.Node.agent(\n    name=\"Input Processor\",\n    prompt=\"Process the initial input: {input}\",\n    agent_id=\"step1\"\n)\n\nstep2 = graphbit.Node.agent(\n    name=\"Data Enricher\", \n    prompt=\"Enrich the processed data: {step1_output}\",\n    agent_id=\"step2\"\n)\n\nstep3 = graphbit.Node.agent(\n    name=\"Final Formatter\",\n    prompt=\"Format the final output: {step2_output}\",\n    agent_id=\"step3\"\n)\n\n# Add nodes and connect sequentially\nid1 = workflow.add_node(step1)\nid2 = workflow.add_node(step2)\nid3 = workflow.add_node(step3)\n\nworkflow.connect(id1, id2)\nworkflow.connect(id2, id3)\n\nworkflow.validate()\n</code></pre>"},{"location":"user-guide/workflow-builder/#parallel-processing-branches","title":"Parallel Processing Branches","text":"<p>Create workflows with multiple parallel processing paths:</p> <pre><code># Create workflow\nworkflow = graphbit.Workflow(\"Parallel Analysis\")\n\n# Input node\ninput_processor = graphbit.Node.agent(\n    name=\"Input Processor\",\n    prompt=\"Prepare data for analysis: {input}\",\n    agent_id=\"input_proc\"\n)\n\n# Parallel analysis branches\nsentiment_analyzer = graphbit.Node.agent(\n    name=\"Sentiment Analyzer\",\n    prompt=\"Analyze sentiment of: {processed_data}\",\n    agent_id=\"sentiment\"\n)\n\ntopic_analyzer = graphbit.Node.agent(\n    name=\"Topic Analyzer\", \n    prompt=\"Extract key topics from: {processed_data}\",\n    agent_id=\"topics\"\n)\n\nquality_analyzer = graphbit.Node.agent(\n    name=\"Quality Analyzer\",\n    prompt=\"Assess content quality of: {processed_data}\",\n    agent_id=\"quality\"\n)\n\n# Result aggregator\naggregator = graphbit.Node.agent(\n    name=\"Result Aggregator\",\n    prompt=\"Combine analysis results:\\nSentiment: {sentiment_output}\\nTopics: {topics_output}\\nQuality: {quality_output}\",\n    agent_id=\"aggregator\"\n)\n\n# Build parallel structure\ninput_id = workflow.add_node(input_processor)\nsentiment_id = workflow.add_node(sentiment_analyzer)\ntopic_id = workflow.add_node(topic_analyzer)\nquality_id = workflow.add_node(quality_analyzer)\nagg_id = workflow.add_node(aggregator)\n\n# Connect input to all analyzers\nworkflow.connect(input_id, sentiment_id)\nworkflow.connect(input_id, topic_id)\nworkflow.connect(input_id, quality_id)\n\n# Connect all analyzers to aggregator\nworkflow.connect(sentiment_id, agg_id)\nworkflow.connect(topic_id, agg_id)\nworkflow.connect(quality_id, agg_id)\n\nworkflow.validate()\n</code></pre>"},{"location":"user-guide/workflow-builder/#conditional-workflows","title":"Conditional Workflows","text":"<p>Use condition nodes for dynamic routing:</p> <pre><code># Create workflow\nworkflow = graphbit.Workflow(\"Conditional Processing\")\n\n# Content analyzer\nanalyzer = graphbit.Node.agent(\n    name=\"Content Analyzer\",\n    prompt=\"Analyze content quality (score 1-10): {input}\",\n    agent_id=\"analyzer\"\n)\n\n# Quality gate condition\nquality_gate = graphbit.Node.condition(\n    name=\"Quality Gate\",\n    expression=\"quality_score &gt;= 7\"\n)\n\n# High quality path\napprover = graphbit.Node.agent(\n    name=\"Content Approver\",\n    prompt=\"Approve high-quality content: {analyzed_content}\",\n    agent_id=\"approver\"\n)\n\n# Low quality path  \nimprover = graphbit.Node.agent(\n    name=\"Content Improver\",\n    prompt=\"Suggest improvements for: {analyzed_content}\",\n    agent_id=\"improver\"\n)\n\n# Build conditional flow\nanalyzer_id = workflow.add_node(analyzer)\ngate_id = workflow.add_node(quality_gate)\napprove_id = workflow.add_node(approver)\nimprove_id = workflow.add_node(improver)\n\n# Connect analyzer to quality gate\nworkflow.connect(analyzer_id, gate_id)\n\n# Connect based on conditions (simplified - actual conditional routing \n# is handled by the executor based on the condition node evaluation)\nworkflow.connect(gate_id, approve_id)\nworkflow.connect(gate_id, improve_id)\n\nworkflow.validate()\n</code></pre>"},{"location":"user-guide/workflow-builder/#data-transformation-pipelines","title":"Data Transformation Pipelines","text":"<p>Combine transform nodes with AI agents:</p> <pre><code># Create workflow\nworkflow = graphbit.Workflow(\"Data Transformation Pipeline\")\n\n# Text preprocessor\npreprocessor = graphbit.Node.transform(\n    name=\"Text Preprocessor\",\n    transformation=\"lowercase\"\n)\n\n# Content processor\nprocessor = graphbit.Node.agent(\n    name=\"Content Processor\", \n    prompt=\"Process the cleaned text: {preprocessed_text}\",\n    agent_id=\"processor\"\n)\n\n# Output formatter\nformatter = graphbit.Node.transform(\n    name=\"Output Formatter\",\n    transformation=\"uppercase\"\n)\n\n# Final quality check\nvalidator = graphbit.Node.condition(\n    name=\"Output Validator\",\n    expression=\"length &gt; 10\"\n)\n\n# Build transformation pipeline\nprep_id = workflow.add_node(preprocessor)\nproc_id = workflow.add_node(processor)\nformat_id = workflow.add_node(formatter)\nvalid_id = workflow.add_node(validator)\n\nworkflow.connect(prep_id, proc_id)\nworkflow.connect(proc_id, format_id)\nworkflow.connect(format_id, valid_id)\n\nworkflow.validate()\n</code></pre>"},{"location":"user-guide/workflow-builder/#node-properties-and-management","title":"Node Properties and Management","text":""},{"location":"user-guide/workflow-builder/#accessing-node-information","title":"Accessing Node Information","text":"<pre><code># Create a node\nanalyzer = graphbit.Node.agent(\n    name=\"Data Analyzer\",\n    prompt=\"Analyze: {input}\",\n    agent_id=\"analyzer\"\n)\n\n# Access node properties\nprint(f\"Node ID: {analyzer.id()}\")\nprint(f\"Node Name: {analyzer.name()}\")\n\n# Add to workflow\nworkflow = graphbit.Workflow(\"Test Workflow\")\nnode_id = workflow.add_node(analyzer)\nprint(f\"Workflow Node ID: {node_id}\")\n</code></pre>"},{"location":"user-guide/workflow-builder/#node-naming-best-practices","title":"Node Naming Best Practices","text":"<p>Use descriptive, clear names for nodes:</p> <pre><code># Good - descriptive and clear\nemail_analyzer = graphbit.Node.agent(\n    name=\"Email Content Analyzer\",\n    prompt=\"Analyze email for spam indicators: {email_content}\",\n    agent_id=\"email_spam_detector\"\n)\n\n# Good - indicates purpose\nquality_gate = graphbit.Node.condition(\n    name=\"Content Quality Gate\",\n    expression=\"spam_score &lt; 0.3\"\n)\n\n# Avoid - vague names\nnode1 = graphbit.Node.agent(\n    name=\"Node1\", \n    prompt=\"Do something: {input}\",\n    agent_id=\"n1\"\n)\n</code></pre>"},{"location":"user-guide/workflow-builder/#workflow-execution","title":"Workflow Execution","text":""},{"location":"user-guide/workflow-builder/#setting-up-execution","title":"Setting up Execution","text":"<pre><code># Create LLM configuration\nllm_config = graphbit.LlmConfig.openai(\n    api_key=\"your-openai-key\",\n    model=\"gpt-4o-mini\"\n)\n\n# Create executor\nexecutor = graphbit.Executor(\n    config=llm_config,\n    timeout_seconds=300,\n    debug=False\n)\n\n# Execute workflow\nresult = executor.execute(workflow)\n\n# Check results\nif result.is_completed():\n    print(\"Success:\", result.output())\nelif result.is_failed():\n    print(\"Failed:\", result.error())\n</code></pre>"},{"location":"user-guide/workflow-builder/#asynchronous-execution","title":"Asynchronous Execution","text":"<pre><code>import asyncio\n\nasync def run_workflow():\n    # Create workflow and executor as above\n    result = await executor.run_async(workflow)\n    return result\n\n# Run asynchronously\nresult = asyncio.run(run_workflow())\n</code></pre>"},{"location":"user-guide/workflow-builder/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"user-guide/workflow-builder/#multi-stage-processing","title":"Multi-Stage Processing","text":"<pre><code>def create_multi_stage_workflow():\n    workflow = graphbit.Workflow(\"Multi-Stage Processing\")\n\n    # Stage 1: Data Preparation\n    cleaner = graphbit.Node.transform(\"Data Cleaner\", \"lowercase\")\n    validator = graphbit.Node.condition(\"Data Validator\", \"length &gt; 5\")\n\n    # Stage 2: Analysis\n    analyzer = graphbit.Node.agent(\n        name=\"Content Analyzer\",\n        prompt=\"Analyze cleaned content: {cleaned_data}\",\n        agent_id=\"analyzer\"\n    )\n\n    # Stage 3: Output Processing\n    formatter = graphbit.Node.transform(\"Output Formatter\", \"uppercase\")\n    finalizer = graphbit.Node.agent(\n        name=\"Output Finalizer\",\n        prompt=\"Finalize the analysis: {formatted_output}\",\n        agent_id=\"finalizer\"\n    )\n\n    # Build multi-stage pipeline\n    clean_id = workflow.add_node(cleaner)\n    valid_id = workflow.add_node(validator)\n    analyze_id = workflow.add_node(analyzer)\n    format_id = workflow.add_node(formatter)\n    final_id = workflow.add_node(finalizer)\n\n    # Connect stages\n    workflow.connect(clean_id, valid_id)\n    workflow.connect(valid_id, analyze_id)\n    workflow.connect(analyze_id, format_id)\n    workflow.connect(format_id, final_id)\n\n    workflow.validate()\n    return workflow\n</code></pre>"},{"location":"user-guide/workflow-builder/#error-handling-in-workflows","title":"Error Handling in Workflows","text":"<pre><code>def create_robust_workflow():\n    workflow = graphbit.Workflow(\"Robust Processing\")\n\n    try:\n        # Add nodes\n        processor = graphbit.Node.agent(\n            name=\"Data Processor\",\n            prompt=\"Process: {input}\",\n            agent_id=\"processor\"\n        )\n\n        node_id = workflow.add_node(processor)\n        # Validate before execution\n        workflow.validate()        \n        return workflow\n\n    except Exception as e:\n        print(f\"Workflow creation failed: {e}\")\n        return None\n</code></pre>"},{"location":"user-guide/workflow-builder/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/workflow-builder/#1-workflow-organization","title":"1. Workflow Organization","text":"<pre><code># Organize complex workflows into functions\ndef create_analysis_workflow():\n    workflow = graphbit.Workflow(\"Content Analysis\")    \n    # Input processing\n    input_node = create_input_processor()    \n    # Analysis stages  \n    sentiment_node = create_sentiment_analyzer()\n    topic_node = create_topic_analyzer()    \n    # Output processing\n    output_node = create_output_formatter()\n\n    # Build workflow\n    input_id = workflow.add_node(input_node)\n    sentiment_id = workflow.add_node(sentiment_node)\n    topic_id = workflow.add_node(topic_node)\n    output_id = workflow.add_node(output_node)    \n    # Connect nodes\n    workflow.connect(input_id, sentiment_id)\n    workflow.connect(input_id, topic_id)\n    workflow.connect(sentiment_id, output_id)\n    workflow.connect(topic_id, output_id)    \n    workflow.validate()\n    return workflow\n\ndef create_input_processor():\n    return graphbit.Node.agent(\n        name=\"Input Processor\",\n        prompt=\"Prepare input for analysis: {input}\",\n        agent_id=\"input_processor\"\n    )\n</code></pre>"},{"location":"user-guide/workflow-builder/#2-validation-and-testing","title":"2. Validation and Testing","text":"<pre><code>def test_workflow(workflow):\n    \"\"\"Test workflow structure before execution\"\"\"\n    try:\n        workflow.validate()\n        print(\"\u2705 Workflow validation passed\")\n        return True\n    except Exception as e:\n        print(f\"\u274c Workflow validation failed: {e}\")\n        return False\n\n# Always validate before execution\nworkflow = create_analysis_workflow()\nif test_workflow(workflow):\n    result = executor.execute(workflow)\n</code></pre>"},{"location":"user-guide/workflow-builder/#3-resource-management","title":"3. Resource Management","text":"<pre><code># Choose appropriate executor for your use case\ndef get_executor_for_workload(workload_type, llm_config):\n    if workload_type == \"batch\":\n        return graphbit.Executor.new_high_throughput(\n            llm_config=llm_config,\n            timeout_seconds=600\n        )\n    elif workload_type == \"realtime\":\n        return graphbit.Executor.new_low_latency(\n            llm_config=llm_config,\n            timeout_seconds=30\n        )\n    elif workload_type == \"constrained\":\n        return graphbit.Executor.new_memory_optimized(\n            llm_config=llm_config,\n            timeout_seconds=300\n        )\n    else:\n        return graphbit.Executor(\n            config=llm_config,\n            timeout_seconds=300\n        )\n</code></pre>"},{"location":"user-guide/workflow-builder/#common-patterns-summary","title":"Common Patterns Summary","text":"Pattern Use Case Example Sequential Linear processing Data cleaning \u2192 Analysis \u2192 Output Parallel Independent analysis Multiple analyzers running simultaneously Conditional Dynamic routing Quality gate directing to approval/rejection Transform Data preprocessing Text cleaning and formatting Multi-stage Complex pipelines Preparation \u2192 Analysis \u2192 Finalization"},{"location":"user-guide/workflow-builder/#whats-next","title":"What's Next","text":"<ul> <li>Learn about LLM Providers for configuring different AI models</li> <li>Explore Agents for advanced agent configuration</li> <li>Check Performance for execution optimization</li> <li>See Validation for workflow validation strategies</li> </ul>"}]}